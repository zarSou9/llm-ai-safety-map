[
  {
    "url": "https://arxiv.org/abs/2409.14160",
    "title": "Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI",
    "published_date": "2024-09-21",
    "abstract": "With the growing attention and investment in recent AI approaches such as large language models, the narrative that the larger the AI system the more valuable, powerful and interesting it is is increasingly seen as common sense. But what is this assumption based on, and how are we measuring value, power, and performance? And what are the collateral consequences of this race to ever-increasing scale? Here, we scrutinize the current scaling trends and trade-offs across multiple axes and refute two common assumptions underlying the 'bigger-is-better' AI paradigm: 1) that improved performance is a product of increased scale, and 2) that all interesting problems addressed by AI require large-scale models. Rather, we argue that this approach is not only fragile scientifically, but comes with undesirable consequences. First, it is not sustainable, as its compute demands increase faster than model performance, leading to unreasonable economic requirements and a disproportionate environmental footprint. Second, it implies focusing on certain problems at the expense of others, leaving aside important applications, e.g. health, education, or the climate. Finally, it exacerbates a concentration of power, which centralizes decision-making in the hands of a few actors while threatening to disempower others in the context of shaping both AI research and its applications throughout society.",
    "citation_count": 6,
    "summary": "The paper critiques the \"bigger-is-better\" paradigm in AI, arguing that scaling up models unsustainably increases computational costs and environmental impact without proportionally improving performance, while also neglecting important applications and concentrating power among a few actors. This approach is deemed scientifically fragile and socially undesirable."
  },
  {
    "url": "https://arxiv.org/abs/2411.18461",
    "title": "Scale Economies and Aggregate Productivity",
    "published_date": "2024-11-27",
    "abstract": "We develop a theoretical framework to investigate the link between rising scale economies and stagnating productivity. Our model features heterogeneous firms, imperfect competition, and firm selection. We demonstrate that scale economies generated by fixed costs have distinct impacts on aggregate productivity compared to those driven by returns to scale. Using UK data, we estimate long-run increases in both fixed costs and returns to scale. Our model implies that this should increase aggregate productivity through improved firm selection and resource allocation. However, increasing markups can offset the productivity gain. Higher markups cushion low-productivity firms' revenues, allowing them to survive, and constrain firm output, which limits exploitation of scale economies.",
    "summary": "The paper models the impact of scale economies (from fixed costs and returns to scale) on aggregate productivity, finding that while increased scale economies improve firm selection and resource allocation, higher markups can offset these gains by protecting less productive firms and limiting output."
  },
  {
    "url": "http://arxiv.org/abs/2401.09718",
    "title": "AI and the Opportunity for Shared Prosperity: Lessons from the History of Technology and the Economy",
    "published_date": "2024-01-18",
    "abstract": "Recent progress in artificial intelligence (AI) marks a pivotal moment in human history. It presents the opportunity for machines to learn, adapt, and perform tasks that have the potential to assist people, from everyday activities to their most creative and ambitious projects. It also has the potential to help businesses and organizations harness knowledge, increase productivity, innovate, transform, and power shared prosperity. This tremendous potential raises two fundamental questions: (1) Will AI actually advance national and global economic transformation to benefit society at large? and (2) What issues must we get right to fully realize AI's economic value, expand prosperity and improve lives everywhere? We explore these questions by considering the recent history of technology and innovation as a guide for the likely impact of AI and what we must do to realize its economic potential to benefit society. While we do not presume the future will be entirely like that past, for reasons we will discuss, we do believe prior experience with technological change offers many useful lessons. We conclude that while progress in AI presents a historic opportunity to advance our economic prosperity and future wellbeing, its economic benefits will not come automatically and that AI risks exacerbating existing economic challenges unless we collectively and purposefully act to enable its potential and address its challenges. We suggest a collective policy agenda - involving developers, deployers and users of AI, infrastructure providers, policymakers, and those involved in workforce training - that may help both realize and harness AI's economic potential and address its risks to our shared prosperity.",
    "citation_count": 1,
    "summary": "This paper argues that AI's potential for shared prosperity is substantial but not guaranteed, requiring proactive policy interventions to address potential risks and ensure equitable distribution of its economic benefits. Lessons from past technological advancements highlight the need for a collaborative approach involving all stakeholders to harness AI's potential while mitigating its challenges."
  },
  {
    "url": "https://arxiv.org/pdf/2402.08797.pdf",
    "title": "Computing Power and the Governance of Artificial Intelligence",
    "published_date": "2024-02-13",
    "abstract": "Computing power, or\"compute,\"is crucial for the development and deployment of artificial intelligence (AI) capabilities. As a result, governments and companies have started to leverage compute as a means to govern AI. For example, governments are investing in domestic compute capacity, controlling the flow of compute to competing countries, and subsidizing compute access to certain sectors. However, these efforts only scratch the surface of how compute can be used to govern AI development and deployment. Relative to other key inputs to AI (data and algorithms), AI-relevant compute is a particularly effective point of intervention: it is detectable, excludable, and quantifiable, and is produced via an extremely concentrated supply chain. These characteristics, alongside the singular importance of compute for cutting-edge AI models, suggest that governing compute can contribute to achieving common policy objectives, such as ensuring the safety and beneficial use of AI. More precisely, policymakers could use compute to facilitate regulatory visibility of AI, allocate resources to promote beneficial outcomes, and enforce restrictions against irresponsible or malicious AI development and usage. However, while compute-based policies and technologies have the potential to assist in these areas, there is significant variation in their readiness for implementation. Some ideas are currently being piloted, while others are hindered by the need for fundamental research. Furthermore, naive or poorly scoped approaches to compute governance carry significant risks in areas like privacy, economic impacts, and centralization of power. We end by suggesting guardrails to minimize these risks from compute governance.",
    "citation_count": 16,
    "summary": "Governments and companies increasingly use control over computing power to govern artificial intelligence development and deployment, leveraging its quantifiable and controllable nature to promote beneficial AI while mitigating risks. However, effective implementation requires careful consideration of potential downsides like privacy violations and power centralization."
  },
  {
    "url": "https://www.lesswrong.com/posts/mXD53GFvMCWQhcCwt/distinguishing-ways-ai-can-be-concentrated",
    "author": "Matthew Barnett",
    "title": "Distinguishing ways AI can be \"concentrated\"",
    "published_date": "2024-10-21",
    "summary": "The author argues that discussions of AI concentration lack clarity, proposing three distinct dimensions: concentration of development, service provision, and control over AI systems. These dimensions are independent and should be analyzed separately to accurately assess the risks and potential trajectories of AI development."
  },
  {
    "url": "https://arxiv.org/pdf/2301.13442.pdf",
    "title": "Scaling laws for single-agent reinforcement learning",
    "published_date": "2023-01-31",
    "abstract": "Recent work has shown that, in generative modeling, cross-entropy loss improves smoothly with model size and training compute, following a power law plus constant scaling law. One challenge in extending these results to reinforcement learning is that the main performance objective of interest, mean episode return, need not vary smoothly. To overcome this, we introduce *intrinsic performance*, a monotonic function of the return defined as the minimum compute required to achieve the given return across a family of models of different sizes. We find that, across a range of environments, intrinsic performance scales as a power law in model size and environment interactions. Consequently, as in generative modeling, the optimal model size scales as a power law in the training compute budget. Furthermore, we study how this relationship varies with the environment and with other properties of the training setup. In particular, using a toy MNIST-based environment, we show that varying the\"horizon length\"of the task mostly changes the coefficient but not the exponent of this relationship.",
    "citation_count": 16,
    "summary": "The paper establishes scaling laws for single-agent reinforcement learning by introducing \"intrinsic performance,\" a monotonic function of episode return, showing that it scales as a power law with model size and training interactions, mirroring generative modeling's scaling behavior. This implies optimal model size also scales as a power law with compute budget, a relationship influenced by environment and task properties."
  },
  {
    "url": "https://www.lesswrong.com/posts/4xGAmZ9GTGAkszHoH/parameter-scaling-comes-for-rl-maybe",
    "author": "1a3orn",
    "title": "Parameter Scaling Comes for RL, Maybe",
    "published_date": "2023-01-24",
    "summary": "Recent DeepMind research demonstrates that, contrary to previous limitations, scaling up the parameters of reinforcement learning (RL) models significantly improves both reward and sample efficiency. This breakthrough, similar to scaling in language models, suggests a potential key advancement for RL and artificial general intelligence."
  },
  {
    "url": "https://arxiv.org/abs/2212.04724",
    "title": "$\\Lambda$-Returns to Scale and Individual Minimum Extrapolation Principle",
    "published_date": "2022-12-09",
    "abstract": "This paper proposes to estimate the returns-to-scale of production sets by considering the individual return of each observed firm through the notion of $\\Lambda$-returns to scale assumption. Along this line, the global technology is then constructed as the intersection of all the individual technologies. Hence, an axiomatic foundation is proposed to present the notion of $\\Lambda$-returns to scale. This new characterization of the returns-to-scale encompasses the definition of $\\alpha$-returns to scale, as a special case as well as the standard non-increasing and non-decreasing returns-to-scale models. A non-parametric procedure based upon the goodness of fit approach is proposed to assess these individual returns-to-scale. To illustrate this notion of $\\Lambda$-returns to scale assumption, an empirical illustration is provided based upon a dataset involving 63 industries constituting the whole American economy over the period 1987-2018.",
    "summary": "This paper introduces a new measure of returns to scale, called $\\Lambda$-returns, which encompasses existing models and is estimated individually for each firm before aggregating to a global technology. A non-parametric goodness-of-fit method is used for estimation and illustrated with an empirical application to the US economy."
  }
]