[
  {
    "url": "http://arxiv.org/abs/2401.09718",
    "title": "AI and the Opportunity for Shared Prosperity: Lessons from the History of Technology and the Economy",
    "published_date": "2024-01-18",
    "abstract": "Recent progress in artificial intelligence (AI) marks a pivotal moment in human history. It presents the opportunity for machines to learn, adapt, and perform tasks that have the potential to assist people, from everyday activities to their most creative and ambitious projects. It also has the potential to help businesses and organizations harness knowledge, increase productivity, innovate, transform, and power shared prosperity. This tremendous potential raises two fundamental questions: (1) Will AI actually advance national and global economic transformation to benefit society at large? and (2) What issues must we get right to fully realize AI's economic value, expand prosperity and improve lives everywhere? We explore these questions by considering the recent history of technology and innovation as a guide for the likely impact of AI and what we must do to realize its economic potential to benefit society. While we do not presume the future will be entirely like that past, for reasons we will discuss, we do believe prior experience with technological change offers many useful lessons. We conclude that while progress in AI presents a historic opportunity to advance our economic prosperity and future wellbeing, its economic benefits will not come automatically and that AI risks exacerbating existing economic challenges unless we collectively and purposefully act to enable its potential and address its challenges. We suggest a collective policy agenda - involving developers, deployers and users of AI, infrastructure providers, policymakers, and those involved in workforce training - that may help both realize and harness AI's economic potential and address its risks to our shared prosperity.",
    "citation_count": 1,
    "summary": "The paper argues that AI's potential for widespread economic prosperity is significant but not guaranteed, requiring proactive policy and collaborative action to mitigate risks and ensure equitable benefits. Lessons from past technological advancements highlight the need for a concerted effort involving various stakeholders to harness AI's potential while addressing its challenges."
  },
  {
    "url": "https://arxiv.org/abs/2406.00392",
    "title": "Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning",
    "published_date": "2024-06-01",
    "abstract": "Cultural accumulation drives the open-ended and diverse progress in capabilities spanning human history. It builds an expanding body of knowledge and skills by combining individual exploration with inter-generational information transmission. Despite its widespread success among humans, the capacity for artificial learning agents to accumulate culture remains under-explored. In particular, approaches to reinforcement learning typically strive for improvements over only a single lifetime. Generational algorithms that do exist fail to capture the open-ended, emergent nature of cultural accumulation, which allows individuals to trade-off innovation and imitation. Building on the previously demonstrated ability for reinforcement learning agents to perform social learning, we find that training setups which balance this with independent learning give rise to cultural accumulation. These accumulating agents outperform those trained for a single lifetime with the same cumulative experience. We explore this accumulation by constructing two models under two distinct notions of a generation: episodic generations, in which accumulation occurs via in-context learning and train-time generations, in which accumulation occurs via in-weights learning. In-context and in-weights cultural accumulation can be interpreted as analogous to knowledge and skill accumulation, respectively. To the best of our knowledge, this work is the first to present general models that achieve emergent cultural accumulation in reinforcement learning, opening up new avenues towards more open-ended learning systems, as well as presenting new opportunities for modelling human culture.",
    "citation_count": 3,
    "summary": "This paper introduces artificial generational intelligence, showing that reinforcement learning agents, by balancing independent learning with social learning across generations, achieve emergent cultural accumulation surpassing single-lifetime learning. Two models are presented, demonstrating in-context (knowledge) and in-weights (skill) cultural accumulation."
  },
  {
    "url": "https://arxiv.org/abs/2405.09679",
    "title": "Simulating Policy Impacts: Developing a Generative Scenario Writing Method to Evaluate the Perceived Effects of Regulation",
    "published_date": "2024-05-15",
    "abstract": "The rapid advancement of AI technologies yields numerous future impacts on individuals and society. Policymakers are tasked to react quickly and establish policies that mitigate those impacts. However, anticipating the effectiveness of policies is a difficult task, as some impacts might only be observable in the future and respective policies might not be applicable to the future development of AI. In this work we develop a method for using large language models (LLMs) to evaluate the efficacy of a given piece of policy at mitigating specified negative impacts. We do so by using GPT-4 to generate scenarios both pre- and post-introduction of policy and translating these vivid stories into metrics based on human perceptions of impacts. We leverage an already established taxonomy of impacts of generative AI in the media environment to generate a set of scenario pairs both mitigated and non-mitigated by the transparency policy in Article 50 of the EU AI Act. We then run a user study (n=234) to evaluate these scenarios across four risk-assessment dimensions: severity, plausibility, magnitude, and specificity to vulnerable populations. We find that this transparency legislation is perceived to be effective at mitigating harms in areas such as labor and well-being, but largely ineffective in areas such as social cohesion and security. Through this case study we demonstrate the efficacy of our method as a tool to iterate on the effectiveness of policy for mitigating various negative impacts. We expect this method to be useful to researchers or other stakeholders who want to brainstorm the potential utility of different pieces of policy or other mitigation strategies.",
    "citation_count": 1,
    "summary": "This paper presents a novel method using large language models (LLMs) to simulate and evaluate the perceived effectiveness of policies aimed at mitigating negative impacts of AI, demonstrated through a user study assessing the EU AI Act's Article 50 transparency policy. The results reveal varying perceived effectiveness across different impact areas, highlighting the method's utility for policy iteration and impact assessment."
  },
  {
    "url": "https://www.lesswrong.com/posts/324pQjqoHEHeF2vPs/ai-clarity-an-initial-research-agenda",
    "author": "Justin Bullock, Corin Katzke, Zershaaneh Qureshi, David_Kristoffersson",
    "title": "AI Clarity: An Initial Research Agenda",
    "published_date": "2024-05-03",
    "summary": "The AI Clarity research program uses scenario planning to explore potential pathways to existential risks posed by transformative AI (TAI), particularly those emerging within the next decade. The program aims to evaluate strategies for mitigating these risks across various scenarios, aiming to clarify uncertainties surrounding TAI's future."
  },
  {
    "url": "http://arxiv.org/abs/2401.05377",
    "title": "The impact of generative artificial intelligence on socioeconomic inequalities and policy making",
    "published_date": "2023-12-16",
    "abstract": "Abstract Generative artificial intelligence (AI) has the potential to both exacerbate and ameliorate existing socioeconomic inequalities. In this article, we provide a state-of-the-art interdisciplinary overview of the potential impacts of generative AI on (mis)information and three information-intensive domains: work, education, and healthcare. Our goal is to highlight how generative AI could worsen existing inequalities while illuminating how AI may help mitigate pervasive social problems. In the information domain, generative AI can democratize content creation and access but may dramatically expand the production and proliferation of misinformation. In the workplace, it can boost productivity and create new jobs, but the benefits will likely be distributed unevenly. In education, it offers personalized learning, but may widen the digital divide. In healthcare, it might improve diagnostics and accessibility, but could deepen pre-existing inequalities. In each section, we cover a specific topic, evaluate existing research, identify critical gaps, and recommend research directions, including explicit trade-offs that complicate the derivation of a priori hypotheses. We conclude with a section highlighting the role of policymaking to maximize generative AI's potential to reduce inequalities while mitigating its harmful effects. We discuss strengths and weaknesses of existing policy frameworks in the European Union, the United States, and the United Kingdom, observing that each fails to fully confront the socioeconomic challenges we have identified. We propose several concrete policies that could promote shared prosperity through the advancement of generative AI. This article emphasizes the need for interdisciplinary collaborations to understand and address the complex challenges of generative AI.",
    "citation_count": 37,
    "summary": "Generative AI's impact on socioeconomic inequality is complex, potentially exacerbating disparities in information access, work, education, and healthcare while also offering opportunities for mitigation. Effective policy interventions are crucial to harness AI's benefits and minimize its harms, requiring interdisciplinary collaboration and addressing shortcomings in existing regulatory frameworks."
  },
  {
    "url": "https://arxiv.org/abs/2305.02561",
    "title": "Beneficence Signaling in AI Development Dynamics",
    "published_date": "2023-05-04",
    "abstract": "This paper motivates and develops a framework for understanding how the socio-technical systems surrounding AI development interact with social welfare. It introduces the concept of ``signaling'' from evolutionary game theory and demonstrates how it can enhance existing theory and practice surrounding the evaluation and governance of AI systems.",
    "summary": "This paper proposes a framework using signaling theory from evolutionary game theory to analyze how AI development's socio-technical systems impact social welfare. It argues that this framework improves the evaluation and governance of AI systems."
  },
  {
    "url": "http://arxiv.org/abs/2312.14804",
    "title": "Using large language models to promote health equity",
    "published_date": "2023-12-22",
    "abstract": "Advances in large language models (LLMs) have driven an explosion of interest about their societal impacts. Much of the discourse around how they will impact social equity has been cautionary or negative, focusing on questions like\"how might LLMs be biased and how would we mitigate those biases?\"This is a vital discussion: the ways in which AI generally, and LLMs specifically, can entrench biases have been well-documented. But equally vital, and much less discussed, is the more opportunity-focused counterpoint:\"what promising applications do LLMs enable that could promote equity?\"If LLMs are to enable a more equitable world, it is not enough just to play defense against their biases and failure modes. We must also go on offense, applying them positively to equity-enhancing use cases to increase opportunities for underserved groups and reduce societal discrimination. There are many choices which determine the impact of AI, and a fundamental choice very early in the pipeline is the problems we choose to apply it to. If we focus only later in the pipeline -- making LLMs marginally more fair as they facilitate use cases which intrinsically entrench power -- we will miss an important opportunity to guide them to equitable impacts. Here, we highlight the emerging potential of LLMs to promote equity by presenting four newly possible, promising research directions, while keeping risks and cautionary points in clear view.",
    "citation_count": 3,
    "summary": "This paper argues that while mitigating bias in LLMs is crucial, equitable outcomes require proactively developing LLM applications that directly address the needs of underserved groups and reduce societal discrimination. The authors propose four promising research directions to achieve this."
  },
  {
    "url": "https://www.lesswrong.com/posts/yT22RcWrxZcXyGjsA/how-to-have-polygenically-screened-children",
    "author": "GeneSmith",
    "title": "How to have Polygenically Screened Children",
    "published_date": "2023-05-07",
    "summary": "Polygenic embryo screening allows parents to select embryos with a reduced risk of common diseases and higher IQs, though the effects are modest and the procedure is expensive (around $30,000-$100,000). This process involves in-vitro fertilization (IVF) and selecting the most favorable embryo based on genetic testing."
  }
]