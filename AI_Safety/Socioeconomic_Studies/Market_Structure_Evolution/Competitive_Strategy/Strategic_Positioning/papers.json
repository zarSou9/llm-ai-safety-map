[
  {
    "url": "https://www.alignmentforum.org/posts/6nNwMbdRXZDuNd4Gx/analysis-of-global-ai-governance-strategies",
    "author": "Sammy Martin, Justin Bullock, Corin Katzke",
    "title": "Analysis of Global AI Governance Strategies",
    "published_date": "2024-12-04",
    "summary": "The article analyzes three strategies for governing transformative AI: Cooperative Development, Strategic Advantage, and Global Moratorium, evaluating their effectiveness based on alignment difficulty and development timelines. The optimal strategy shifts depending on these factors, with Cooperative Development preferred for longer timelines and easier alignment, Strategic Advantage for shorter timelines or moderate difficulty, and Global Moratorium only considered necessary under extremely challenging conditions."
  },
  {
    "url": "https://www.lesswrong.com/posts/BCynDEwguEiogicAo/reflection-of-hierarchical-relationship-via-nuanced?commentId=LxbpnsnaqWD3xEwZc",
    "author": "Kyoung-cheol Kim",
    "title": "Reflection of Hierarchical Relationship via Nuanced Conditioning of Game Theory Approach for AI Development and Utilization - LessWrong",
    "published_date": "2023-02-07",
    "summary": "This article examines the application of game theory to AI development within organizational structures, highlighting both its strengths in multi-agent systems and limitations regarding complex decision-making. It argues that bureaucratic principles, including hierarchical authority and job specialization, remain crucial even with advanced AI, emphasizing the continued necessity of human-AI collaboration to achieve complex goals."
  },
  {
    "url": "https://www.alignmentforum.org/s/HBMLmW9WsgsdZWg4R",
    "author": "Edouard Harris, Simon Suo",
    "title": "Experiments in instrumental convergence - AI Alignment Forum",
    "published_date": "2022-10-12",
    "summary": "Researchers used multi-agent reinforcement learning to study whether faster-learning AIs inherently compete with humans for resources, investigating the concept of instrumental convergence and power-seeking. Experiments explored whether superior learning speed in AI leads to inherent competition."
  },
  {
    "url": "https://www.lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very",
    "author": "HoldenKarnofsky",
    "title": "How might we align transformative AI if it's developed very soon?",
    "published_date": "2022-08-29",
    "summary": "This article explores strategies for aligning transformative AI, assuming its imminent development by a major AI company (\"Magma\"). It focuses on Magma's predicament of mitigating risks from its own AI and those of less cautious competitors, outlining potential alignment techniques and crucial considerations like accurate reinforcement, robustness, exploit prevention, and ongoing testing."
  },
  {
    "url": "https://www.lesswrong.com/posts/BCynDEwguEiogicAo/reflection-of-hierarchical-relationship-via-nuanced",
    "author": "Kyoung-cheol Kim",
    "title": "Reflection of Hierarchical Relationship via Nuanced Conditioning of Game Theory Approach for AI Development and Utilization",
    "published_date": "2021-06-04",
    "summary": "This article explores applying game theory to AI development within organizational structures, highlighting the limitations of a purely game-theoretic approach and emphasizing the enduring relevance of bureaucratic principles like hierarchical authority and job specialization, even with the integration of AI agents. The authors argue that human-AI collaboration, leveraging comparative advantage, will remain crucial for complex problem-solving, necessitating ongoing organizational structures."
  },
  {
    "url": "https://www.alignmentforum.org/s/57bsaXbJXbzKqNkrf",
    "author": "Mark Xu",
    "title": "Intermittent Distllations - AI Alignment Forum",
    "published_date": "2021-04-14",
    "summary": "The publication summarizes AI safety-related content, reflecting the principle that careful reading should be followed by summarization."
  },
  {
    "url": "https://www.alignmentforum.org/posts/CtGwGgxfoefiwfcor/disentangling-perspectives-on-strategy-stealing-in-ai-safety",
    "author": "shawnghu",
    "title": "Disentangling Perspectives On Strategy-Stealing in AI Safety",
    "published_date": "2021-12-18",
    "summary": "This article clarifies the often-misunderstood \"strategy-stealing\" argument from game theory, explaining its limited direct applicability to AI safety. It aims to refine the concept's use in AI alignment discussions, focusing on its value as a tool for managing intuitions about competition rather than a literal strategy-copying mechanism."
  },
  {
    "url": "https://www.lesswrong.com/posts/G4KHuYC3pHry6yMhi/compute-research-questions-and-metrics-transformative-ai-and",
    "author": "lennart",
    "title": "Compute Research Questions and Metrics - Transformative AI and Compute [4/4]",
    "published_date": "2021-11-28",
    "summary": "This appendix to a series on transformative AI and compute offers research questions on AI hardware trends, analyzes compute metrics and their limitations, and lists AI hardware startups. It also explores the relationship between compute spending, hardware performance, and the scaling of AI models."
  }
]