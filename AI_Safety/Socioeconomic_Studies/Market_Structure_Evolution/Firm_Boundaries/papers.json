[
  {
    "url": "https://arxiv.org/abs/2405.18434",
    "title": "Modeling the Feedback of AI Price Estimations on Actual Market Values",
    "published_date": "2024-03-13",
    "abstract": "Public availability of Artificial Intelligence generated information can change the markets forever, and its factoring into economical dynamics may take economists by surprise, out-dating models and schools of thought. Real estate hyper-inflation is not a new phenomenon but its consistent and almost monotonous persistence over 12 years, coinciding with prominence of public estimation information from Zillow, a successful Mass Real Estate Estimator (MREE), could not escape unobserved. What we model is a repetitive theoretical game between the MREE and the home owners, where each player has secret information and expertise. If the intention is to keep housing affordable and maintain old American lifestyle with broad home-ownership, new challenges are defined. Simulations show that a simple restriction of MREE-style price estimation availability to opt-in properties may help partially reduce feedback loop by acting on its likely causes, as suggested by experimental simulation models. The conjecture that the MREE pressure on real estate inflation rate is correlated with the absolute MREE estimation errors, which is logically explainable, is then validated in simulations.",
    "summary": "This paper models the feedback loop between AI-driven real estate price estimations (like Zillow's) and actual market values, finding that readily available estimations contribute to inflation. Simulations suggest restricting access to estimations to opt-in properties could mitigate this effect."
  },
  {
    "url": "https://www.lesswrong.com/posts/mXD53GFvMCWQhcCwt/distinguishing-ways-ai-can-be-concentrated",
    "author": "Matthew Barnett",
    "title": "Distinguishing ways AI can be \"concentrated\"",
    "published_date": "2024-10-21",
    "summary": "The author argues that discussions of AI concentration lack clarity, proposing three distinct dimensions: concentration of AI development, service provision, and control. These dimensions are independent and should be analyzed separately to accurately predict AI's trajectory and inform policy."
  },
  {
    "url": "https://arxiv.org/abs/2302.09438",
    "title": "Does Machine Learning Amplify Pricing Errors in the Housing Market? - The Economics of Machine Learning Feedback Loops",
    "published_date": "2023-02-18",
    "abstract": "Machine learning algorithms are increasingly employed to price or value homes for sale, properties for rent, rides for hire, and various other goods and services. Machine learning-based prices are typically generated by complex algorithms trained on historical sales data. However, displaying these prices to consumers anchors the realized sales prices, which will in turn become training samples for future iterations of the algorithms. The economic implications of this machine learning\"feedback loop\"- an indirect human-algorithm interaction - remain relatively unexplored. In this work, we develop an analytical model of machine learning feedback loops in the context of the housing market. We show that feedback loops lead machine learning algorithms to become overconfident in their own accuracy (by underestimating its error), and leads home sellers to over-rely on possibly erroneous algorithmic prices. As a consequence at the feedback loop equilibrium, sale prices can become entirely erratic (relative to true consumer preferences in absence of ML price interference). We then identify conditions (choice of ML models, seller characteristics and market characteristics) where the economic payoffs for home sellers at the feedback loop equilibrium is worse off than no machine learning. We also empirically validate primitive building blocks of our analytical model using housing market data from Zillow. We conclude by prescribing algorithmic corrective strategies to mitigate the effects of machine learning feedback loops, discuss the incentives for platforms to adopt these strategies, and discuss the role of policymakers in regulating the same.",
    "summary": "This paper models the feedback loop between machine learning algorithms used for housing price estimations and actual sale prices, demonstrating that this loop can amplify pricing errors, leading to overconfident algorithms and erratic sale prices. The authors identify conditions where this feedback loop harms sellers and propose algorithmic and policy solutions to mitigate the negative effects."
  },
  {
    "url": "https://arxiv.org/pdf/2309.12122.pdf",
    "title": "Buyer-Optimal Algorithmic Consumption",
    "published_date": "2023-09-21",
    "abstract": "An algorithm recommends a product to a buyer based on the product's value to the buyer and its price. We characterize an algorithm that maximizes the buyer's expected payoff and show that it strategically biases recommendations to incentivize lower prices. Under optimal algorithmic consumption, informing a seller about the buyer's value does not affect the buyer's expected payoff but leads to a more equitable distribution of payoffs across different values. These results extend to Pareto-optimal algorithms and multiseller markets.",
    "citation_count": 2,
    "summary": "This paper introduces a buyer-optimal algorithm for product recommendations that strategically biases suggestions to negotiate lower prices, maximizing the buyer's expected payoff. The algorithm's strategic information disclosure doesn't impact the buyer's overall payoff but improves fairness in profit distribution."
  },
  {
    "url": "https://www.lesswrong.com/posts/BCynDEwguEiogicAo/reflection-of-hierarchical-relationship-via-nuanced?commentId=LxbpnsnaqWD3xEwZc",
    "author": "Kyoung-cheol Kim",
    "title": "Reflection of Hierarchical Relationship via Nuanced Conditioning of Game Theory Approach for AI Development and Utilization - LessWrong",
    "published_date": "2023-02-07",
    "summary": "The article explores applying game theory to AI development within organizational structures, highlighting the limitations of a purely game-theoretic approach and advocating for a model that integrates bureaucratic principles like hierarchical authority and job specialization. This integrated model acknowledges the comparative advantages of both human and AI agents in achieving shared goals through collaborative, systematic processes."
  },
  {
    "url": "https://arxiv.org/abs/2211.00496",
    "title": "Can maker-taker fees prevent algorithmic cooperation in market making?",
    "published_date": "2022-10-26",
    "abstract": "In a semi-realistic market simulator, independent reinforcement learning algorithms may facilitate market makers to maintain wide spreads even without communication. This unexpected outcome challenges the current antitrust law framework. We study the effectiveness of maker-taker fee models in preventing cooperation via algorithms. After modeling market making as a repeated general-sum game, we experimentally show that the relation between net transaction costs and maker rebates is not necessarily monotone. Besides an upper bound on taker fees, we may also need a lower bound on maker rebates to destabilize the cooperation. We also consider the taker-maker model and the effects of mid-price volatility, inventory risk, and the number of agents.",
    "citation_count": 1,
    "summary": "Reinforcement learning algorithms in a simulated market can lead to unexpectedly wide spreads maintained by cooperating market makers, even without explicit communication, challenging antitrust frameworks. Maker-taker fee models' effectiveness in preventing this algorithmic cooperation depends on a complex interplay of maker rebates and taker fees, requiring potentially both upper and lower bounds for destabilization."
  },
  {
    "url": "https://arxiv.org/abs/2202.05947",
    "title": "Artificial Intelligence and Auction Design",
    "published_date": "2022-02-12",
    "abstract": "Motivated by online advertising auctions, we study auction design in repeated auctions played by simple Artificial Intelligence algorithms (Q-learning). We find that first-price auctions with no additional feedback lead to tacit-collusive outcomes (bids lower than values), while second-price auctions do not. We show that the difference is driven by the incentive in first-price auctions to outbid opponents by just one bid increment. This facilitates re-coordination on low bids after a phase of experimentation. We also show that providing information about the lowest bid to win, as introduced by Google at the time of the switch to first-price auctions, increases competitiveness of auctions.",
    "citation_count": 26,
    "summary": "In repeated first-price auctions with simple AI agents, tacit collusion emerges due to the incentive to minimally outbid competitors, unlike in second-price auctions; revealing the winning bid's value mitigates this collusive behavior."
  },
  {
    "title": "Market Making Strategy Optimization via Deep Reinforcement Learning",
    "abstract": "Optimization of market making strategy is a vital issue for participants in security markets. Traditional strategies are mostly designed manually, and orders are mechanically issued according to rules based on predefined market conditions. On one hand, market conditions cannot be well represented by arbitrarily defined indicators, and on the other hand, rule-based strategies cannot fully capture relations between the market conditions and strategies' actions. Therefore, it is worthwhile to investigate how to incorporate deep reinforcement learning model to address those issues. In this paper, we propose an end-to-end deep reinforcement learning market making model, i.e., Deep Reinforcement Learning Market Making. It exploits long short-term memory network to extract temporal patterns of the market directly from limit order books, and it learns state-action relations via a reinforcement learning approach. In order to control inventory risk and information asymmetry, a deep Q-network is introduced to adaptively select different action subsets and train the market making agent according to the inventory states. Experiments are conducted on a six-month Level-2 data set, including 10 stock, from Shanghai Stock Exchange in China. Our model is compared with a conventional market making baseline and a state-of-the-art market making model. Experimental results show that our approach outperforms the benchmarks over 10 stocks by at least 10.63%.",
    "published_date": "2022-01-01",
    "citation_count": 8,
    "url": "https://ieeexplore.ieee.org/ielx7/6287639/9668973/09682687.pdf",
    "summary": "This paper proposes Deep Reinforcement Learning Market Making (DRLMM), an end-to-end deep reinforcement learning model that uses LSTM networks to learn temporal patterns from order book data and a deep Q-network to manage inventory risk, outperforming traditional and state-of-the-art market making strategies by at least 10.63% in experiments on Shanghai Stock Exchange data."
  },
  {
    "url": "https://www.alignmentforum.org/posts/8oMF8Lv5jiGaQSFvo/boundaries-part-1-a-key-missing-concept-from-utility-theory",
    "author": "Andrew_Critch",
    "title": "«Boundaries», Part 1: a key missing concept from utility theory",
    "published_date": "2022-07-26",
    "summary": "This LessWrong post introduces the concept of \"boundaries\" – physical or cognitive separations of living systems from their environment – as a missing element in game and bargaining theories. The author argues that incorporating boundaries, exemplified by cell membranes, national borders, or social norms, is crucial for understanding multi-agent rationality and has broader implications for effective altruism and x-risk."
  },
  {
    "url": "https://arxiv.org/pdf/2105.10099v1.pdf",
    "title": "Learning from Zero: How to Make Consumption-Saving Decisions in a Stochastic Environment with an AI Algorithm",
    "published_date": "2021-05-21",
    "abstract": "This exercise offers an innovative learning mechanism to model economic agent's decision-making process using a deep reinforcement learning algorithm. In particular, this AI agent is born in an economic environment with no information on the underlying economic structure and its own preference. I model how the AI agent learns from square one in terms of how it collects and processes information. It is able to learn in real time through constantly interacting with the environment and adjusting its actions accordingly (i.e., online learning). I illustrate that the economic agent under deep reinforcement learning is adaptive to changes in a given environment in real time. AI agents differ in their ways of collecting and processing information, and this leads to different learning behaviours and welfare distinctions. The chosen economic structure can be generalised to other decision-making processes and economic models.",
    "citation_count": 6,
    "summary": "This paper uses deep reinforcement learning to model how an AI agent, starting with no prior knowledge, learns to make optimal consumption-saving decisions in a stochastic economic environment through online learning and interaction. The study demonstrates the agent's adaptability and highlights how differing information processing strategies impact learning outcomes and welfare."
  }
]