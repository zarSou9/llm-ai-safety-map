[
  {
    "url": "https://arxiv.org/abs/2402.15821",
    "title": "Cooperation and Control in Delegation Games",
    "published_date": "2024-02-24",
    "abstract": "Many settings of interest involving humans and machines – from virtual personal assistants to autonomous vehicles – can naturally be modelled as principals (humans) delegating to agents (machines), which then interact with each other on their principals' behalf. We refer to these multi-principal, multi-agent scenarios as delegation games. In such games, there are two important failure modes: problems of control (where an agent fails to act in line their principal's preferences) and problems of cooperation (where the agents fail to work well together). In this paper we formalise and analyse these problems, further breaking them down into issues of alignment (do the players have similar preferences?) and capabilities (how competent are the players at satisfying those preferences?). We show – theoretically and empirically – how these measures determine the principals' welfare, how they can be estimated using limited observations, and thus how they might be used to help us design more aligned and cooperative AI systems.",
    "summary": "This paper models human-machine interactions as delegation games, analyzing two key failure modes: control (agents deviating from principals' preferences) and cooperation (agents failing to collaborate effectively). The authors theoretically and empirically investigate how agent alignment and capability affect principal welfare and propose methods for estimating these factors to improve AI system design."
  },
  {
    "url": "http://arxiv.org/abs/2401.13138",
    "title": "Visibility into AI Agents",
    "published_date": "2024-01-23",
    "abstract": "Increased delegation of commercial, scientific, governmental, and personal activities to AI agents—systems capable of pursuing complex goals with limited supervision—may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as visibility, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: agent identifiers, real-time monitoring, and activity logging. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for various actors in the supply chain including hardware and software service providers. Finally, we discuss the implications of our measures for privacy and concentration of power. Further work into understanding the measures and mitigating their negative impacts can help to build a foundation for the governance of AI agents.",
    "citation_count": 12,
    "summary": "This paper examines methods to increase transparency (\"visibility\") in the use of AI agents by analyzing agent identifiers, real-time monitoring, and activity logging, considering their implementation across deployment contexts and implications for privacy and power. The authors propose these measures as crucial for mitigating societal risks associated with increased AI agent autonomy."
  },
  {
    "url": "https://arxiv.org/abs/2406.01722",
    "title": "On Labs and Fabs: Mapping How Alliances, Acquisitions, and Antitrust are Shaping the Frontier AI Industry",
    "published_date": "2024-06-03",
    "abstract": "As frontier AI models advance, policy proposals for safe AI development are gaining increasing attention from researchers and policymakers. This paper explores the current integration in the AI supply chain, focusing on vertical relationships and strategic partnerships among AI labs, cloud providers, chip manufacturers, and lithography companies. It aims to lay the groundwork for a deeper understanding of the implications of various governance interventions, including antitrust measures. The study has two main contributions. First, it profiles 25 leading companies in the AI supply chain, analyzing 300 relationships and noting 80 significant mergers and acquisitions along with 40 antitrust cases. Second, we discuss potential market definitions and the integration drivers based on the observed trends. The analysis reveals predominant horizontal integration through natural growth rather than acquisitions and notable trends of backward vertical integration in the semiconductor supply chain. Strategic partnerships are also significant downstream, especially between AI companies and cloud providers, with large tech companies often pursuing conglomerate integration by acquiring specialized AI startups or forming alliances with frontier AI labs. To further understand the strategic partnerships in the industry, we provide three brief case studies featuring companies like OpenAI and Nvidia. We conclude by posing open research questions on market dynamics and possible governance interventions, such as licensing and safety audits.",
    "summary": "This paper analyzes the evolving AI supply chain, mapping relationships between AI labs, cloud providers, and hardware manufacturers through mergers, acquisitions, and alliances, to inform policy discussions on antitrust and AI governance. The analysis reveals prevalent horizontal integration and significant backward vertical integration in the semiconductor industry, highlighting the strategic importance of partnerships."
  },
  {
    "url": "https://arxiv.org/pdf/2402.08797.pdf",
    "title": "Computing Power and the Governance of Artificial Intelligence",
    "published_date": "2024-02-13",
    "abstract": "Computing power, or\"compute,\"is crucial for the development and deployment of artificial intelligence (AI) capabilities. As a result, governments and companies have started to leverage compute as a means to govern AI. For example, governments are investing in domestic compute capacity, controlling the flow of compute to competing countries, and subsidizing compute access to certain sectors. However, these efforts only scratch the surface of how compute can be used to govern AI development and deployment. Relative to other key inputs to AI (data and algorithms), AI-relevant compute is a particularly effective point of intervention: it is detectable, excludable, and quantifiable, and is produced via an extremely concentrated supply chain. These characteristics, alongside the singular importance of compute for cutting-edge AI models, suggest that governing compute can contribute to achieving common policy objectives, such as ensuring the safety and beneficial use of AI. More precisely, policymakers could use compute to facilitate regulatory visibility of AI, allocate resources to promote beneficial outcomes, and enforce restrictions against irresponsible or malicious AI development and usage. However, while compute-based policies and technologies have the potential to assist in these areas, there is significant variation in their readiness for implementation. Some ideas are currently being piloted, while others are hindered by the need for fundamental research. Furthermore, naive or poorly scoped approaches to compute governance carry significant risks in areas like privacy, economic impacts, and centralization of power. We end by suggesting guardrails to minimize these risks from compute governance.",
    "citation_count": 16,
    "summary": "Governments and companies increasingly use control over computing power to govern artificial intelligence development and deployment, leveraging its quantifiable and controllable nature to promote beneficial AI use and mitigate risks; however, careful consideration of potential negative impacts on privacy, economics, and power centralization is crucial."
  },
  {
    "url": "https://arxiv.org/abs/2307.04699",
    "title": "International Institutions for Advanced AI",
    "published_date": "2023-07-10",
    "abstract": "International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.",
    "citation_count": 12,
    "summary": "This paper proposes four international institutional models—a commission, governance organization, collaborative, and safety project—to manage the risks and benefits of advanced AI, focusing on facilitating international cooperation, setting safety standards, and promoting equitable access to AI technology. These models aim to address global externalities and ensure advanced AI benefits humanity."
  },
  {
    "url": "https://www.lesswrong.com/posts/BCynDEwguEiogicAo/reflection-of-hierarchical-relationship-via-nuanced?commentId=LxbpnsnaqWD3xEwZc",
    "author": "Kyoung-cheol Kim",
    "title": "Reflection of Hierarchical Relationship via Nuanced Conditioning of Game Theory Approach for AI Development and Utilization - LessWrong",
    "published_date": "2023-02-07",
    "summary": "This article examines the application of game theory to AI development within organizational structures, highlighting its limitations and proposing a model integrating AI agents into bureaucratic systems. It argues that even with advanced AI, human-AI collaboration within a hierarchical structure, leveraging comparative advantage, remains necessary for efficient complex problem-solving."
  },
  {
    "url": "https://www.alignmentforum.org/tag/coordination-cooperation",
    "author": "Andrew Critch",
    "title": "Coordination / Cooperation - AI Alignment Forum",
    "published_date": "2023-07-11",
    "summary": "Coordination and cooperation, key game theory concepts, involve multiple actors choosing actions for mutual benefit, overcoming challenges like the Prisoner's Dilemma. Failures in coordination and cooperation pose significant risks, including existential threats like AI risk or nuclear war."
  },
  {
    "url": "https://arxiv.org/pdf/2201.12658.pdf",
    "title": "Learning Intuitive Policies Using Action Features",
    "published_date": "2022-01-29",
    "abstract": "An unaddressed challenge in multi-agent coordination is to enable AI agents to exploit the semantic relationships between the features of actions and the features of observations. Humans take advantage of these relationships in highly intuitive ways. For instance, in the absence of a shared language, we might point to the object we desire or hold up our fingers to indicate how many objects we want. To address this challenge, we investigate the effect of network architecture on the propensity of learning algorithms to exploit these semantic relationships. Across a procedurally generated coordination task, we find that attention-based architectures that jointly process a featurized representation of observations and actions have a better inductive bias for learning intuitive policies. Through fine-grained evaluation and scenario analysis, we show that the resulting policies are human-interpretable. Moreover, such agents coordinate with people without training on any human data.",
    "citation_count": 1,
    "summary": "This paper investigates how network architecture influences the ability of multi-agent systems to learn intuitive, human-interpretable coordination policies by leveraging semantic relationships between action and observation features. Results show that attention-based architectures jointly processing these features are superior, leading to policies readily understood and utilized by humans without prior human data training."
  }
]