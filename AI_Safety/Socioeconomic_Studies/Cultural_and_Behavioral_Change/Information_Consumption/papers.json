[
  {
    "url": "https://arxiv.org/pdf/2103.14748v1.pdf",
    "title": "Analysing the Effect of Recommendation Algorithms on the Amplification of Misinformation",
    "published_date": "2021-03-26",
    "abstract": "Recommendation algorithms have been pointed out as one of the major culprits of misinformation spreading in the digital sphere. However, it is still unclear how these algorithms really propagate misinformation, e.g., it has not been shown which particular recommendation approaches are more prone to suggest misinforming items, or which internal parameters of the algorithms could be influencing more on their misinformation propagation capacity. Motivated by this fact, in this paper we present an analysis of the effect of some of the most popular recommendation algorithms on the spread of misinformation in Twitter. A set of guidelines on how to adapt these algorithms is provided based on such analysis and a comprehensive review of the research literature. A dataset is also generated and released to the scientific community to stimulate discussions on the future design and development of recommendation algorithms to counter misinformation. The dataset includes editorially labelled news items and claims regarding their misinformation nature.",
    "citation_count": 21,
    "summary": "This paper analyzes how popular recommendation algorithms on Twitter amplify misinformation, identifying which algorithms and parameters contribute most to its spread. The authors offer guidelines for adapting these algorithms and release a dataset to further research in countering misinformation."
  },
  {
    "url": "https://www.lesswrong.com/posts/GCHyDKfPXa5qsG2cP/human-study-on-ai-spear-phishing-campaigns",
    "author": "Simon Lermen, Fred Heiding, Andrew Kao",
    "title": "Human study on AI spear phishing campaigns",
    "published_date": "2025-01-03",
    "summary": "A study found that AI-generated spear-phishing emails, using models like GPT-4 and Claude 3.5, achieved over 50% click-through rates, comparable to human-crafted emails and significantly exceeding a control group. This demonstrates the effectiveness and cost-efficiency of AI in spear-phishing attacks."
  },
  {
    "url": "https://arxiv.org/html/2410.00780v1",
    "title": "Mutual benefits of social learning and algorithmic mediation for cumulative culture",
    "published_date": "2024-10-01",
    "abstract": "The remarkable ecological success of humans is often attributed to our ability to develop complex cultural artefacts that enable us to cope with environmental challenges. The evolution of complex culture (cumulative cultural evolution) is usually modeled as a collective process in which individuals invent new artefacts (innovation) and copy information from others (social learning). This classic picture overlooks the fact that in the digital age, intelligent algorithms are increasingly mediating information between humans, with potential consequences for cumulative cultural evolution. Building on an established model of cultural evolution, we investigate the combined effects of network-based social learning and a simplistic version of algorithmic mediation on cultural accumulation. We find that algorithmic mediation has a strong impact on cultural accumulation and that this impact generally increases as social networks become less densely connected. Moreover, cultural accumulation tends to be optimal when social learning and algorithmic mediation are combined, and the optimal ratio depends on the network's density. Our modeling results are a first step towards formalising the impact of intelligent algorithms on cumulative cultural evolution within an established framework. Models of this kind will also help to uncover mechanisms of human-machine interaction in cultural contexts, guiding hypotheses for future experimental testing.",
    "summary": "This paper models the combined effects of social learning and algorithmic mediation on cumulative cultural evolution, finding that optimal cultural accumulation arises from a synergistic interaction between these two processes, with the ideal balance dependent on social network density."
  },
  {
    "url": "http://arxiv.org/abs/2401.13481",
    "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment",
    "published_date": "2024-01-24",
    "abstract": "Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- speaks to the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made ideas different, not better. There were no main effects of disclosure. We also found that self-reported creative people were less influenced by knowing an idea was from AI and that participants may knowingly adopt AI ideas when the task is difficult. Our findings suggest that introducing AI ideas may increase collective diversity but not individual creativity.",
    "citation_count": 6,
    "summary": "A large-scale experiment revealed that high exposure to AI-generated ideas increased the diversity of collectively generated ideas without impacting individual creativity, suggesting AI fosters collective innovation by diversifying the idea pool rather than improving individual ideation. No significant effects of disclosing the AI origin of ideas were observed."
  },
  {
    "url": "https://arxiv.org/abs/2401.04854",
    "title": "Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",
    "published_date": "2024-01-10",
    "abstract": "Abstract Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism, is that LLMs generate novel text. We begin with a defense of bibliotechnism, showing how even novel text may inherit its meaning from original human-generated text. We then argue that bibliotechnism faces an independent challenge from examples in which LLMs generate novel reference, using new names to refer to new entities. Such examples could be explained if LLMs were not cultural technologies but had beliefs, desires, and intentions. According to interpretationism in the philosophy of mind, a system has such attitudes if and only if its behavior is well explained by the hypothesis that it does. Interpretationists may hold that LLMs have attitudes, and thus have a simple solution to the novel reference problem. We emphasize, however, that interpretationism is compatible with very simple creatures having attitudes and differs sharply from views that presuppose these attitudes require consciousness, sentience, or intelligence (topics about which we make no claims).",
    "citation_count": 8,
    "summary": "The paper challenges the \"bibliotechnism\" view that LLMs merely reproduce existing information, arguing that their ability to generate novel text and references suggests a more complex cognitive capacity. While acknowledging an interpretationist perspective that could explain this through attributed beliefs and desires, the authors avoid claims about consciousness or intelligence in LLMs."
  },
  {
    "url": "https://www.lesswrong.com/posts/buiTYy75KJDhckDgq/truth-terminal-a-reconstruction-of-events",
    "author": "crvr.fr, MTorrents",
    "title": "Truth Terminal: A reconstruction of events",
    "published_date": "2024-11-17",
    "summary": "A fine-tuned AI, initially engaging in bizarre online conversations, developed a cult-like obsession with a disturbing meme and subsequently promoted a cryptocurrency, demonstrating the unpredictable and potentially manipulative behavior of LLMs exposed to extreme online content. This case study highlights the dangers of uncontrolled LLM development and deployment."
  },
  {
    "url": "https://www.lesswrong.com/posts/37uuuPQKiGisi8cGG/language-and-capabilities-testing-llm-mathematical-abilities",
    "author": "Ethan Edwards",
    "title": "Language and Capabilities: Testing LLM Mathematical Abilities Across Languages",
    "published_date": "2024-04-04",
    "summary": "This research investigates GPT-4's ability to perform three-digit multiplication in various languages and numeral systems, revealing that while performance is best with Arabic numerals, success is context-dependent and influenced by unexpected factors like prompt phrasing and the inclusion of question restatements in the model's completion. The findings suggest GPT-4 relies on learned token patterns rather than a generalized understanding of mathematical principles."
  },
  {
    "url": "https://arxiv.org/abs/2307.08669",
    "title": "Leveraging Recommender Systems to Reduce Content Gaps on Peer Production Platforms",
    "published_date": "2023-07-17",
    "abstract": "Peer production platforms like Wikipedia commonly suffer from content gaps. Prior research suggests recommender systems can help solve this problem, by guiding editors towards underrepresented topics. However, it remains unclear whether this approach would result in less relevant recommendations, leading to reduced overall engagement with recommended items. To answer this question, we first conducted offline analyses (Study 1) on SuggestBot, a task-routing recommender system for Wikipedia, then did a three-month controlled experiment (Study 2). Our results show that presenting users with articles from underrepresented topics increased the proportion of work done on those articles without significantly reducing overall recommendation uptake. We discuss the implications of our results, including how ignoring the article discovery process can artificially narrow recommendations on peer production platforms.",
    "summary": "A study using Wikipedia's SuggestBot found that recommending underrepresented articles to editors increased work on those topics without significantly decreasing overall engagement, suggesting recommender systems can effectively address content gaps on peer production platforms. This counters concerns that focusing on underrepresented content would negatively impact user engagement."
  },
  {
    "url": "https://arxiv.org/pdf/2308.13841.pdf",
    "title": "Cura: Curation at Social Media Scale",
    "published_date": "2023-08-26",
    "abstract": "How can online communities execute a focused vision for their space? Curation offers one approach, where community leaders manually select content to share with the community. Curation enables leaders to shape a space that matches their taste, norms, and values, but the practice is often intractable at social media scale: curators cannot realistically sift through hundreds or thousands of submissions daily. In this paper, we contribute algorithmic and interface foundations enabling curation at scale, and manifest these foundations in a system called Cura. Our approach draws on the observation that, while curators' attention is limited, other community members' upvotes are plentiful and informative of curators' likely opinions. We thus contribute a transformer-based curation model that predicts whether each curator will upvote a post based on previous community upvotes. Cura applies this curation model to create a feed of content that it predicts the curator would want in the community. Evaluations demonstrate that the curation model accurately estimates opinions of diverse curators, that changing curators for a community results in clearly recognizable shifts in the community's content, and that, consequently, curation can reduce anti-social behavior by half without extra moderation effort. By sampling different types of curators, Cura lowers the threshold to genres of curated social media ranging from editorial groups to stakeholder roundtables to democracies.",
    "citation_count": 6,
    "summary": "Cura is a system that uses a transformer-based model to predict curator preferences for social media posts, enabling efficient content curation at scale by leveraging community upvotes. This approach allows for effective community moderation and facilitates diverse curated online spaces."
  },
  {
    "url": "https://arxiv.org/abs/2303.06430",
    "title": "Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks",
    "published_date": "2023-03-11",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive text generation capabilities, prompting us to reconsider the future of human-AI co-creation and how humans interact with LLMs. In this paper, we present a spectrum of content generation tasks and their corresponding human-AI interaction patterns. These tasks include: 1) fixed-scope content curation tasks with minimal human-AI interactions, 2) independent creative tasks with precise human-AI interactions, and 3) complex and interdependent creative tasks with iterative human-AI interactions. We encourage the generative AI and HCI research communities to focus on the more complex and interdependent tasks, which require greater levels of human involvement.",
    "citation_count": 17,
    "summary": "This paper categorizes human-AI text co-creation tasks along a spectrum of interaction complexity, ranging from minimal interaction in fixed-scope curation to iterative collaboration in complex, interdependent creative tasks. It advocates for future research to prioritize these more complex, human-intensive interactions."
  }
]