### Mini Description

Analysis of how trust in AI systems develops, evolves, and influences decision-making behaviors, including factors affecting over-reliance and under-reliance

### Description

Trust Dynamics in AI decision-making systems explores how humans develop, maintain, and adjust their trust in artificial intelligence systems over time. This encompasses both the psychological mechanisms underlying trust formation and the contextual factors that influence trust calibration. Key research challenges include understanding how different user populations develop appropriate levels of trust, identifying factors that lead to over-trust or under-trust, and designing interventions to promote well-calibrated trust relationships.

A central focus is the investigation of how system characteristics - including accuracy, transparency, consistency, and error patterns - shape user trust. Researchers examine how different types of AI failures affect trust differently, how trust recovers after system errors, and how users' prior experiences and beliefs influence their trust development. This includes studying how various forms of AI explanations and uncertainty communications impact trust formation and maintenance.

Current research emphasizes understanding the dynamic nature of trust, moving beyond static models to examine how trust evolves through continued interaction. This includes investigating how different learning and adaptation patterns in AI systems affect user trust, how trust transfers across similar systems or contexts, and how organizational or social factors influence individual trust development. Particular attention is paid to preventing both excessive skepticism that limits AI utilization and dangerous over-reliance that leads to automation bias.

### Order

1. Trust_Formation_Mechanisms
2. Calibration_Factors
3. Error_Impact_Patterns
4. Trust_Evolution_Trajectories
5. Intervention_Design
