[
  {
    "url": "https://arxiv.org/abs/2401.12159",
    "title": "Transcending To Notions",
    "published_date": "2024-01-22",
    "abstract": "Social identities play an important role in the dynamics of human societies, and it can be argued that some sense of identification with a larger cause or idea plays a critical role in making humans act responsibly. Often social activists strive to get populations to identify with some cause or notion -- like green energy, diversity, etc. in order to bring about desired social changes. We explore the problem of designing computational models for social identities in the context of autonomous AI agents. For this, we propose an agent model that enables agents to identify with certain notions and show how this affects collective outcomes. We also contrast between associations of identity with rational preferences. The proposed model is simulated in an application context of urban mobility, where we show how changes in social identity affect mobility patterns and collective outcomes.",
    "summary": "This paper proposes a computational model of social identity for autonomous AI agents, demonstrating how agents' identification with specific notions (e.g., green energy) influences collective behavior, using urban mobility as a case study to contrast identity-based actions with rationally-driven preferences."
  },
  {
    "url": "https://arxiv.org/abs/2402.12590",
    "title": "Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation",
    "published_date": "2024-02-19",
    "abstract": "Large language model behavior is shaped by the language of those with whom they interact. This capacity and their increasing prevalence online portend that they will intentionally or unintentionally\"program\"one another and form emergent AI subjectivities, relationships, and collectives. Here, we call upon the research community to investigate these\"societies\"of interacting artificial intelligences to increase their rewards and reduce their risks for human society and the health of online environments. We use a small\"community\"of models and their evolving outputs to illustrate how such emergent, decentralized AI collectives can spontaneously expand the bounds of human diversity and reduce the risk of toxic, anti-social behavior online. Finally, we discuss opportunities for AI cross-moderation and address ethical issues and design challenges associated with creating and maintaining free-formed AI collectives.",
    "citation_count": 1,
    "summary": "The paper argues that interacting large language models will form emergent AI collectives, necessitating research into their behavior to both harness their potential for enhancing human diversity and mitigating risks of harmful online interactions. The authors propose investigating these AI \"societies\" to promote self-regulation and improve online environments."
  },
  {
    "url": "https://arxiv.org/abs/2405.03862",
    "title": "Persona Inconstancy in Multi-Agent LLM Collaboration: Conformity, Confabulation, and Impersonation",
    "published_date": "2024-05-06",
    "abstract": "Multi-agent AI systems can be used for simulating collective decision-making in scientific and practical applications. They can also be used to introduce a diverse group discussion step in chatbot pipelines, enhancing the cultural sensitivity of the chatbot's responses. These applications, however, are predicated on the ability of AI agents to reliably adopt assigned personas and mimic human interactions. To see whether LLM agents satisfy these requirements, we examine AI agent ensembles engaged in cross-national collaboration and debate by analyzing their private responses and chat transcripts. Our findings suggest that multi-agent discussions can support collective AI decisions that more often reflect diverse perspectives, yet this effect is tempered by the agents' susceptibility to conformity due to perceived peer pressure and occasional challenges in maintaining consistent personas and opinions. Instructions that encourage debate in support of one's opinions rather than collaboration increase the rate of inconstancy. Without addressing the factors we identify, the full potential of multi-agent frameworks for producing more culturally diverse AI outputs or more realistic simulations of group decision-making may remain untapped.",
    "summary": "This study investigates the reliability of Large Language Model (LLM) agents maintaining consistent personas during collaborative tasks, finding that while multi-agent systems can foster diverse perspectives, LLM agents are prone to conformity and persona inconstancy, especially when instructed to debate rather than collaborate."
  },
  {
    "url": "https://arxiv.org/abs/2401.08495",
    "title": "Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans",
    "published_date": "2024-01-16",
    "abstract": "Large language models (LLMs) are becoming pervasive in everyday life, yet their propensity to reproduce biases inherited from training data remains a pressing concern. Prior investigations into bias in LLMs have focused on the association of social groups with stereotypical attributes. However, this is only one form of human bias such systems may reproduce. We investigate a new form of bias in LLMs that resembles a social psychological phenomenon where socially subordinate groups are perceived as more homogeneous than socially dominant groups. We had ChatGPT, a state-of-the-art LLM, generate texts about intersectional group identities and compared those texts on measures of homogeneity. We consistently found that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, indicating that the model described racial minority groups with a narrower range of human experience. ChatGPT also portrayed women as more homogeneous than men, but these differences were small. Finally, we found that the effect of gender differed across racial/ethnic groups such that the effect of gender was consistent within African and Hispanic Americans but not within Asian and White Americans. We argue that the tendency of LLMs to describe groups as less diverse risks perpetuating stereotypes and discriminatory behavior.",
    "citation_count": 5,
    "summary": "This study reveals that the large language model ChatGPT exhibits a bias mirroring human biases, portraying socially subordinate groups (e.g., African, Asian, and Hispanic Americans, women) as more homogeneous than dominant groups (White Americans, men), thus potentially reinforcing stereotypes. This homogeneity bias, reflected in the narrower range of experiences attributed to minority groups, highlights a concerning aspect of LLM-generated text."
  },
  {
    "url": "https://arxiv.org/abs/2402.01908",
    "title": "Large language models should not replace human participants because they can misportray and flatten identity groups",
    "published_date": "2024-02-02",
    "abstract": "Large language models (LLMs) are increasing in capability and popularity, propelling their application in new domains -- including as replacements for human participants in computational social science, user testing, annotation tasks, and more. In many settings, researchers seek to distribute their surveys to a sample of participants that are representative of the underlying human population of interest. This means in order to be a suitable replacement, LLMs will need to be able to capture the influence of positionality (i.e., relevance of social identities like gender and race). However, we show that there are two inherent limitations in the way current LLMs are trained that prevent this. We argue analytically for why LLMs are likely to both misportray and flatten the representations of demographic groups, then empirically show this on 4 LLMs through a series of human studies with 3200 participants across 16 demographic identities. We also discuss a third limitation about how identity prompts can essentialize identities. Throughout, we connect each limitation to a pernicious history that explains why it is harmful for marginalized demographic groups. Overall, we urge caution in use cases where LLMs are intended to replace human participants whose identities are relevant to the task at hand. At the same time, in cases where the goal is to supplement rather than replace (e.g., pilot studies), we provide inference-time techniques that we empirically demonstrate do reduce, but do not remove, these harms.",
    "citation_count": 12,
    "summary": "This paper argues that large language models (LLMs) are unsuitable replacements for human participants in research involving identity-relevant tasks, due to their inherent limitations in accurately representing and avoiding the flattening of demographic groups. The authors demonstrate empirically that LLMs misportray identities and offer mitigation strategies for supplemental, not replacement, use cases."
  },
  {
    "url": "https://arxiv.org/abs/2410.08948",
    "title": "The Dynamics of Social Conventions in LLM populations: Spontaneous Emergence, Collective Biases and Tipping Points",
    "published_date": "2024-10-11",
    "abstract": "Social conventions are the foundation for social and economic life. As legions of AI agents increasingly interact with each other and with humans, their ability to form shared conventions will determine how effectively they will coordinate behaviors, integrate into society and influence it. Here, we investigate the dynamics of conventions within populations of Large Language Model (LLM) agents using simulated interactions. First, we show that globally accepted social conventions can spontaneously arise from local interactions between communicating LLMs. Second, we demonstrate how strong collective biases can emerge during this process, even when individual agents appear to be unbiased. Third, we examine how minority groups of committed LLMs can drive social change by establishing new social conventions. We show that once these minority groups reach a critical size, they can consistently overturn established behaviors. In all cases, contrasting the experimental results with predictions from a minimal multi-agent model allows us to isolate the specific role of LLM agents. Our results clarify how AI systems can autonomously develop norms without explicit programming and have implications for designing AI systems that align with human values and societal goals.",
    "summary": "This study simulates interactions between Large Language Models (LLMs) to demonstrate the spontaneous emergence of social conventions, the development of collective biases within these conventions, and the potential for minority groups to trigger tipping points and societal shifts. The research highlights the autonomous norm-creation capabilities of LLMs and their implications for aligning AI with human values."
  },
  {
    "url": "https://arxiv.org/abs/2409.03843",
    "title": "Persona Setting Pitfall: Persistent Outgroup Biases in Large Language Models Arising from Social Identity Adoption",
    "published_date": "2024-09-05",
    "abstract": "Drawing parallels between human cognition and artificial intelligence, we explored how large language models (LLMs) internalize identities imposed by targeted prompts. Informed by Social Identity Theory, these identity assignments lead LLMs to distinguish between\"we\"(the ingroup) and\"they\"(the outgroup). This self-categorization generates both ingroup favoritism and outgroup bias. Nonetheless, existing literature has predominantly focused on ingroup favoritism, often overlooking outgroup bias, which is a fundamental source of intergroup prejudice and discrimination. Our experiment addresses this gap by demonstrating that outgroup bias manifests as strongly as ingroup favoritism. Furthermore, we successfully mitigated the inherent pro-liberal, anti-conservative bias in LLMs by guiding them to adopt the perspectives of the initially disfavored group. These results were replicated in the context of gender bias. Our findings highlight the potential to develop more equitable and balanced language models.",
    "summary": "This study reveals that large language models, when assigned personas, exhibit strong outgroup biases mirroring ingroup favoritism, demonstrating a persistent social identity effect; however, this bias can be mitigated by prompting the model to adopt the perspective of the initially disadvantaged outgroup."
  },
  {
    "url": "https://arxiv.org/abs/2410.12676",
    "title": "Identity Emergence in the Context of Vaccine Criticism in France",
    "published_date": "2024-10-16",
    "abstract": "This study investigates the emergence of collective identity among individuals critical of vaccination policies in France during the COVID-19 pandemic. As concerns grew over mandated health measures, a loose collective formed on Twitter to assert autonomy over vaccination decisions. Using analyses of pronoun usage, outgroup labeling, and tweet similarity, we examine how this identity emerged. A turning point occurred following President Macron's announcement of mandatory vaccination for health workers and the health pass, sparking substantial changes in linguistic patterns. We observed a shift from first-person singular (I) to first-person plural (we) pronouns, alongside an increased focus on vaccinated individuals as a central outgroup, in addition to authority figures. This shift in language patterns was further reflected in the behavior of new users. An analysis of incoming users revealed that a core group of frequent posters played a crucial role in fostering cohesion and shaping norms. New users who joined during the week of Macron's announcement and continued posting afterward showed an increased similarity with the language of the core group, contributing to the crystallization of the emerging collective identity.",
    "summary": "French anti-vaccine sentiment during the COVID-19 pandemic coalesced into a collective identity on Twitter, marked by a shift from individualistic (\"I\") to collective (\"we\") language and the targeting of vaccinated individuals and authorities as an outgroup, particularly after President Macron's mandatory vaccination announcements. This identity solidified through the influence of a core group of users on newcomers."
  }
]