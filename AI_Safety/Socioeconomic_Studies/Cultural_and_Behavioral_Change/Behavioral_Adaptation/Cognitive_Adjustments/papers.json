[
  {
    "url": "https://arxiv.org/abs/2406.06051",
    "title": "On the Utility of Accounting for Human Beliefs about AI Intention in Human-AI Collaboration",
    "published_date": "2024-06-10",
    "abstract": "To enable effective human-AI collaboration, merely optimizing AI performance without considering human factors is insufficient. Recent research has shown that designing AI agents that take human behavior into account leads to improved performance in human-AI collaboration. However, a limitation of most existing approaches is their assumption that human behavior remains static, regardless of the AI agent's actions. In reality, humans may adjust their actions based on their beliefs about the AI's intentions, specifically, the subtasks they perceive the AI to be attempting to complete based on its behavior. In this paper, we address this limitation by enabling a collaborative AI agent to consider its human partner's beliefs about its intentions, i.e., what the human partner thinks the AI agent is trying to accomplish, and to design its action plan accordingly to facilitate more effective human-AI collaboration. Specifically, we developed a model of human beliefs that captures how humans interpret and reason about their AI partner's intentions. Using this belief model, we created an AI agent that incorporates both human behavior and human beliefs when devising its strategy for interacting with humans. Through extensive real-world human-subject experiments, we demonstrate that our belief model more accurately captures human perceptions of AI intentions. Furthermore, we show that our AI agent, designed to account for human beliefs over its intentions, significantly enhances performance in human-AI collaboration.",
    "summary": "This paper argues that effective human-AI collaboration requires AI agents to model human beliefs about the AI's intentions, not just human behavior. Experiments demonstrate that an AI incorporating this belief model improves collaboration performance compared to agents that do not."
  },
  {
    "url": "https://arxiv.org/abs/2410.07728",
    "title": "Give Me a Choice: The Consequences of Restricting Choices Through AI-Support for Perceived Autonomy, Motivational Variables, and Decision Performance",
    "published_date": "2024-10-10",
    "abstract": "Design optimizations in human-AI collaboration often focus on cognitive aspects like attention and task load. Drawing on work design literature, we propose that effective human-AI collaboration requires broader consideration of human needs (e.g., autonomy) that affect motivational variables (e.g., meaningfulness). In a simulated drone oversight experiment, participants (N=274, between-subject) faced 10 critical decision-making scenarios with varying levels of choice restrictions with an AI recommending only 1, 2, 4 or all 6 possible actions. Restricting participants to one selectable action improved task performance (with a perfect AI) but significantly reduced perceived autonomy and work meaningfulness, and these effects intensified over time. In conditions with multiple action choices, participants with higher perceived autonomy performed better. The findings underscore the importance of considering motivational factors to design successful long-term human-AI collaboration at work.",
    "summary": "Restricting choices in AI-supported decision-making improved task performance in the short-term but negatively impacted perceived autonomy and work meaningfulness, ultimately hindering long-term performance; optimal human-AI collaboration requires balancing efficiency with user agency."
  },
  {
    "url": "https://arxiv.org/abs/2412.19010",
    "title": "A theory of appropriateness with applications to generative artificial intelligence",
    "published_date": "2024-12-26",
    "abstract": "What is appropriateness? Humans navigate a multi-scale mosaic of interlocking notions of what is appropriate for different situations. We act one way with our friends, another with our family, and yet another in the office. Likewise for AI, appropriate behavior for a comedy-writing assistant is not the same as appropriate behavior for a customer-service representative. What determines which actions are appropriate in which contexts? And what causes these standards to change over time? Since all judgments of AI appropriateness are ultimately made by humans, we need to understand how appropriateness guides human decision making in order to properly evaluate AI decision making and improve it. This paper presents a theory of appropriateness: how it functions in human society, how it may be implemented in the brain, and what it means for responsible deployment of generative AI technology.",
    "summary": "This paper proposes a theory of appropriateness, examining its multifaceted nature in human behavior and its implications for evaluating and improving the responsible deployment of generative AI, focusing on how human judgments of appropriateness shape AI's acceptable actions across various contexts."
  },
  {
    "url": "https://arxiv.org/abs/2408.14640",
    "title": "Effect of Adaptation Rate and Cost Display in a Human-AI Interaction Game",
    "published_date": "2024-08-26",
    "abstract": "As interactions between humans and AI become more prevalent, it is critical to have better predictors of human behavior in these interactions. We investigated how changes in the AI's adaptive algorithm impact behavior predictions in two-player continuous games. In our experiments, the AI adapted its actions using a gradient descent algorithm under different adaptation rates while human participants were provided cost feedback. The cost feedback was provided by one of two types of visual displays: (a) cost at the current joint action vector, or (b) cost in a local neighborhood of the current joint action vector. Our results demonstrate that AI adaptation rate can significantly affect human behavior, having the ability to shift the outcome between two game theoretic equilibrium. We observed that slow adaptation rates shift the outcome towards the Nash equilibrium, while fast rates shift the outcome towards the human-led Stackelberg equilibrium. The addition of localized cost information had the effect of shifting outcomes towards Nash, compared to the outcomes from cost information at only the current joint action vector. Future work will investigate other effects that influence the convergence of gradient descent games.",
    "summary": "This study examined how an AI's adaptation rate and cost display method in a two-player game influenced human behavior, finding that slower AI adaptation rates led to Nash equilibrium outcomes while faster rates favored Stackelberg equilibrium, with localized cost information further promoting Nash equilibrium."
  },
  {
    "url": "https://arxiv.org/abs/2408.14154",
    "title": "Investigating the effect of Mental Models in User Interaction with an Adaptive Dialog Agent",
    "published_date": "2024-08-26",
    "abstract": "Mental models play an important role in whether user interaction with intelligent systems, such as dialog systems is successful or not. Adaptive dialog systems present the opportunity to align a dialog agent's behavior with heterogeneous user expectations. However, there has been little research into what mental models users form when interacting with a task-oriented dialog system, how these models affect users' interactions, or what role system adaptation can play in this process, making it challenging to avoid damage to human-AI partnership. In this work, we collect a new publicly available dataset for exploring user mental models about information seeking dialog systems. We demonstrate that users have a variety of conflicting mental models about such systems, the validity of which directly impacts the success of their interactions and perceived usability of system. Furthermore, we show that adapting a dialog agent's behavior to better align with users' mental models, even when done implicitly, can improve perceived usability, dialog efficiency, and success. To this end, we argue that implicit adaptation can be a valid strategy for task-oriented dialog systems, so long as developers first have a solid understanding of users' mental models.",
    "summary": "This paper investigates how users' mental models of information-seeking dialog systems impact interaction success and usability, finding diverse and conflicting models that affect performance. The study demonstrates that implicitly adapting the dialog agent's behavior to align with these models improves user experience and interaction efficiency."
  },
  {
    "url": "https://arxiv.org/abs/2410.09099",
    "title": "Adaptive Active Inference Agents for Heterogeneous and Lifelong Federated Learning",
    "published_date": "2024-10-09",
    "abstract": "Handling heterogeneity and unpredictability are two core problems in pervasive computing. The challenge is to seamlessly integrate devices with varying computational resources in a dynamic environment to form a cohesive system that can fulfill the needs of all participants. Existing work on systems that adapt to changing requirements typically focuses on optimizing individual variables or low-level Service Level Objectives (SLOs), such as constraining the usage of specific resources. While low-level control mechanisms permit fine-grained control over a system, they introduce considerable complexity, particularly in dynamic environments. To this end, we propose drawing from Active Inference (AIF), a neuroscientific framework for designing adaptive agents. Specifically, we introduce a conceptual agent for heterogeneous pervasive systems that permits setting global systems constraints as high-level SLOs. Instead of manually setting low-level SLOs, the system finds an equilibrium that can adapt to environmental changes. We demonstrate the viability of AIF agents with an extensive experiment design, using heterogeneous and lifelong federated learning as an application scenario. We conduct our experiments on a physical testbed of devices with different resource types and vendor specifications. The results provide convincing evidence that an AIF agent can adapt a system to environmental changes. In particular, the AIF agent can balance competing SLOs in resource heterogeneous environments to ensure up to 98% fulfillment rate.",
    "summary": "This paper proposes using Active Inference (AIF), a neuroscientific framework, to create adaptive agents for heterogeneous and lifelong federated learning, enabling systems to autonomously adapt to changing environments and resource constraints by optimizing high-level Service Level Objectives instead of manually managing low-level controls. Experiments demonstrate the agent's effectiveness in balancing competing objectives and achieving high fulfillment rates in diverse, real-world settings."
  },
  {
    "url": "https://arxiv.org/abs/2410.23630",
    "title": "Adaptive Alignment: Dynamic Preference Adjustments via Multi-Objective Reinforcement Learning for Pluralistic AI",
    "published_date": "2024-10-31",
    "abstract": "Emerging research in Pluralistic Artificial Intelligence (AI) alignment seeks to address how intelligent systems can be designed and deployed in accordance with diverse human needs and values. We contribute to this pursuit with a dynamic approach for aligning AI with diverse and shifting user preferences through Multi Objective Reinforcement Learning (MORL), via post-learning policy selection adjustment. In this paper, we introduce the proposed framework for this approach, outline its anticipated advantages and assumptions, and discuss technical details about the implementation. We also examine the broader implications of adopting a retroactive alignment approach through the sociotechnical systems perspective.",
    "summary": "This paper proposes a novel method for aligning AI with diverse and evolving human preferences using multi-objective reinforcement learning, focusing on post-learning policy adjustments to achieve adaptive alignment in pluralistic AI systems. The framework considers both technical implementation details and the broader sociotechnical implications of this retroactive alignment approach."
  },
  {
    "url": "https://www.alignmentforum.org/posts/aoEnDEmoKCK9S99hL/cognitive-biases-contributing-to-ai-x-risk-a-deleted-excerpt",
    "author": "Andrew_Critch",
    "title": "Cognitive Biases Contributing to AI X-risk â€” a deleted excerpt from my 2018 ARCHES draft",
    "published_date": "2024-12-03",
    "summary": "The author discusses cognitive biases, particularly the \"illusion of control\" and \"scope insensitivity,\" that hinder accurate assessment of AI existential risks, specifically focusing on the potential for unrecognized prepotent (uncontrollably transformative) and misaligned AI. These biases, supported by existing research, can lead to underestimation of the catastrophic potential of advanced AI systems."
  }
]