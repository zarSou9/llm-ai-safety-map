[
  {
    "url": "https://arxiv.org/abs/2407.10244",
    "title": "Reimagining AI in Social Work: Practitioner Perspectives on Incorporating Technology in their Practice",
    "published_date": "2024-04-30",
    "abstract": "There has been a surge in the number and type of AI tools being tested and deployed within both national and local government in the UK, including within the social care sector. Given the many ongoing and planned future developments, the time is ripe to review and reflect on the state of AI in social care. We do so by conducting semi-structured interviews with UK-based social work professionals about their experiences and opinions of past and current AI systems. Our aim is to understand what systems would practitioners like to see developed and how. We find that all our interviewees had overwhelmingly negative past experiences of technology in social care, unanimous aversion to algorithmic decision systems in particular, but also strong interest in AI applications that could allow them to spend less time on administrative tasks. In response to our findings, we offer a series of concrete recommendations, which include commitment to participatory design, as well as the necessity of regaining practitioner trust.",
    "citation_count": 1,
    "summary": "A study of UK social workers reveals overwhelmingly negative past experiences with technology in social care, particularly algorithmic decision-making systems, but identifies strong interest in AI tools for administrative task reduction, prompting recommendations for participatory design and rebuilding practitioner trust."
  },
  {
    "url": "https://arxiv.org/abs/2403.12599",
    "title": "Preventing Eviction-Caused Homelessness through ML-Informed Distribution of Rental Assistance",
    "published_date": "2024-03-19",
    "abstract": "Rental assistance programs provide individuals with financial assistance to prevent housing instabilities caused by evictions and avert homelessness. Since these programs operate under resource constraints, they must decide who to prioritize. Typically, funding is distributed by a reactive allocation process that does not systematically consider risk of future homelessness. We partnered with Anonymous County (PA) to explore a proactive and preventative allocation approach that prioritizes individuals facing eviction based on their risk of future homelessness. Our ML models, trained on state and county administrative data accurately identify at-risk individuals, outperforming simpler prioritization approaches by at least 20% while meeting our equity and fairness goals across race and gender. Furthermore, our approach would reach 28% of individuals who are overlooked by the current process and end up homeless. Beyond improvements to the rental assistance program in Anonymous County, this study can inform the development of evidence-based decision support tools in similar contexts, including lessons about data needs, model design, evaluation, and field validation.",
    "summary": "This study demonstrates that machine learning models can significantly improve the targeting of rental assistance, reducing homelessness by proactively identifying at-risk individuals and outperforming current reactive methods by at least 20% while maintaining equity. The approach also reaches a substantial portion (28%) of individuals currently overlooked by existing systems."
  },
  {
    "url": "https://arxiv.org/pdf/2308.05224.pdf",
    "title": "Algorithmic Harms in Child Welfare: Uncertainties in Practice, Organization, and Street-level Decision-making",
    "published_date": "2023-08-09",
    "abstract": "Algorithms in public services such as child welfare, criminal justice, and education are increasingly being used to make high-stakes decisions about human lives. Drawing upon findings from a two-year ethnography conducted at a child welfare agency, we highlight how algorithmic systems are embedded within a complex decision-making ecosystem at critical points of the child welfare process. Caseworkers interact with algorithms in their daily lives where they must collect information about families and feed it to algorithms to make critical decisions. We show how the interplay between systemic mechanics and algorithmic decision-making can adversely impact the fairness of the decision-making process itself. We show how functionality issues in algorithmic systems can lead to process-oriented harms where they adversely affect the nature of professional practice, and administration at the agency, and lead to inconsistent and unreliable decisions at the street level. In addition, caseworkers are compelled to undertake additional labor in the form of repair work to restore disrupted administrative processes and decision-making, all while facing organizational pressures and time and resource constraints. Finally, we share the case study of a simple algorithmic tool that centers caseworkers' decision-making within a trauma-informed framework and leads to better outcomes, however, required a significant amount of investments on the agency's part in creating the ecosystem for its proper use.",
    "citation_count": 12,
    "summary": "This ethnographic study of algorithmic use in child welfare reveals how systemic issues and flawed algorithms negatively impact fairness and create extra work for caseworkers, leading to inconsistent decisions and process-oriented harms; however, it also demonstrates that well-designed algorithmic tools, supported by adequate organizational investment, can improve outcomes."
  },
  {
    "url": "https://arxiv.org/abs/2311.14706",
    "title": "Social AI Improves Well-Being Among Female Young Adults",
    "published_date": "2023-11-12",
    "abstract": "The rise of language models like ChatGPT has introduced Social AI as a new form of entertainment, particularly among young adults who engage with AI-powered agents. This paper investigates the effects of these interactions on users' social and mental well-being, a subject that has incited extensive debate among both the public and scholars. Our study involved a survey of 5,260 users of Chai, a Social AI Platform. The findings indicate significant benefits, with notable variations across demographics. Female users, in particular, reported the most substantial improvements: 43.4% strongly agreed that Social AI positively impacted their mental health, exceeding male users by 10.5%. In managing social anxieties, 38.9% of females strongly agreed on a positive impact, compared to 30.0% for males and 27.1% for other genders. Historically, new media and technology have often been met with groundless moral panic, with societal figures raising concerns without substantial evidence of harm. Our research indicates the importance of approaching such claims with caution and emphasizes the necessity of an evidence-based perspective in discussions about the behavioral effects of emerging technologies.",
    "citation_count": 1,
    "summary": "A survey of 5,260 Chai users revealed that Social AI interaction significantly improved the mental and social well-being of female young adults, more so than male users, suggesting potential benefits that counter previous unsubstantiated concerns. This contradicts prevalent anxieties surrounding the impact of emerging technologies."
  },
  {
    "url": "https://arxiv.org/abs/2303.09743",
    "title": "Understanding Frontline Workers' and Unhoused Individuals' Perspectives on AI Used in Homeless Services",
    "published_date": "2023-03-17",
    "abstract": "Recent years have seen growing adoption of AI-based decision-support systems (ADS) in homeless services, yet we know little about stakeholder desires and concerns surrounding their use. In this work, we aim to understand impacted stakeholders' perspectives on a deployed ADS that prioritizes scarce housing resources. We employed AI lifecycle comicboarding, an adapted version of the comicboarding method, to elicit stakeholder feedback and design ideas across various components of an AI system's design. We elicited feedback from county workers who operate the ADS daily, service providers whose work is directly impacted by the ADS, and unhoused individuals in the region. Our participants shared concerns and design suggestions around the AI system's overall objective, specific model design choices, dataset selection, and use in deployment. Our findings demonstrate that stakeholders, even without AI knowledge, can provide specific and critical feedback on an AI system's design and deployment, if empowered to do so.",
    "citation_count": 39,
    "summary": "This study uses a novel comicboarding method to gather feedback from frontline workers, service providers, and unhoused individuals on an AI-powered system allocating housing resources, revealing stakeholder concerns and suggestions across all stages of the AI system's lifecycle. The findings highlight the value of soliciting input from diverse stakeholders, even those lacking AI expertise, to improve AI system design and deployment in social services."
  },
  {
    "url": "https://www.lesswrong.com/posts/BCynDEwguEiogicAo/reflection-of-hierarchical-relationship-via-nuanced?commentId=LxbpnsnaqWD3xEwZc",
    "author": "Kyoung-cheol Kim",
    "title": "Reflection of Hierarchical Relationship via Nuanced Conditioning of Game Theory Approach for AI Development and Utilization - LessWrong",
    "published_date": "2023-02-07",
    "summary": "This article examines the application of game theory to AI development within organizational structures, highlighting both its benefits in multi-agent systems and its limitations. It emphasizes the enduring relevance of bureaucratic principles—hierarchical authority and specialization—even with the integration of AI, arguing that human-AI collaboration based on comparative advantage remains crucial for complex problem-solving."
  },
  {
    "title": "Can Robots Understand Welfare? Exploring Machine Bureaucracies in Welfare-to-Work",
    "abstract": "Abstract The exercise of administrative discretion by street-level workers plays a key role in shaping citizens' access to welfare and employment services. Governance reforms of social services delivery, such as performance-based contracting, have often been driven by attempts to discipline this discretion. In several countries, these forms of market governance are now being eclipsed by new modes of digital governance that seek to reshape the delivery of services using algorithms and machine learning. Australia, a pioneer of marketisation, is one example, proposing to deploy digitalisation to fully automate most of its employment services rather than as a supplement to face-to-face case management. We examine the potential and limits of this project to replace human-to-human with 'machine bureaucracies'. To what extent are welfare and employment services amenable to digitalisation? What trade-offs are involved? In addressing these questions, we consider the purported benefits of machine bureaucracies in achieving higher levels of efficiency, accountability, and consistency in policy delivery. While recognising the potential benefits of machine bureaucracies for both governments and jobseekers, we argue that trade-offs will be faced between enhancing the efficiency and consistency of services and ensuring that services remain accessible and responsive to highly personalised circumstances.",
    "published_date": "2022-03-16",
    "citation_count": 12,
    "url": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/488DC4BF167D98C5A3F92F4ABC69D68C/S0047279422000174a.pdf/can_robots_understand_welfare_exploring_machine_bureaucracies_in_welfaretowork.pdf",
    "summary": "This paper examines the implications of replacing human caseworkers with automated systems in welfare-to-work programs, analyzing the potential gains in efficiency and consistency against the risks of reduced accessibility and responsiveness to individual needs. The authors explore the trade-offs inherent in shifting from human-to-human to \"machine bureaucracies\" in delivering welfare services."
  },
  {
    "title": "\"We Would Never Write That Down\"",
    "abstract": "This paper draws attention to new complexities of deploying artificial intelligence (AI) to sensitive contexts, such as welfare allocation. AI is increasingly used in public administration with the promise of improving decision-making through predictive modelling. To accurately predict, it needs all the agreed criteria used as part of decisions, formal and informal. This paper empirically explores the informal classifications used by caseworkers to make unemployed welfare seekers 'fit' into the formal categories applied in a Danish job centre. Our findings show that these classifications are documentable, and hence traceable to AI. However, to the caseworkers, they are at odds with the stable explanations assumed by any bureaucratic recording system as they involve negotiated and situated judgments of people's character. Thus, for moral reasons, caseworkers find them ill-suited for formal representation and predictive purposes and choose not to write them down. As a result, although classification work is crucial to the job centre's activities, AI is denuded of the real-world (and real work) character of decision-making in this context. This is an important finding for CSCW as it is not only about whether AI can 'do' decision-making in particular contexts, as previous research has argued. This paper shows that problems may also be caused by people's unwillingness to provide data to systems. It is the purpose of this paper to present the empirical results of this research, followed by a discussion of implications for AI-supported practice and research.",
    "published_date": "2021-04-22",
    "citation_count": 29,
    "url": "https://dl.acm.org/doi/10.1145/3449176",
    "summary": "Caseworkers in a Danish job center utilize undocumented, informal classifications when assessing welfare applicants, which are crucial for accurate decision-making but intentionally omitted from formal records, rendering AI systems unable to accurately model the real-world complexities of the process. This highlights a crucial limitation of AI deployment in sensitive contexts stemming from human reluctance to formalize nuanced judgment."
  }
]