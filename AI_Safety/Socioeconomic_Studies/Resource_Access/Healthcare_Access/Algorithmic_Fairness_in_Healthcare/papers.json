[
  {
    "url": "https://arxiv.org/abs/2406.09307",
    "title": "What is Fair? Defining Fairness in Machine Learning for Health",
    "published_date": "2024-06-13",
    "abstract": "Ensuring that machine learning (ML) models are safe, effective, and equitable across all patient groups is essential for clinical decision-making and for preventing the reinforcement of existing health disparities. This review examines notions of fairness used in ML for health, including a review of why ML models can be unfair and how fairness has been quantified in a wide range of real-world examples. We provide an overview of commonly used fairness metrics and supplement our discussion with a case-study of an openly available electronic health record (EHR) dataset. We also discuss the outlook for future research, highlighting current challenges and opportunities in defining fairness in health.",
    "citation_count": 7,
    "summary": "This review examines definitions of fairness in machine learning for healthcare, analyzing why biases arise in models, exploring various fairness metrics, and illustrating challenges and opportunities via real-world examples and a case study. It emphasizes the importance of equitable ML models to avoid exacerbating existing health disparities."
  },
  {
    "url": "https://arxiv.org/pdf/2110.00603.pdf",
    "title": "Algorithm Fairness in AI for Medicine and Healthcare",
    "published_date": "2021-10-01",
    "abstract": "In the current development and deployment of many artiﬁcial intelligence (AI) systems in healthcare, algorithm fairness is a challenging problem in delivering equitable care. Recent evaluation of AI models stratiﬁed across race sub-populations have revealed inequalities in how patients are diagnosed, given treatments, and billed for healthcare costs. In this perspective article, we summarize the intersectional ﬁeld of fairness in machine learning through the context of current issues in healthcare, outline how algorithmic biases ( e.g. - image acquisition, genetic variation, intra-observer labeling variability) arise in current clinical workﬂows and their resulting healthcare disparities. Lastly, we also review emerging technology for mitigating bias via federated learning, disentanglement, and model explainability, and their role in AI-SaMD development.",
    "citation_count": 39,
    "summary": "This paper examines algorithmic fairness issues in AI-driven healthcare, highlighting how biases in data and algorithms lead to inequitable patient care across different populations. It reviews the sources of bias and explores potential mitigation strategies using techniques like federated learning, disentanglement, and model explainability."
  },
  {
    "title": "Dissecting Racial Bias in an Algorithm that Guides Health Decisions for 70 Million People",
    "abstract": "A single algorithm drives an important health care decision for over 70 million people in the US. When health systems anticipate that a patient will have especially complex and intensive future health care needs, she is enrolled in a 'care management' program, which provides considerable additional resources: greater attention from trained providers and help with coordination of her care. To determine which patients will have complex future health care needs, and thus benefit from program enrollment, many systems rely on an algorithmically generated commercial risk score. In this paper, we exploit a rich dataset to study racial bias in a commercial algorithm that is deployed nationwide today in many of the US's most prominent Accountable Care Organizations (ACOs). We document significant racial bias in this widely used algorithm, using data on primary care patients at a large hospital. Blacks and whites with the same algorithmic risk scores have very different realized health. For example, the highest-risk black patients (those at the threshold where patients are auto-enrolled in the program), have significantly more chronic illnesses than white enrollees with the same risk score. We use detailed physiological data to show the pervasiveness of the bias: across a range of biomarkers, from HbA1c levels for diabetics to blood pressure control for hypertensives, we find significant racial health gaps conditional on risk score. This bias has significant material consequences for patients: it effectively means that white patients with the same health as black patients are far more likely be enrolled in the care management program, and benefit from its resources. If we simulated a world without this gap in predictions, blacks would be auto-enrolled into the program at more than double the current rate. An unusual aspect of our dataset is that we observe not just the risk scores but also the input data and objective function used to construct it. This provides a unique window into the mechanisms by which bias arises. The algorithm is given a data frame with (1) Yit (label), total medical expenditures ('costs') in year t; and (2) Xi,t--1 (features), fine-grained care utilization data in year t -- 1 (e.g., visits to cardiologists, number of x-rays, etc.). The algorithm's predicted risk of developing complex health needs is thus in fact predicted costs. And by this metric, one could easily call the algorithm unbiased: costs are very similar for black and white patients with the same risk scores. So far, this is inconsistent with algorithmic bias: conditional on risk score, predictions do not favor whites or blacks. The fundamental problem we uncover is that when thinking about 'health care needs,' hospitals and insurers focus on costs. They use an algorithm whose specific objective is cost prediction, and from this perspective, predictions are accurate and unbiased. Yet from the social perspective, actual health -- not just costs -- also matters. This is where the problem arises: costs are not the same as health. While costs are a reasonable proxy for health (the sick do cost more, on average), they are an imperfect one: factors other than health can drive cost -- for example, race. We find that blacks cost more than whites on average; but this gap can be decomposed into two countervailing effects. First, blacks bear a different and larger burden of disease, making them costlier. But this difference in illness is offset by a second factor: blacks cost less, holding constant their exact chronic conditions, a force that dramatically reduces the overall cost gap. Perversely, the fact that blacks cost less than whites conditional on health means an algorithm that predicts costs accurately across racial groups will necessarily also generate biased predictions on health. The root cause of this bias is not in the procedure for prediction, or the underlying data, but the algorithm's objective function itself. This bias is akin to, but distinct from, 'mis-measured labels': it arises here from the choice of labels, not their measurement, which is in turn a consequence of the differing objective functions of private actors in the health sector and society. From the private perspective, the variable they focus on -- cost -- is being appropriately optimized. But our results hint at how algorithms may amplify a fundamental problem in health care as a whole: externalities produced when health care providers focus too narrowly on financial motives, optimizing on costs to the detriment of health. In this sense, our results suggest that a pervasive problem in health care -- incentives that induce health systems to focus on dollars rather than health -- also has consequences for the way algorithms are built and monitored.",
    "published_date": "2019-01-29",
    "citation_count": 108,
    "url": "https://dl.acm.org/doi/10.1145/3287560.3287593",
    "summary": "A widely used algorithm predicting healthcare needs for 70 million Americans exhibits racial bias, favoring white patients even with the same predicted risk score as Black patients. This bias stems not from flawed data or algorithm design, but from the algorithm's objective function: predicting cost, which imperfectly reflects health and inadvertently penalizes Black patients who, despite greater illness burden, incur lower costs."
  },
  {
    "url": "https://arxiv.org/abs/2403.07911",
    "title": "Standing on FURM ground - A framework for evaluating Fair, Useful, and Reliable AI Models in healthcare systems",
    "published_date": "2024-02-27",
    "abstract": "The impact of using artificial intelligence (AI) to guide patient care or operational processes is an interplay of the AI model's output, the decision-making protocol based on that output, and the capacity of the stakeholders involved to take the necessary subsequent action. Estimating the effects of this interplay before deployment, and studying it in real time afterwards, are essential to bridge the chasm between AI model development and achievable benefit. To accomplish this, the Data Science team at Stanford Health Care has developed a Testing and Evaluation (T&E) mechanism to identify fair, useful and reliable AI models (FURM) by conducting an ethical review to identify potential value mismatches, simulations to estimate usefulness, financial projections to assess sustainability, as well as analyses to determine IT feasibility, design a deployment strategy, and recommend a prospective monitoring and evaluation plan. We report on FURM assessments done to evaluate six AI guided solutions for potential adoption, spanning clinical and operational settings, each with the potential to impact from several dozen to tens of thousands of patients each year. We describe the assessment process, summarize the six assessments, and share our framework to enable others to conduct similar assessments. Of the six solutions we assessed, two have moved into a planning and implementation phase. Our novel contributions - usefulness estimates by simulation, financial projections to quantify sustainability, and a process to do ethical assessments - as well as their underlying methods and open source tools, are available for other healthcare systems to conduct actionable evaluations of candidate AI solutions.",
    "citation_count": 2,
    "summary": "The Stanford Health Care Data Science team developed the FURM framework for evaluating the fairness, usefulness, and reliability of AI models in healthcare, incorporating ethical review, simulation, financial projections, and deployment planning; this framework has already guided the assessment of six AI solutions, two of which are now in implementation."
  },
  {
    "url": "https://arxiv.org/pdf/2304.13493.pdf",
    "title": "Towards clinical AI fairness: A translational perspective",
    "published_date": "2023-04-26",
    "abstract": "Artificial intelligence (AI) has demonstrated the ability to extract insights from data, but the issue of fairness remains a concern in high-stakes fields such as healthcare. Despite extensive discussion and efforts in algorithm development, AI fairness and clinical concerns have not been adequately addressed. In this paper, we discuss the misalignment between technical and clinical perspectives of AI fairness, highlight the barriers to AI fairness' translation to healthcare, advocate multidisciplinary collaboration to bridge the knowledge gap, and provide possible solutions to address the clinical concerns pertaining to AI fairness.",
    "summary": "This paper argues that current AI fairness research inadequately addresses clinical concerns in healthcare, advocating for multidisciplinary collaboration to bridge the gap between technical and clinical perspectives on fairness and improve AI implementation."
  },
  {
    "url": "https://arxiv.org/pdf/2308.02081.pdf",
    "title": "Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare",
    "published_date": "2023-08-03",
    "abstract": "Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology – the science of measurement – suggests ways of counteracting target specification bias and avoiding its harmful consequences.",
    "citation_count": 6,
    "summary": "Healthcare machine learning models often suffer from target specification bias, where the model's target variable mismatches the intended clinical goal, leading to inflated accuracy estimates, resource misallocation, and potentially harmful decisions. Addressing this bias, often rooted in the need to predict counterfactual scenarios, requires careful alignment of model targets with actual clinical decision-making."
  },
  {
    "url": "https://www.alignmentforum.org/tag/ai",
    "author": "Evan Hubinger",
    "title": "AI - AI Alignment Forum",
    "published_date": "2023-02-06",
    "summary": "Artificial intelligence alignment focuses on ensuring powerful AI systems act according to human values, addressing the risk of unintended consequences and existential threats. This involves diverse approaches, from narrowly defined goals to achieving a beneficial future for humanity, and spans theoretical research, engineering solutions, and organizational efforts."
  },
  {
    "url": "https://www.lesswrong.com/posts/cCbybRT8bgiMbEHEv/a-list-of-all-the-deadlines-in-biden-s-executive-order-on-ai",
    "author": "Ricki Heicklen",
    "title": "Toward a Broader Conception of Adverse Selection",
    "published_date": "2023-11-01",
    "summary": "Biden's October 30, 2023 executive order on AI mandates various federal agencies to produce reports and implement plans regarding AI's use in transportation, workforce development, national security, and other sectors, with deadlines ranging from 30 to 90 days after the order's publication. These actions aim to promote responsible AI development and deployment across the government."
  }
]