[
  {
    "url": "https://arxiv.org/abs/2404.15058?utm_campaign=%F0%9F%9F%A1%20Preparing%20for%20AI%20Newsletter&utm_medium=email&_hsenc=p2ANqtz-_gUw_9cQjdn6wt8vtDekBij1SZaoLHhBVU8fg6tJUrmVKQA1CEXIjf6aM1AAvZB_WnKTEslWasnMIanJ8b_s7HXFIu9w&_hsmi=304334294&utm_content=304334294&utm_source=hs_email",
    "title": "A Mechanism-Based Approach to Mitigating Harms from Persuasive Generative AI",
    "published_date": "2024-04-23",
    "abstract": "Recent generative AI systems have demonstrated more advanced persuasive capabilities and are increasingly permeating areas of life where they can influence decision-making. Generative AI presents a new risk profile of persuasion due the opportunity for reciprocal exchange and prolonged interactions. This has led to growing concerns about harms from AI persuasion and how they can be mitigated, highlighting the need for a systematic study of AI persuasion. The current definitions of AI persuasion are unclear and related harms are insufficiently studied. Existing harm mitigation approaches prioritise harms from the outcome of persuasion over harms from the process of persuasion. In this paper, we lay the groundwork for the systematic study of AI persuasion. We first put forward definitions of persuasive generative AI. We distinguish between rationally persuasive generative AI, which relies on providing relevant facts, sound reasoning, or other forms of trustworthy evidence, and manipulative generative AI, which relies on taking advantage of cognitive biases and heuristics or misrepresenting information. We also put forward a map of harms from AI persuasion, including definitions and examples of economic, physical, environmental, psychological, sociocultural, political, privacy, and autonomy harm. We then introduce a map of mechanisms that contribute to harmful persuasion. Lastly, we provide an overview of approaches that can be used to mitigate against process harms of persuasion, including prompt engineering for manipulation classification and red teaming. Future work will operationalise these mitigations and study the interaction between different types of mechanisms of persuasion.",
    "citation_count": 11,
    "summary": "This paper proposes a framework for understanding and mitigating harms from persuasive generative AI, distinguishing between rational and manipulative persuasion, mapping potential harms across various domains, and outlining mechanisms and mitigation strategies focusing on the process of persuasion rather than solely its outcome. The framework includes definitions, examples, and potential mitigation techniques such as prompt engineering and red teaming."
  },
  {
    "url": "https://arxiv.org/abs/2405.09679",
    "title": "Simulating Policy Impacts: Developing a Generative Scenario Writing Method to Evaluate the Perceived Effects of Regulation",
    "published_date": "2024-05-15",
    "abstract": "The rapid advancement of AI technologies yields numerous future impacts on individuals and society. Policymakers are tasked to react quickly and establish policies that mitigate those impacts. However, anticipating the effectiveness of policies is a difficult task, as some impacts might only be observable in the future and respective policies might not be applicable to the future development of AI. In this work we develop a method for using large language models (LLMs) to evaluate the efficacy of a given piece of policy at mitigating specified negative impacts. We do so by using GPT-4 to generate scenarios both pre- and post-introduction of policy and translating these vivid stories into metrics based on human perceptions of impacts. We leverage an already established taxonomy of impacts of generative AI in the media environment to generate a set of scenario pairs both mitigated and non-mitigated by the transparency policy in Article 50 of the EU AI Act. We then run a user study (n=234) to evaluate these scenarios across four risk-assessment dimensions: severity, plausibility, magnitude, and specificity to vulnerable populations. We find that this transparency legislation is perceived to be effective at mitigating harms in areas such as labor and well-being, but largely ineffective in areas such as social cohesion and security. Through this case study we demonstrate the efficacy of our method as a tool to iterate on the effectiveness of policy for mitigating various negative impacts. We expect this method to be useful to researchers or other stakeholders who want to brainstorm the potential utility of different pieces of policy or other mitigation strategies.",
    "citation_count": 1,
    "summary": "This paper presents a novel method using GPT-4 to generate scenarios illustrating the perceived impact of AI policies, evaluating the effectiveness of EU AI Act Article 50's transparency requirements through a user study (n=234) which showed varied effectiveness across different impact areas."
  },
  {
    "url": "https://arxiv.org/abs/2407.17129",
    "title": "Mapping the individual, social and biospheric impacts of Foundation Models",
    "published_date": "2024-06-03",
    "abstract": "Responding to the rapid roll-out and large-scale commercialization of foundation models, large language models, and generative AI, an emerging body of work is shedding light on the myriad impacts these technologies are having across society. Such research is expansive, ranging from the production of discriminatory, fake and toxic outputs, and privacy and copyright violations, to the unjust extraction of labor and natural resources. The same has not been the case in some of the most prominent AI governance initiatives in the global north like the UK's AI Safety Summit and the G7's Hiroshima process, which have influenced much of the international dialogue around AI governance. Despite the wealth of cautionary tales and evidence of algorithmic harm, there has been an ongoing over-emphasis within the AI governance discourse on technical matters of safety and global catastrophic or existential risks. This narrowed focus has tended to draw attention away from very pressing social and ethical challenges posed by the current brute-force industrialization of AI applications. To address such a visibility gap between real-world consequences and speculative risks, this paper offers a critical framework to account for the social, political, and environmental dimensions of foundation models and generative AI. Drawing on a review of the literature on the harms and risks of foundations models, and insights from critical data studies, science and technology studies, and environmental justice scholarship, we identify 14 categories of risks and harms and map them according to their individual, social, and biospheric impacts. We argue that this novel typology offers an integrative perspective to address the most urgent negative impacts of foundation models and their downstream applications. We conclude with recommendations on how this typology could be used to inform technical and normative interventions to advance responsible AI.",
    "citation_count": 5,
    "summary": "This paper critiques the current AI governance focus on hypothetical risks, arguing instead for a framework that maps the individual, social, and environmental harms of foundation models across fourteen categories. This framework aims to guide responsible AI development by prioritizing real-world impacts over speculative ones."
  },
  {
    "url": "https://arxiv.org/abs/2410.22985",
    "title": "Troubling Taxonomies in GenAI Evaluation",
    "published_date": "2024-10-30",
    "abstract": "To evaluate the societal impacts of GenAI requires a model of how social harms emerge from interactions between GenAI, people, and societal structures. Yet a model is rarely explicitly defined in societal impact evaluations, or in the taxonomies of societal impacts that support them. In this provocation, we argue that societal impacts should be conceptualised as application- and context-specific, incommensurable, and shaped by questions of social power. Doing so leads us to conclude that societal impact evaluations using existing taxonomies are inherently limited, in terms of their potential to reveal how GenAI systems may interact with people when introduced into specific social contexts. We therefore propose a governance-first approach to managing societal harms attended by GenAI technologies.",
    "summary": "Existing GenAI societal impact evaluations rely on flawed taxonomies that fail to account for context-specific harms and power dynamics, necessitating a governance-first approach prioritizing harm mitigation over current evaluation methods."
  },
  {
    "url": "https://www.alignmentforum.org/posts/6nNwMbdRXZDuNd4Gx/analysis-of-global-ai-governance-strategies",
    "author": "Sammy Martin, Justin Bullock, Corin Katzke",
    "title": "Analysis of Global AI Governance Strategies",
    "published_date": "2024-12-04",
    "summary": "The article analyzes three AI governance strategies—Cooperative Development, Strategic Advantage, and Global Moratorium—evaluating their effectiveness based on the difficulty of aligning AI and development timelines. The optimal strategy shifts depending on these factors, with Cooperative Development preferred for longer timelines and easier alignment, Strategic Advantage for shorter timelines and moderate difficulty, and Global Moratorium for extremely short timelines or difficult alignment."
  },
  {
    "url": "https://www.alignmentforum.org/posts/Z8bthnjW52uTCFGku/a-narrow-path-a-plan-to-deal-with-ai-extinction-risk",
    "author": "Andrea Miotti; Davekasten; Tolga",
    "title": "A Narrow Path: a plan to deal with AI extinction risk",
    "published_date": "2024-10-07",
    "summary": "\"A Narrow Path\" proposes a three-phase plan to mitigate the existential risk posed by uncontrolled artificial superintelligence development, aiming to prevent its creation for at least 20 years through international cooperation and policy changes, ultimately enabling safe and beneficial AI advancement."
  },
  {
    "url": "https://www.lesswrong.com/posts/324pQjqoHEHeF2vPs/ai-clarity-an-initial-research-agenda",
    "author": "Justin Bullock, Corin Katzke, Zershaaneh Qureshi, David_Kristoffersson",
    "title": "AI Clarity: An Initial Research Agenda",
    "published_date": "2024-05-03",
    "summary": "The AI Clarity research program uses scenario planning to explore potential pathways to existential risks from transformative AI (TAI), particularly focusing on short timelines (within a decade). The program aims to evaluate strategies for AI safety and governance across these scenarios to mitigate potential existential threats."
  },
  {
    "url": "http://arxiv.org/abs/2401.05377",
    "title": "The impact of generative artificial intelligence on socioeconomic inequalities and policy making",
    "published_date": "2023-12-16",
    "abstract": "Abstract Generative artificial intelligence (AI) has the potential to both exacerbate and ameliorate existing socioeconomic inequalities. In this article, we provide a state-of-the-art interdisciplinary overview of the potential impacts of generative AI on (mis)information and three information-intensive domains: work, education, and healthcare. Our goal is to highlight how generative AI could worsen existing inequalities while illuminating how AI may help mitigate pervasive social problems. In the information domain, generative AI can democratize content creation and access but may dramatically expand the production and proliferation of misinformation. In the workplace, it can boost productivity and create new jobs, but the benefits will likely be distributed unevenly. In education, it offers personalized learning, but may widen the digital divide. In healthcare, it might improve diagnostics and accessibility, but could deepen pre-existing inequalities. In each section, we cover a specific topic, evaluate existing research, identify critical gaps, and recommend research directions, including explicit trade-offs that complicate the derivation of a priori hypotheses. We conclude with a section highlighting the role of policymaking to maximize generative AI's potential to reduce inequalities while mitigating its harmful effects. We discuss strengths and weaknesses of existing policy frameworks in the European Union, the United States, and the United Kingdom, observing that each fails to fully confront the socioeconomic challenges we have identified. We propose several concrete policies that could promote shared prosperity through the advancement of generative AI. This article emphasizes the need for interdisciplinary collaborations to understand and address the complex challenges of generative AI.",
    "citation_count": 37,
    "summary": "Generative AI's impact on socioeconomic inequality is multifaceted, potentially exacerbating disparities in information access, work, education, and healthcare while also offering opportunities for mitigation. Effective policy interventions are crucial to harness its benefits and minimize its harmful effects on equitable societal development."
  }
]