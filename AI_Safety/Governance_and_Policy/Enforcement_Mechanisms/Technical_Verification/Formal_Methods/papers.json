[
  {
    "url": "https://arxiv.org/pdf/2104.02466.pdf",
    "title": "A Review of Formal Methods applied to Machine Learning",
    "published_date": "2021-04-06",
    "abstract": "We review state-of-the-art formal methods applied to the emerging field of the verification of machine learning systems. Formal methods can provide rigorous correctness guarantees on hardware and software systems. Thanks to the availability of mature tools, their use is well established in the industry, and in particular to check safety-critical applications as they undergo a stringent certification process. As machine learning is becoming more popular, machine-learned components are now considered for inclusion in critical systems. This raises the question of their safety and their verification. Yet, established formal methods are limited to classic, i.e. non machine-learned software. Applying formal methods to verify systems that include machine learning has only been considered recently and poses novel challenges in soundness, precision, and scalability. We first recall established formal methods and their current use in an exemplar safety-critical field, avionic software, with a focus on abstract interpretation based techniques as they provide a high level of scalability. This provides a golden standard and sets high expectations for machine learning verification. We then provide a comprehensive and detailed review of the formal methods developed so far for machine learning, highlighting their strengths and limitations. The large majority of them verify trained neural networks and employ either SMT, optimization, or abstract interpretation techniques. We also discuss methods for support vector machines and decision tree ensembles, as well as methods targeting training and data preparation, which are critical but often neglected aspects of machine learning. Finally, we offer perspectives for future research directions towards the formal verification of machine learning systems.",
    "citation_count": 49,
    "summary": "This paper reviews the application of formal methods, particularly abstract interpretation, to verify machine learning systems, focusing on techniques for neural networks, support vector machines, and decision trees, while highlighting challenges in soundness, precision, and scalability. The authors also discuss the importance of verifying the training and data preparation phases and suggest future research directions."
  },
  {
    "url": "https://arxiv.org/abs/2405.10611",
    "title": "A Certified Proof Checker for Deep Neural Network Verification",
    "published_date": "2024-05-17",
    "abstract": "Recent advances in the verification of deep neural networks (DNNs) have opened the way for broader usage of DNN verification technology in many application areas, including safety-critical ones. DNN verifiers are themselves complex programs that have been shown to be susceptible to errors and imprecisions; this in turn has raised the question of trust in DNN verifiers. One prominent attempt to address this issue is enhancing DNN verifiers with the capability of producing proofs of their results that are subject to independent algorithmic certification (proof checking). Formulations of proof production and proof checking already exist on top of the state-of-the-art Marabou DNN verifier. The native implementation of the proof checking algorithm for Marabou was done in C++ and itself raised the question of trust in the code (e.g., in the precision of floating point calculations or guarantees for implementation soundness). Here, we present an alternative implementation of the Marabou proof checking algorithm in Imandra -- an industrial functional programming language and prover -- that allows us to obtain an implementation with formal guarantees, including proofs of mathematical results underlying the algorithm, such as the use of the Farkas lemma.",
    "citation_count": 1,
    "summary": "This paper presents a formally verified implementation of a proof checker for the Marabou deep neural network verifier, using the Imandra functional programming language to ensure correctness and address trust concerns in existing C++ implementations. This approach provides formal guarantees, including proofs of underlying mathematical lemmas, enhancing the reliability of DNN verification results."
  },
  {
    "url": "https://arxiv.org/abs/2412.16075",
    "title": "Formal Mathematical Reasoning: A New Frontier in AI",
    "published_date": "2024-12-20",
    "abstract": "AI for Mathematics (AI4Math) is not only intriguing intellectually but also crucial for AI-driven discovery in science, engineering, and beyond. Extensive efforts on AI4Math have mirrored techniques in NLP, in particular, training large language models on carefully curated math datasets in text form. As a complementary yet less explored avenue, formal mathematical reasoning is grounded in formal systems such as proof assistants, which can verify the correctness of reasoning and provide automatic feedback. In this position paper, we advocate for formal mathematical reasoning and argue that it is indispensable for advancing AI4Math to the next level. In recent years, we have seen steady progress in using AI to perform formal reasoning, including core tasks such as theorem proving and autoformalization, as well as emerging applications such as verifiable generation of code and hardware designs. However, significant challenges remain to be solved for AI to truly master mathematics and achieve broader impact. We summarize existing progress, discuss open challenges, and envision critical milestones to measure future success. At this inflection point for formal mathematical reasoning, we call on the research community to come together to drive transformative advancements in this field.",
    "summary": "This position paper advocates for formal mathematical reasoning, using proof assistants, as a crucial complement to existing AI for Mathematics (AI4Math) approaches, arguing that it's essential for achieving significant advancements in the field. While progress exists in areas like theorem proving and autoformalization, significant challenges remain before AI can truly master mathematics."
  },
  {
    "title": "Towards AI-Assisted Synthesis of Verified Dafny Methods",
    "abstract": "Large language models show great promise in many domains, including programming. A promise is easy to make but hard to keep, and language models often fail to keep their promises, generating erroneous code. A promising avenue to keep models honest is to incorporate formal verification: generating programs' specifications as well as code so that the code can be proved correct with respect to the specifications. Unfortunately, existing large language models show a severe lack of proficiency in verified programming. \n \n \n \n \n \n \n \nIn this paper, we demonstrate how to improve two pretrained models' proficiency in the Dafny verification-aware language. Using 178 problems from the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to synthesize Dafny methods. We use three different types of prompts: a direct Contextless prompt; a Signature prompt that includes a method signature and test cases, and a Chain of Thought (CoT) prompt that decomposes the problem into steps and includes retrieval augmentation generated example problems and solutions. Our results show that GPT-4 performs better than PaLM-2 on these tasks and that both models perform best with the retrieval augmentation generated CoT prompt. GPT-4 was able to generate verified, human-evaluated, Dafny methods for 58% of the problems, however, GPT-4 managed only 19% of the problems with the Contextless prompt, and even fewer (10%) for the Signature prompt. We are thus able to contribute 153 verified Dafny solutions to MBPP problems, 50 that we wrote manually, and 103 synthesized by GPT-4. \n \n \n \n \n \n \n \nOur results demonstrate that the benefits of formal program verification are now within reach of code generating large language models. Likewise, program verification systems can benefit from large language models, whether to synthesize code wholesale, to generate specifications, or to act as a \"programmer's verification apprentice\", to construct annotations such as loop invariants which are hard for programmers to write or verification tools to find. Finally, we expect that the approach we have pioneered here — generating candidate solutions that are subsequently formally checked for correctness — should transfer to other domains (e.g., legal arguments, transport signaling, structural engineering) where solutions must be correct, where that correctness must be demonstrated, explained and understood by designers and end-users.",
    "published_date": "2024-02-01",
    "citation_count": 19,
    "url": "https://dl.acm.org/doi/10.1145/3643763",
    "summary": "This paper investigates using large language models (LLMs), specifically GPT-4 and PaLM-2, to generate formally verified Dafny code. The authors demonstrate that prompting techniques, particularly a Chain of Thought prompt with retrieval augmentation, significantly improve the models' ability to synthesize correct Dafny methods, achieving a 58% success rate for GPT-4."
  },
  {
    "url": "https://www.alignmentforum.org/posts/LkECxpbjvSifPfjnb/towards-guaranteed-safe-ai-a-framework-for-ensuring-robust-1",
    "author": "Joar Skalse",
    "title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems",
    "published_date": "2024-05-17",
    "summary": "There is no article provided to summarize."
  },
  {
    "url": "https://www.alignmentforum.org/posts/B2bg677TaS4cmDPzL/limitations-on-formal-verification-for-ai-safety",
    "author": "Andrew Dickson",
    "title": "Limitations on Formal Verification for AI Safety",
    "published_date": "2024-08-19",
    "summary": "The article argues that claims of using formal verification to guarantee AI safety are overly optimistic. Practical limitations inherent in modeling complex real-world systems, such as those in biology and physics, make achieving strong, provable guarantees against major AI threats in the near term highly unlikely."
  },
  {
    "url": "https://www.alignmentforum.org/posts/SyeQjjBoEC48MvnQC/formal-verification-heuristic-explanations-and-surprise",
    "author": "Jacob Hilton",
    "title": "Formal verification, heuristic explanations and surprise accounting",
    "published_date": "2024-06-25",
    "summary": "The article discusses the limitations of using formal verification to guarantee the safety of large neural networks due to the complexity of interactions within the network. Instead, it proposes \"heuristic explanations,\" a less rigorous but more practical approach to understanding and assessing neural network behavior, exemplified by \"surprise accounting\" to quantify explanation quality."
  },
  {
    "url": "https://arxiv.org/abs/2301.06136v1",
    "title": "Quantitative Verification With Neural Networks For Probabilistic Programs and Stochastic Systems",
    "published_date": "2023-01-15",
    "abstract": "We present a data-driven approach to the quantitative verification of probabilistic programs and stochastic dynamical models. Our approach leverages neural networks to compute tight and sound bounds for the probability that a stochastic process hits a target condition within finite time. This problem subsumes a variety of quantitative verification questions, from the reachability and safety analysis of discrete-time stochastic dynamical models, to the study of assertion-violation and termination analysis of probabilistic programs. We rely on neural networks to represent supermartingale certificates that yield such probability bounds, which we compute using a counterexample-guided inductive synthesis loop: we train the neural certificate while tightening the probability bound over samples of the state space using stochastic optimisation, and then we formally check the certificate's validity over every possible state using satisfiability modulo theories; if we receive a counterexample, we add it to our set of samples and repeat the loop until validity is confirmed. We demonstrate on a diverse set of benchmarks that, thanks to the expressive power of neural networks, our method yields smaller or comparable probability bounds than existing symbolic methods in all cases, and that our approach succeeds on models that are entirely beyond the reach of such alternative techniques.",
    "citation_count": 2,
    "summary": "This paper introduces a novel data-driven approach to quantitatively verifying probabilistic programs and stochastic systems using neural networks to compute tight probability bounds for hitting target conditions. The method employs a counterexample-guided inductive synthesis loop combining neural network training with formal verification to achieve superior performance compared to existing symbolic methods."
  }
]