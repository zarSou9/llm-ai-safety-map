[
  {
    "url": "http://arxiv.org/abs/2401.14908",
    "title": "A Framework for Assurance Audits of Algorithmic Systems",
    "published_date": "2024-01-26",
    "abstract": "An increasing number of regulations propose 'AI audits' as a mechanism for achieving transparency and accountability for artificial intelligence (AI) systems. Despite some converging norms around various forms of AI auditing, auditing for the purpose of compliance and assurance currently lacks agreed-upon practices, procedures, taxonomies, and standards. We propose the 'criterion audit' as an operationalizable compliance and assurance external audit framework. We model elements of this approach after financial auditing practices, and argue that AI audits should similarly provide assurance to their stakeholders about AI organizations' ability to govern their algorithms in ways that mitigate harms and uphold human values. We discuss the necessary conditions for the criterion audit and provide a procedural blueprint for performing an audit engagement in practice. We illustrate how this framework can be adapted to current regulations by deriving the criteria on which 'bias audits' can be performed for in-scope hiring algorithms, as required by the recently effective New York City Local Law 144 of 2021. We conclude by offering a critical discussion on the benefits, inherent limitations, and implementation challenges of applying practices of the more mature financial auditing industry to AI auditing where robust guardrails against quality assurance issues are only starting to emerge. Our discussion—informed by experiences in performing these audits in practice—highlights the critical role that an audit ecosystem plays in ensuring the effectiveness of audits.",
    "citation_count": 2,
    "summary": "This paper proposes a \"criterion audit\" framework for assuring compliance and mitigating harms in algorithmic systems, drawing parallels with financial auditing practices to provide a structured approach for AI audits that addresses current regulatory needs. The framework is illustrated with a bias audit example and discusses its benefits, limitations, and implementation challenges within the context of a developing audit ecosystem."
  },
  {
    "url": "https://arxiv.org/pdf/2107.14046.pdf",
    "title": "Audit and Assurance of AI Algorithms: A framework to ensure ethical algorithmic practices in Artificial Intelligence",
    "published_date": "2021-07-14",
    "abstract": "Algorithms are becoming more widely used in business, and businesses are becoming increasingly concerned that their algorithms will cause significant reputational or financial damage. We should emphasize that any of these damages stem from situations in which the United States lacks strict legislative prohibitions or specified protocols for measuring damages. As a result, governments are enacting legislation and enforcing prohibitions, regulators are fining businesses, and the judiciary is debating whether or not to make artificially intelligent computer models as the decision-makers in the eyes of the law. From autonomous vehicles and banking to medical care, housing, and legal decisions, there will soon be enormous amounts of algorithms that make decisions with limited human interference. Governments, businesses, and society would have an algorithm audit, which would have systematic verification that algorithms are lawful, ethical, and secure, similar to financial audits. A modern market, auditing, and assurance of algorithms developed to professionalize and industrialize AI, machine learning, and related algorithms. Stakeholders of this emerging field include policymakers and regulators, along with industry experts and entrepreneurs. In addition, we foresee audit thresholds and frameworks providing valuable information to all who are concerned with governance and standardization. This paper aims to review the critical areas required for auditing and assurance and spark discussion in this novel field of study and practice.",
    "citation_count": 5,
    "summary": "This paper proposes a framework for auditing and assuring AI algorithms, addressing growing concerns about algorithmic bias, legality, and ethical implications in various sectors, advocating for systematic verification similar to financial audits to promote responsible AI development and deployment. The framework aims to provide a structured approach for stakeholders including policymakers, regulators, and industry professionals."
  },
  {
    "url": "http://arxiv.org/abs/2401.14462",
    "title": "AI auditing: The Broken Bus on the Road to AI Accountability",
    "published_date": "2024-01-25",
    "abstract": "One of the most concrete measures towards meaningful AI accountability is to consequentially assess and report the systems' performance and impact. However, the practical nature of the \"AI audit\" ecosystem is muddled and imprecise, making it difficult to work through various concepts, practices, and involved (as well as ignored) stakeholders. First, we taxonomize current AI audit practices as completed by regulators, law firms, civil society, journalism, academia, and consulting agencies. Next, we assess the impact of audits done by stakeholders within each domain. We find that only a subset of AI audit studies translate to desired accountability outcomes. We thus assess and isolate practices necessary for effective AI audit results, articulating the observed connections between AI audit design, methodology and institutional context on its effectiveness as a meaningful mechanism for accountability.",
    "citation_count": 27,
    "summary": "This paper analyzes the current state of AI auditing across various sectors, finding inconsistencies and limited impact on AI accountability. It proposes that effective AI audits require specific design, methodology, and institutional context to achieve meaningful accountability outcomes."
  },
  {
    "url": "https://arxiv.org/abs/2404.13060",
    "title": "The Necessity of AI Audit Standards Boards",
    "published_date": "2024-04-11",
    "abstract": "Auditing of AI systems is a promising way to understand and manage ethical problems and societal risks associated with contemporary AI systems, as well as some anticipated future risks. Efforts to develop standards for auditing Artificial Intelligence (AI) systems have therefore understandably gained momentum. However, we argue that creating auditing standards is not just insufficient, but actively harmful by proliferating unheeded and inconsistent standards, especially in light of the rapid evolution and ethical and safety challenges of AI. Instead, the paper proposes the establishment of an AI Audit Standards Board, responsible for developing and updating auditing methods and standards in line with the evolving nature of AI technologies. Such a body would ensure that auditing practices remain relevant, robust, and responsive to the rapid advancements in AI. The paper argues that such a governance structure would also be helpful for maintaining public trust in AI and for promoting a culture of safety and ethical responsibility within the AI industry. Throughout the paper, we draw parallels with other industries, including safety-critical industries like aviation and nuclear energy, as well as more prosaic ones such as financial accounting and pharmaceuticals. AI auditing should emulate those fields, and extend beyond technical assessments to include ethical considerations and stakeholder engagement, but we explain that this is not enough; emulating other fields' governance mechanisms for these processes, and for audit standards creation, is a necessity. We also emphasize the importance of auditing the entire development process of AI systems, not just the final products...",
    "citation_count": 3,
    "summary": "The paper argues that inconsistent AI auditing standards are harmful and proposes the creation of an AI Audit Standards Board to develop and maintain unified, evolving auditing methods encompassing technical, ethical, and stakeholder considerations across the entire AI development lifecycle. This centralized governance is crucial for fostering public trust and responsible AI innovation."
  },
  {
    "url": "https://www.alignmentforum.org/posts/XSqntCNMafhcy9irf/third-party-testing-as-a-key-ingredient-of-ai-policy",
    "author": "Zac Hatfield-Dodds",
    "title": "Third-party testing as a key ingredient of AI policy",
    "published_date": "2024-03-25",
    "summary": "The article advocates for third-party testing of large-scale AI systems to mitigate societal harms, arguing that such a regime, involving industry, government, and academia, is crucial for managing the risks of powerful AI models while fostering innovation and preventing overly restrictive regulation. This testing would focus on high-impact systems, aiming to build trust and facilitate international cooperation."
  },
  {
    "url": "https://arxiv.org/abs/2312.00044",
    "title": "Advancing AI Audits for Enhanced AI Governance",
    "published_date": "2023-11-26",
    "abstract": "As artificial intelligence (AI) is integrated into various services and systems in society, many companies and organizations have proposed AI principles, policies, and made the related commitments. Conversely, some have proposed the need for independent audits, arguing that the voluntary principles adopted by the developers and providers of AI services and systems insufficiently address risk. This policy recommendation summarizes the issues related to the auditing of AI services and systems and presents three recommendations for promoting AI auditing that contribute to sound AI governance. Recommendation1.Development of institutional design for AI audits. Recommendation2.Training human resources for AI audits. Recommendation3. Updating AI audits in accordance with technological progress. In this policy recommendation, AI is assumed to be that which recognizes and predicts data with the last chapter outlining how generative AI should be audited.",
    "summary": "This policy recommendation advocates for independent AI audits to supplement voluntary AI principles, proposing the development of auditing institutions, training programs for auditors, and mechanisms for adapting audits to evolving AI technologies, including generative AI."
  },
  {
    "url": "https://www.alignmentforum.org/posts/LwJwDNFhjurAKFiJm/theories-of-change-for-ai-auditing",
    "author": "Lee Sharkey, beren, Marius Hobbhahn",
    "title": "Theories of Change for AI Auditing",
    "published_date": "2023-11-13",
    "summary": "Apollo Research argues that AI auditing, encompassing evaluations of AI systems, training designs, deployment, security, and governance, is crucial for mitigating catastrophic AI risks. While acknowledging limitations like false confidence and overfitting, the authors propose that external audits, combined with regulatory backing, offer a vital defense-in-depth strategy for improving AI safety."
  },
  {
    "title": "Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing",
    "abstract": "Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.",
    "published_date": "2020-01-03",
    "citation_count": 646,
    "url": "https://dl.acm.org/doi/10.1145/3351095.3372873",
    "summary": "This paper proposes an end-to-end internal algorithmic auditing framework to proactively identify and mitigate harms in AI systems throughout their development lifecycle, aiming to close the accountability gap before deployment. The framework generates a comprehensive audit report based on organizational values, ensuring audit integrity and responsible AI development."
  },
  {
    "url": "https://arxiv.org/abs/2411.00069",
    "title": "Meta-Sealing: A Revolutionizing Integrity Assurance Protocol for Transparent, Tamper-Proof, and Trustworthy AI System",
    "published_date": "2024-10-31",
    "abstract": "The Artificial intelligence in critical sectors-healthcare, finance, and public safety-has made system integrity paramount for maintaining societal trust. Current verification methods for AI systems lack comprehensive lifecycle assurance, creating significant vulnerabilities in deployment of both powerful and trustworthy AI. This research introduces Meta-Sealing, a cryptographic framework that fundamentally changes integrity verification in AI systems throughout their operational lifetime. Meta-Sealing surpasses traditional integrity protocols through its implementation of cryptographic seal chains, establishing verifiable, immutable records for all system decisions and transformations. The framework combines advanced cryptography with distributed verification, delivering tamper-evident guarantees that achieve both mathematical rigor and computational efficiency. Our implementation addresses urgent regulatory requirements for AI system transparency and auditability. The framework integrates with current AI governance standards, specifically the EU's AI Act and FDA's healthcare AI guidelines, enabling organizations to maintain operational efficiency while meeting compliance requirements. Testing on financial institution data demonstrated Meta-Sealing's capability to reduce audit timeframes by 62% while enhancing stakeholder confidence by 47%. Results can establish a new benchmark for integrity assurance in enterprise AI deployments. This research presents Meta-Sealing not merely as a technical solution, but as a foundational framework ensuring AI system integrity aligns with human values and regulatory requirements. As AI continues to influence critical decisions, provides the necessary bridge between technological advancement and verifiable trust. Meta-Sealing serves as a guardian of trust, ensuring that the AI systems we depend on are as reliable and transparent as they are powerful.",
    "summary": "Meta-Sealing is a novel cryptographic framework providing comprehensive, tamper-proof integrity assurance for AI systems throughout their lifecycle, using cryptographic seal chains to create verifiable records of all system actions and significantly improving audit efficiency and stakeholder trust. This framework aligns with emerging AI regulations, ensuring both technological advancement and trustworthy AI deployment."
  },
  {
    "url": "https://www.lesswrong.com/posts/ECnLBSxw4TvpWPnae/ai-model-registries-a-regulatory-review",
    "author": "Deric Cheng, Elliot_Mckernon",
    "title": "AI Model Registries: A Regulatory Review",
    "published_date": "2024-03-22",
    "summary": "This article, part of a series reviewing the 2024 AI regulatory landscape, examines AI model registries—centralized databases tracking AI systems used in the real world. These registries, mandated by governments like China's, aim to monitor AI development, inform future legislation, and function similarly to existing registries for products like pharmaceuticals."
  }
]