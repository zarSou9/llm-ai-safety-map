[
  {
    "url": "https://arxiv.org/abs/2403.08501",
    "title": "Governing Through the Cloud: The Intermediary Role of Compute Providers in AI Regulation",
    "published_date": "2024-03-13",
    "abstract": "As jurisdictions around the world take their first steps toward regulating the most powerful AI systems, such as the EU AI Act and the US Executive Order 14110, there is a growing need for effective enforcement mechanisms that can verify compliance and respond to violations. We argue that compute providers should have legal obligations and ethical responsibilities associated with AI development and deployment, both to provide secure infrastructure and to serve as intermediaries for AI regulation. Compute providers can play an essential role in a regulatory ecosystem via four key capacities: as securers, safeguarding AI systems and critical infrastructure; as record keepers, enhancing visibility for policymakers; as verifiers of customer activities, ensuring oversight; and as enforcers, taking actions against rule violations. We analyze the technical feasibility of performing these functions in a targeted and privacy-conscious manner and present a range of technical instruments. In particular, we describe how non-confidential information, to which compute providers largely already have access, can provide two key governance-relevant properties of a computational workload: its type-e.g., large-scale training or inference-and the amount of compute it has consumed. Using AI Executive Order 14110 as a case study, we outline how the US is beginning to implement record keeping requirements for compute providers. We also explore how verification and enforcement roles could be added to establish a comprehensive AI compute oversight scheme. We argue that internationalization will be key to effective implementation, and highlight the critical challenge of balancing confidentiality and privacy with risk mitigation as the role of compute providers in AI regulation expands.",
    "citation_count": 4,
    "summary": "This paper argues that cloud compute providers should play a central role in AI regulation, acting as intermediaries to secure infrastructure, maintain records of AI activity, verify compliance, and enforce regulations. The authors propose leveraging existing provider data to track AI workload type and compute consumption, while acknowledging the need for international cooperation and careful balancing of privacy concerns."
  },
  {
    "url": "https://arxiv.org/abs/2409.10031",
    "title": "Assessing the Impact of Sanctions in the Crypto Ecosystem: Effective Measures or Ineffective Deterrents?",
    "published_date": "2024-09-16",
    "abstract": "Regulatory authorities aim to tackle illegal activities by targeting the economic incentives that drive such behaviour. This is typically achieved through the implementation of financial sanctions against the entities involved in the crimes. However, the rise of cryptocurrencies has presented new challenges, allowing entities to evade these sanctions and continue criminal operations. Consequently, enforcement measures have been expanded to include crypto assets information of sanctioned entities. Yet, due to the nature of the crypto ecosystem, blocking or freezing these digital assets is harder and, in some cases, such as with Bitcoin, unfeasible. Therefore, sanctions serve merely as deterrents. For this reason, in this study, we aim to assess the impact of these sanctions on entities' crypto activities, particularly those related to the Bitcoin ecosystem. Our objective is to shed light on the validity and effectiveness (or lack thereof) of such countermeasures. Specifically, we analyse the transactions and the amount of USD moved by punished entities that possess crypto addresses after being sanctioned by the authority agency. Results indicate that while sanctions have been effective for half of the examined entities, the others continue to move funds through sanctioned addresses. Furthermore, punished entities demonstrate a preference for utilising rapid exchange services to convert their funds, rather than employing dedicated money laundering services. To the best of our knowledge, this study offers valuable insights into how entities use crypto assets to circumvent sanctions.",
    "summary": "This study analyzes the effectiveness of financial sanctions on entities using Bitcoin, finding that while sanctions deter some, others continue illicit crypto activity, often using rapid exchange services to move funds. The research highlights the challenges of enforcing sanctions within the decentralized nature of the cryptocurrency ecosystem."
  },
  {
    "url": "https://arxiv.org/abs/2410.05642",
    "title": "Minimally Intrusive Access Management to Content Delivery Networks based on Performance Models and Access Patterns",
    "published_date": "2024-10-08",
    "abstract": "This paper presents an approach to managing access to Content Delivery Networks (CDNs), focusing on combating the misuse of tokens through performance analysis and statistical access patterns. In particular, we explore the impact of token sharing on the content delivery infrastructure, proposing the definition of acceptable request limits to detect and block abnormal accesses. Additionally, we introduce countermeasures against piracy, such as degrading the quality of service for pirate users to discourage them from illegal sharing, and using queuing models to quantify system performance in different piracy scenarios. Adopting these measures can improve the consistency and efficiency of CDN access and cost management, protecting the infrastructure and the legitimate user experience.",
    "summary": "This paper proposes a minimally intrusive CDN access management system that uses performance modeling and access pattern analysis to detect and mitigate token misuse and piracy, improving CDN efficiency and protecting legitimate users. The approach involves defining acceptable request limits, degrading service for suspected pirates, and using queuing models to analyze system performance under various piracy scenarios."
  },
  {
    "url": "https://arxiv.org/abs/2408.16074",
    "title": "Verification methods for international AI agreements",
    "published_date": "2024-08-28",
    "abstract": "What techniques can be used to verify compliance with international agreements about advanced AI development? In this paper, we examine 10 verification methods that could detect two types of potential violations: unauthorized AI training (e.g., training runs above a certain FLOP threshold) and unauthorized data centers. We divide the verification methods into three categories: (a) national technical means (methods requiring minimal or no access from suspected non-compliant nations), (b) access-dependent methods (methods that require approval from the nation suspected of unauthorized activities), and (c) hardware-dependent methods (methods that require rules around advanced hardware). For each verification method, we provide a description, historical precedents, and possible evasion techniques. We conclude by offering recommendations for future work related to the verification and enforcement of international AI governance agreements.",
    "summary": "This paper analyzes ten verification methods for international AI agreements, categorizing them by access requirements (national technical means, access-dependent, and hardware-dependent), to detect unauthorized AI training and data centers, and discusses potential evasion techniques."
  },
  {
    "url": "https://arxiv.org/abs/2406.14724",
    "title": "An Exploratory Mixed-Methods Study on General Data Protection Regulation (GDPR) Compliance in Open-Source Software",
    "published_date": "2024-06-20",
    "abstract": "Background: Governments worldwide are considering data privacy regulations. These laws, e.g. the European Union's General Data Protection Regulation (GDPR), require software developers to meet privacy-related requirements when interacting with users' data. Prior research describes the impact of such laws on software development, but only for commercial software. Open-source software is commonly integrated into regulated software, and thus must be engineered or adapted for compliance. We do not know how such laws impact open-source software development. Aims: To understand how data privacy laws affect open-source software development. We studied the European Union's GDPR, the most prominent such law. We investigated how GDPR compliance activities influence OSS developer activity (RQ1), how OSS developers perceive fulfilling GDPR requirements (RQ2), the most challenging GDPR requirements to implement (RQ3), and how OSS developers assess GDPR compliance (RQ4). Method: We distributed an online survey to explore perceptions of GDPR implementations from open-source developers (N=56). We further conducted a repository mining study to analyze development metrics on pull requests (N=31462) submitted to open-source GitHub repositories. Results: GDPR policies complicate open-source development processes and introduce challenges for developers, primarily regarding the management of users' data, implementation costs and time, and assessments of compliance. Moreover, we observed negative perceptions of GDPR from open-source developers and significant increases in development activity, in particular metrics related to coding and reviewing activity, on GitHub pull requests related to GDPR compliance. Conclusions: Our findings motivate policy-related resources and automated tools to support data privacy regulation implementation and compliance efforts in open-source software.",
    "citation_count": 3,
    "summary": "This mixed-methods study investigated the impact of GDPR on open-source software development, revealing increased developer activity but also negative perceptions and challenges related to data management, costs, and compliance assessment. The findings suggest a need for policy support and automated tools to aid GDPR compliance in open-source projects."
  },
  {
    "url": "https://arxiv.org/abs/2410.10906",
    "title": "Three Decades of Formal Methods in Business Process Compliance: A Systematic Literature Review",
    "published_date": "2024-10-13",
    "abstract": "Digitalization efforts often face a key challenge: business processes must not only be efficient in achieving their goals but also adhere to legal regulations. Business process compliance refers to aligning processes with these regulations. Numerous frameworks have been developed to address this, with the earliest dating back to 1981. This study focuses on rigorous frameworks using formal methods to verify or ensure compliance. We conducted a systematic literature review (SLR) on process compliance frameworks based on formal models. Our goal was to assess the current state of research on process model compliance and identify gaps and opportunities for future work. Starting with 5018 candidate studies from 1981 to the establishment of GDPR, we selected 46 primary studies. These frameworks were categorized by their phases, the languages used for processes and compliance, and their reasoning techniques. We also examined their practical applicability, the case studies they were tested on, the types of users involved, and the skills needed for compliance. Also, we assessed the maturity of each framework. Our findings reveal strong consensus around verification techniques as central to process compliance, though there is less agreement on the earlier and later phases of compliance. Model checking is the dominant technique, but the compliance and process languages have evolved. Most frameworks are still conceptual with prototype implementations, often failing to account for compliance professionals like legal experts or law changes. In conclusion, there is a need for comprehensive empirical studies to better understand the anatomy and maturity of regulatory compliance frameworks, and for robust evaluation methods to benchmark these frameworks. This review offers valuable insights for researchers and practitioners in process compliance.",
    "summary": "This systematic literature review analyzes three decades of formal methods applied to business process compliance, identifying model checking as the dominant verification technique but highlighting a need for more comprehensive empirical studies and robust evaluation methods to improve the practical applicability and maturity of existing frameworks. The review reveals a lack of consensus on phases beyond verification and limited consideration of non-technical users and evolving regulations."
  },
  {
    "url": "https://arxiv.org/abs/2301.06919",
    "title": "Blockchain based resource governance for decentralized web environments",
    "published_date": "2023-01-17",
    "abstract": "Decentralization initiatives such as Solid, Digi.me, and ActivityPub aim to give data owners more control over their data and to level the playing field by enabling small companies and individuals to gain access to data, thus stimulating innovation. However, these initiatives typically use access control mechanisms that cannot verify compliance with usage conditions after access has been granted to others. In this paper, we extend the state of the art by proposing a resource governance conceptual framework, entitled ReGov, that facilitates usage control in decentralized web environments. We subsequently demonstrate how our framework can be instantiated by combining blockchain and trusted execution environments. Through blockchain technologies, we record policies expressing the usage conditions associated with resources and monitor their compliance. Our instantiation employs trusted execution environments to enforce said policies, inside data consumers' devices. We evaluate the framework instantiation through a detailed analysis of requirments derived from a data market motivating scenario, as well as an assessment of the security, privacy, and affordability aspects of our proposal.",
    "citation_count": 7,
    "summary": "ReGov, a novel blockchain-based resource governance framework, enhances decentralized web environments by enabling verifiable compliance with data usage conditions, leveraging blockchain for policy recording and monitoring, and trusted execution environments for on-device enforcement. This approach aims to improve data control and stimulate innovation in decentralized data ecosystems."
  },
  {
    "url": "https://arxiv.org/abs/2309.00382",
    "title": "Towards Cross-Provider Analysis of Transparency Information for Data Protection",
    "published_date": "2023-09-01",
    "abstract": "Transparency and accountability are indispensable principles for modern data protection, from both, legal and technical viewpoints. Regulations such as the GDPR, therefore, require specific transparency information to be provided including, e.g., purpose specifications, storage periods, or legal bases for personal data processing. However, it has repeatedly been shown that all too often, this information is practically hidden in legalese privacy policies, hindering data subjects from exercising their rights. This paper presents a novel approach to enable large-scale transparency information analysis across service providers, leveraging machine-readable formats and graph data science methods. More specifically, we propose a general approach for building a transparency analysis platform (TAP) that is used to identify data transfers empirically, provide evidence-based analyses of sharing clusters of more than 70 real-world data controllers, or even to simulate network dynamics using synthetic transparency information for large-scale data-sharing scenarios. We provide the general approach for advanced transparency information analysis, an open source architecture and implementation in the form of a queryable analysis platform, and versatile analysis examples. These contributions pave the way for more transparent data processing for data subjects, and evidence-based enforcement processes for data protection authorities. Future work can build upon our contributions to gain more insights into so-far hidden data-sharing practices.",
    "summary": "This paper introduces a novel transparency analysis platform (TAP) using machine-readable formats and graph data science to analyze data protection transparency information across multiple service providers. The TAP facilitates empirical identification of data transfers, analysis of data sharing, and simulations, ultimately promoting greater transparency for data subjects and enforcement for authorities."
  },
  {
    "url": "https://arxiv.org/pdf/2306.15497.pdf",
    "title": "Identifying Practical Challenges in the Implementation of Technical Measures for Data Privacy Compliance",
    "published_date": "2023-06-27",
    "abstract": "Modern privacy regulations provide a strict mandate for data processing entities to implement appropriate technical measures to demonstrate compliance. In practice, determining what measures are indeed\"appropriate\"is not trivial, particularly in light of vague guidelines provided by privacy regulations. To exacerbate the issue, challenges arise not only in the implementation of the technical measures themselves, but also in a variety of factors involving the roles, processes, decisions, and culture surrounding the pursuit of privacy compliance. In this paper, we present 33 challenges faced in the implementation of technical measures for privacy compliance, derived from a qualitative analysis of 16 interviews with privacy professionals. In addition, we evaluate the interview findings in a survey study, which gives way to a discussion of the identified challenges and their implications.",
    "citation_count": 4,
    "summary": "This paper identifies 33 practical challenges in implementing technical data privacy measures, based on interviews with privacy professionals and a subsequent survey, highlighting difficulties in interpreting regulations and navigating organizational factors beyond purely technical considerations."
  },
  {
    "url": "https://www.lesswrong.com/posts/cCbybRT8bgiMbEHEv/a-list-of-all-the-deadlines-in-biden-s-executive-order-on-ai",
    "author": "Ricki Heicklen",
    "title": "Toward a Broader Conception of Adverse Selection",
    "published_date": "2023-11-01",
    "summary": "President Biden's October 30, 2023 executive order on AI outlines numerous deadlines for federal agencies, ranging from 30 to 90 days, to produce reports, assessments, and plans related to AI development, workforce needs, risk management, and ethical considerations across various sectors. These actions aim to coordinate federal AI efforts and address potential risks and societal impacts."
  }
]