[
  {
    "url": "https://arxiv.org/abs/2402.12907",
    "title": "Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects",
    "published_date": "2024-02-20",
    "abstract": "The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions.",
    "citation_count": 2,
    "summary": "This paper introduces the Incentive Compatibility Sociotechnical Alignment Problem (ICSAP), arguing that current AI alignment approaches neglect the sociotechnical context and proposing the use of game-theoretic principles like mechanism design, contract theory, and Bayesian persuasion to better align AI with societal goals."
  },
  {
    "url": "https://arxiv.org/pdf/2010.00403.pdf",
    "title": "Mediating artificial intelligence developments through negative and positive incentives",
    "published_date": "2020-10-01",
    "abstract": "The field of Artificial Intelligence (AI) is going through a period of great expectations, introducing a certain level of anxiety in research, business and also policy. This anxiety is further energised by an AI race narrative that makes people believe they might be missing out. Whether real or not, a belief in this narrative may be detrimental as some stake-holders will feel obliged to cut corners on safety precautions, or ignore societal consequences just to “win”. Starting from a baseline model that describes a broad class of technology races where winners draw a significant benefit compared to others (such as AI advances, patent race, pharmaceutical technologies), we investigate here how positive (rewards) and negative (punishments) incentives may beneficially influence the outcomes. We uncover conditions in which punishment is either capable of reducing the development speed of unsafe participants or has the capacity to reduce innovation through over-regulation. Alternatively, we show that, in several scenarios, rewarding those that follow safety measures may increase the development speed while ensuring safe choices. Moreover, in the latter regimes, rewards do not suffer from the issue of over-regulation as is the case for punishment. Overall, our findings provide valuable insights into the nature and kinds of regulatory actions most suitable to improve safety compliance in the contexts of both smooth and sudden technological shifts.",
    "citation_count": 41,
    "summary": "This paper models the impact of positive and negative incentives on AI development speed and safety, finding that rewarding safe practices is more effective than punishing unsafe ones in promoting both innovation and responsible AI development. The analysis considers scenarios with both gradual and rapid technological advancements."
  },
  {
    "url": "https://www.alignmentforum.org/tag/mechanism-design",
    "author": "johnswentworth",
    "title": "Mechanism Design - AI Alignment Forum",
    "published_date": "2020-11-08",
    "summary": "Mechanism design is the engineering application of game theory, focusing on creating incentive structures that guide self-interested agents toward a predetermined, desirable outcome. This applies across diverse fields including institution design, voting systems, and market mechanisms."
  },
  {
    "url": "https://www.lesswrong.com/tag/mechanism-design",
    "author": "badger",
    "title": "Mechanism Design - LessWrong",
    "published_date": "2014-04-30",
    "summary": "Mechanism design is the engineering application of game theory, focusing on creating incentive structures that guide self-interested agents toward a pre-determined, socially desirable outcome. This applies to diverse fields like voting systems, auctions, and market regulation."
  },
  {
    "url": "https://arxiv.org/abs/2411.18393",
    "title": "Exploring the Impact of Rewards on Developers' Proactive AI Accountability Behavior",
    "published_date": "2024-11-27",
    "abstract": "The rapid integration of Artificial Intelligence (AI)-based systems offers benefits for various domains of the economy and society but simultaneously raises concerns due to emerging scandals. These scandals have led to the increasing importance of AI accountability to ensure that actors provide justification and victims receive compensation. However, AI accountability has a negative connotation due to its emphasis on penalizing sanctions, resulting in reactive approaches to emerging concerns. To counteract the prevalent negative view and offer a proactive approach to facilitate the AI accountability behavior of developers, we explore rewards as an alternative mechanism to sanctions. We develop a theoretical model grounded in Self-Determination Theory to uncover the potential impact of rewards and sanctions on AI developers. We further identify typical sanctions and bug bounties as potential reward mechanisms by surveying related research from various domains, including cybersecurity.",
    "summary": "This paper investigates the potential of reward systems, such as bug bounties, to incentivize proactive AI accountability behaviors among developers, contrasting this approach with the less effective, reactive nature of traditional sanctions. A theoretical model based on Self-Determination Theory is used to explore the impact of both rewards and sanctions."
  },
  {
    "url": "http://arxiv.org/abs/2401.09718",
    "title": "AI and the Opportunity for Shared Prosperity: Lessons from the History of Technology and the Economy",
    "published_date": "2024-01-18",
    "abstract": "Recent progress in artificial intelligence (AI) marks a pivotal moment in human history. It presents the opportunity for machines to learn, adapt, and perform tasks that have the potential to assist people, from everyday activities to their most creative and ambitious projects. It also has the potential to help businesses and organizations harness knowledge, increase productivity, innovate, transform, and power shared prosperity. This tremendous potential raises two fundamental questions: (1) Will AI actually advance national and global economic transformation to benefit society at large? and (2) What issues must we get right to fully realize AI's economic value, expand prosperity and improve lives everywhere? We explore these questions by considering the recent history of technology and innovation as a guide for the likely impact of AI and what we must do to realize its economic potential to benefit society. While we do not presume the future will be entirely like that past, for reasons we will discuss, we do believe prior experience with technological change offers many useful lessons. We conclude that while progress in AI presents a historic opportunity to advance our economic prosperity and future wellbeing, its economic benefits will not come automatically and that AI risks exacerbating existing economic challenges unless we collectively and purposefully act to enable its potential and address its challenges. We suggest a collective policy agenda - involving developers, deployers and users of AI, infrastructure providers, policymakers, and those involved in workforce training - that may help both realize and harness AI's economic potential and address its risks to our shared prosperity.",
    "citation_count": 1,
    "summary": "This paper examines the potential of AI to drive shared prosperity, drawing parallels with historical technological advancements. It argues that realizing AI's economic benefits requires proactive policies addressing both its opportunities and risks to ensure inclusive growth."
  },
  {
    "url": "https://arxiv.org/abs/2305.02561",
    "title": "Beneficence Signaling in AI Development Dynamics",
    "published_date": "2023-05-04",
    "abstract": "This paper motivates and develops a framework for understanding how the socio-technical systems surrounding AI development interact with social welfare. It introduces the concept of ``signaling'' from evolutionary game theory and demonstrates how it can enhance existing theory and practice surrounding the evaluation and governance of AI systems.",
    "summary": "This paper proposes a framework for analyzing the interplay between AI development and social welfare, using the concept of beneficence signaling from game theory to improve AI evaluation and governance. It explores how socio-technical systems influence and are influenced by efforts to ensure AI benefits society."
  },
  {
    "url": "https://www.lesswrong.com/posts/2eaLH7zp6pxdQwYSH",
    "author": "Austin Witte",
    "title": "A Brief Overview of AI Safety/Alignment Orgs, Fields, Researchers, and Resources for ML Researchers",
    "published_date": "2023-02-02",
    "summary": "Two overview documents, a short and long version, catalog AI safety research, organizations, and researchers to help machine learning experts quickly assess potential areas of contribution based on their existing skills and interests. The resource aims to facilitate entry into the AI alignment field by providing easily accessible summaries and keyword tagging."
  },
  {
    "url": "https://arxiv.org/pdf/2201.11441.pdf",
    "title": "Human-centered mechanism design with Democratic AI",
    "published_date": "2022-01-27",
    "abstract": "Building artificial intelligence (AI) that aligns with human values is an unsolved problem. Here, we developed a human-in-the-loop research pipeline called Democratic AI, in which reinforcement learning is used to design a social mechanism that humans prefer by majority. A large group of humans played an online investment game that involved deciding whether to keep a monetary endowment or to share it with others for collective benefit. Shared revenue was returned to players under two different redistribution mechanisms, one designed by the AI and the other by humans. The AI discovered a mechanism that redressed initial wealth imbalance, sanctioned free riders, and successfully won the majority vote. By optimizing for human preferences, Democratic AI may be a promising method for value-aligned policy innovation.",
    "citation_count": 2,
    "summary": "Democratic AI, a human-in-the-loop reinforcement learning system, designed a social mechanism preferred by a majority of human participants in an online investment game, demonstrating its potential for value-aligned policy innovation by optimizing for human preferences. This mechanism successfully addressed wealth inequality and punished free-riders."
  },
  {
    "url": "https://www.lesswrong.com/tag/reward-functions",
    "author": "TurnTrout",
    "title": "Reward Functions - LessWrong",
    "published_date": "2022-07-25",
    "summary": "In reinforcement learning, a reward function assigns numerical values to actions or outcomes, guiding an AI towards desired behaviors by optimizing for these rewards. Designing effective reward functions that prevent unintended consequences is a crucial, and often difficult, aspect of AI development."
  },
  {
    "url": "https://arxiv.org/pdf/2112.10190.pdf",
    "title": "Demanding and Designing Aligned Cognitive Architectures",
    "published_date": "2021-12-19",
    "abstract": "With AI systems becoming more powerful and pervasive, there is increasing debate about keeping their actions aligned with the broader goals and needs of humanity. This multi-disciplinary and multi-stakeholder debate must resolve many issues, here we examine three of them. The first issue is to clarify what demands stakeholders might usefully make on the designers of AI systems, useful because the technology exists to implement them. We make this technical topic more accessible by using the framing of cognitive architectures. The second issue is to move beyond an analytical framing that treats useful intelligence as being reward maximization only. To support this move, we define several AI cognitive architectures that combine reward maximization with other technical elements designed to improve alignment. The third issue is how stakeholders should calibrate their interactions with modern machine learning researchers. We consider how current fashions in machine learning create a narrative pull that participants in technical and policy discussions should be aware of, so that they can compensate for it. We identify several technically tractable but currently unfashionable options for improving AI alignment.",
    "citation_count": 1,
    "summary": "This paper analyzes the alignment problem in AI, arguing for clearer stakeholder demands on AI designers using the framework of cognitive architectures, proposing architectures beyond reward maximization, and advising on effective stakeholder interaction with machine learning researchers to pursue technically feasible alignment solutions."
  },
  {
    "url": "https://arxiv.org/abs/2101.06060v1",
    "title": "The Challenge of Value Alignment: from Fairer Algorithms to AI Safety",
    "published_date": "2021-01-15",
    "abstract": "This paper addresses the question of how to align AI systems with human values and situates it within a wider body of thought regarding technology and value. Far from existing in a vacuum, there has long been an interest in the ability of technology to 'lock-in' different value systems. There has also been considerable thought about how to align technologies with specific social values, including through participatory design-processes. In this paper we look more closely at the question of AI value alignment and suggest that the power and autonomy of AI systems gives rise to opportunities and challenges in the domain of value that have not been encountered before. Drawing important continuities between the work of the fairness, accountability, transparency and ethics community, and work being done by technical AI safety researchers, we suggest that more attention needs to be paid to the question of 'social value alignment' - that is, how to align AI systems with the plurality of values endorsed by groups of people, especially on the global level.",
    "citation_count": 34,
    "summary": "This paper examines the challenge of aligning AI systems with human values, arguing that the unique power and autonomy of AI necessitate a broader approach than previous technological value alignment efforts. It emphasizes the need for \"social value alignment,\" focusing on aligning AI with the diverse values of global populations."
  },
  {
    "url": "https://arxiv.org/abs/2105.08475",
    "title": "AI and Shared Prosperity",
    "published_date": "2021-05-18",
    "abstract": "Future advances in AI that automate away human labor may have stark implications for labor markets and inequality. This paper proposes a framework to analyze the effects of specific types of AI systems on the labor market, based on how much labor demand they will create versus displace, while taking into account that productivity gains also make society wealthier and thereby contribute to additional labor demand. This analysis enables ethically-minded companies creating or deploying AI systems as well as researchers and policymakers to take into account the effects of their actions on labor markets and inequality, and therefore to steer progress in AI in a direction that advances shared prosperity and an inclusive economic future for all of humanity.",
    "citation_count": 25,
    "summary": "This paper presents a framework for assessing the impact of AI on labor markets and inequality, considering both job displacement and the creation of new jobs due to increased productivity, aiming to guide the ethical development and deployment of AI for shared prosperity."
  },
  {
    "url": "https://arxiv.org/pdf/2112.10859v1.pdf",
    "title": "Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning",
    "published_date": "2021-12-20",
    "abstract": "Critical sectors of human society are progressing toward the adoption of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents' behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the principle of online cross-validation, the incentive designer explicitly accounts for its impact on agents' learning and, through them, the impact on future social welfare. Experiments on didactic benchmark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a complex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis.",
    "citation_count": 19,
    "summary": "This paper introduces a model-free meta-gradient reinforcement learning method for adaptive incentive design in multi-agent systems, enabling a central planner to indirectly guide self-interested agents towards socially optimal behavior by adjusting their payoffs. Experiments demonstrate its effectiveness in inducing cooperation and achieving superior outcomes compared to simpler methods, particularly in complex scenarios like simulated economies."
  },
  {
    "title": "AI and Shared Prosperity",
    "abstract": "Future advances in AI that automate away human labor may have stark implications for labor markets and inequality. This paper proposes a framework to analyze the effects of specific types of AI systems on the labor market, based on how much labor demand they will create versus displace, while taking into account that productivity gains also make society wealthier and thereby contribute to additional labor demand. This analysis enables ethically-minded companies creating or deploying AI systems as well as researchers and policymakers to take into account the effects of their actions on labor markets and inequality, and therefore to steer progress in AI in a direction that advances shared prosperity and an inclusive economic future for all of humanity.",
    "published_date": "2021-05-18",
    "citation_count": 25,
    "url": "https://dl.acm.org/doi/10.1145/3461702.3462619",
    "summary": "This paper presents a framework for analyzing AI's impact on labor markets and inequality, considering both job displacement and the creation of new jobs due to increased productivity, aiming to guide ethical AI development for shared prosperity."
  }
]