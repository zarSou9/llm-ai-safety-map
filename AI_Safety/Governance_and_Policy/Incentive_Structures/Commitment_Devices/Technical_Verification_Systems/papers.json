[
  {
    "url": "https://arxiv.org/abs/2411.00069",
    "title": "Meta-Sealing: A Revolutionizing Integrity Assurance Protocol for Transparent, Tamper-Proof, and Trustworthy AI System",
    "published_date": "2024-10-31",
    "abstract": "The Artificial intelligence in critical sectors-healthcare, finance, and public safety-has made system integrity paramount for maintaining societal trust. Current verification methods for AI systems lack comprehensive lifecycle assurance, creating significant vulnerabilities in deployment of both powerful and trustworthy AI. This research introduces Meta-Sealing, a cryptographic framework that fundamentally changes integrity verification in AI systems throughout their operational lifetime. Meta-Sealing surpasses traditional integrity protocols through its implementation of cryptographic seal chains, establishing verifiable, immutable records for all system decisions and transformations. The framework combines advanced cryptography with distributed verification, delivering tamper-evident guarantees that achieve both mathematical rigor and computational efficiency. Our implementation addresses urgent regulatory requirements for AI system transparency and auditability. The framework integrates with current AI governance standards, specifically the EU's AI Act and FDA's healthcare AI guidelines, enabling organizations to maintain operational efficiency while meeting compliance requirements. Testing on financial institution data demonstrated Meta-Sealing's capability to reduce audit timeframes by 62% while enhancing stakeholder confidence by 47%. Results can establish a new benchmark for integrity assurance in enterprise AI deployments. This research presents Meta-Sealing not merely as a technical solution, but as a foundational framework ensuring AI system integrity aligns with human values and regulatory requirements. As AI continues to influence critical decisions, provides the necessary bridge between technological advancement and verifiable trust. Meta-Sealing serves as a guardian of trust, ensuring that the AI systems we depend on are as reliable and transparent as they are powerful.",
    "summary": "Meta-Sealing is a novel cryptographic framework providing comprehensive, tamper-proof integrity assurance for AI systems throughout their lifecycle, using cryptographic seal chains to create verifiable records of all system actions and significantly improving audit efficiency and stakeholder trust. This framework aligns with emerging AI governance regulations, enhancing transparency and accountability."
  },
  {
    "url": "https://arxiv.org/abs/2411.18798",
    "title": "Formal Verification of Digital Twins with TLA and Information Leakage Control",
    "published_date": "2024-11-27",
    "abstract": "Verifying the correctness of a digital twin provides a formal guarantee that the digital twin operates as intended. Digital twin verification is challenging due to the presence of uncertainties in the virtual representation, the physical environment, and the bidirectional flow of information between physical and virtual. A further challenge is that a digital twin of a complex system is composed of distributed components. This paper presents a methodology to specify and verify digital twin behavior, translating uncertain processes into a formally verifiable finite state machine. We use the Temporal Logic of Actions (TLA) to create a specification, an implementation abstraction that defines the properties required for correct system behavior. Our approach includes a novel weakening of formal security properties, allowing controlled information leakage while preserving theoretical guarantees. We demonstrate this approach on a digital twin of an unmanned aerial vehicle, verifying synchronization of physical-to-virtual and virtual-to-digital data flows to detect unintended misalignments.",
    "summary": "This paper proposes a TLA-based methodology for formally verifying digital twin behavior, addressing challenges posed by uncertainty and distributed components by modeling uncertain processes as finite state machines and employing a novel approach to controlled information leakage. The method is demonstrated on an unmanned aerial vehicle digital twin, verifying data synchronization between physical and virtual components."
  },
  {
    "url": "https://arxiv.org/abs/2406.17455",
    "title": "Smart Casual Verification of the Confidential Consortium Framework",
    "published_date": "2024-06-25",
    "abstract": "The Confidential Consortium Framework (CCF) is an open-source platform for developing trustworthy and reliable cloud applications. CCF powers Microsoft's Azure Confidential Ledger service and as such it is vital to build confidence in the correctness of CCF's design and implementation. This paper reports our experiences applying smart casual verification to validate the correctness of CCF's novel distributed protocols, focusing on its unique distributed consensus protocol and its custom client consistency model. We use the term smart casual verification to describe our hybrid approach, which combines the rigor of formal specification and model checking with the pragmatism of automated testing, in our case binding the formal specification in TLA+ to the C++ implementation. While traditional formal methods approaches require substantial buy-in and are often one-off efforts by domain experts, we have integrated our smart casual verification approach into CCF's CI pipeline, allowing contributors to continuously validate CCF as it evolves. We describe the challenges we faced in applying smart casual verification to a complex existing codebase and how we overcame them to find six subtle bugs in the design and implementation before they could impact production",
    "citation_count": 1,
    "summary": "This paper details a \"smart casual\" verification approach—combining formal specification (TLA+) and automated testing—used to validate the Confidential Consortium Framework (CCF)'s distributed protocols, identifying six bugs within its C++ implementation and integrating this process into CCF's CI pipeline. The method bridges the rigor of formal methods with practical application, enabling continuous validation of the evolving codebase."
  },
  {
    "url": "https://arxiv.org/abs/2409.09174",
    "title": "Incorporation of Verifier Functionality in the Software for Operations and Network Attack Results Review and the Autonomous Penetration Testing System",
    "published_date": "2024-09-13",
    "abstract": "The software for operations and network attack results review (SONARR) and the autonomous penetration testing system (APTS) use facts and common properties in digital twin networks to represent real-world entities. However, in some cases fact values will change regularly, making it difficult for objects in SONARR and APTS to consistently and accurately represent their real-world counterparts. This paper proposes and evaluates the addition of verifiers, which check real-world conditions and update network facts, to SONARR. This inclusion allows SONARR to retrieve fact values from its executing environment and update its network, providing a consistent method of ensuring that the operations and, therefore, the results align with the real-world systems being assessed. Verifiers allow arbitrary scripts and dynamic arguments to be added to normal SONARR operations. This provides a layer of flexibility and consistency that results in more reliable output from the software.",
    "summary": "This paper proposes adding \"verifier\" functionality to the Software for Operations and Network Attack Results Review (SONARR) system to dynamically update its network representation based on real-world conditions, ensuring consistent alignment between simulated and actual systems during penetration testing. This enhances the reliability of SONARR's output by incorporating real-time verification of facts within the digital twin network."
  },
  {
    "url": "https://www.lesswrong.com/posts/cCbybRT8bgiMbEHEv/a-list-of-all-the-deadlines-in-biden-s-executive-order-on-ai",
    "author": "Ricki Heicklen",
    "title": "Toward a Broader Conception of Adverse Selection",
    "published_date": "2023-11-01",
    "summary": "Biden's October 30, 2023 Executive Order on AI outlines numerous deadlines for various federal agencies to submit reports, develop strategies, and implement plans related to AI's responsible development and use across sectors, with key deadlines ranging from 30 to 90 days after the order's publication."
  },
  {
    "url": "https://arxiv.org/abs/2211.03490",
    "title": "Detecting and Preventing Credential Misuse in OTP-Based Two and Half Factor Authentication Toward Centralized Services Utilizing Blockchain-Based Identity Management",
    "published_date": "2022-11-07",
    "abstract": "This paper focuses on the problem of detection and prevention of stolen and misused secrets (such as private keys) for authentication toward centralized services. We propose a solution for this problem, based on SmartOTPs, the two-factor authentication scheme against the blockchain, which is intended for smart contract wallets and utilizes one-time passwords (OTPs). We modify SmartOTPs for our purposes and utilize them in the setting of two-and-a-half-factor authentication against a centralized service provider. Out of two and a half factors of our solution, the first factor stands for the private key, and the second and a half factor stands for OTPs and their precursors (a.k.a., pre-images), where OTPs are obtained from the precursors by cryptoaraphically secure hashing. We describe the protocol for bootstrapping our approach as well as the authentication procedure. In the case of stolen creden-tials from the client, we show that our solution enables the user to immediately detect it and proceed to re-initialization with fresh credentials. We utilize blockchain-based identity management and decentralized identities of users to simplify the overhead of the registration process and reinitialization.",
    "summary": "This paper proposes a two-and-a-half-factor authentication system using SmartOTPs and blockchain-based identity management to detect and prevent credential misuse in centralized services, enabling users to quickly identify and recover from stolen credentials. The system leverages OTPs and their pre-images, along with a private key, for enhanced security."
  },
  {
    "url": "https://arxiv.org/abs/2211.08162",
    "title": "Single Squaring Verifiable Delay Function from Time-lock Puzzle in the Group of Known Order",
    "published_date": "2022-11-15",
    "abstract": "A Verifiable Delay Function (VDF) is a function that takes a specified sequential time $T$ to be evaluated, but can be verified in $\\Omega(\\log{T})$-time. For meaningful security, $T$ can be at most subexponential in the security parameter $\\lambda$ but has no lower bound. VDFs are useful in several applications ranging from randomness beacons to sustainable blockchains but are really rare in practice. The verification in all of these VDFs requires $\\Omega(\\lambda, \\log{T})$ sequential time. This paper derives a verifiable delay function that is verifiable in $O(1)$-sequential time. The key observation is that the prior works use subexponentially-hard algebraic assumptions for their sequentiality. On the contrary, we derive our VDF from a polynomially-hard sequential assumption namely the time-lock puzzle over the group of known order. In particular, we show that time-lock puzzle can be sequentially-hard even when the order of the group is known but the delay parameter is polynomially-bounded i.e., $T \\le \\mathtt{poly}(\\lambda)$. As an add-on advantage, our VDF requires only one sequential squaring to verify. Thus, in our VDF, the sequential effort required for verification is fixed and independent of the delay parameter $T$.",
    "summary": "This paper presents a novel verifiable delay function (VDF) based on the polynomially-hard time-lock puzzle in a group of known order, achieving constant-time verification ($O(1)$) using only a single sequential squaring, unlike prior VDFs requiring significantly more verification time."
  },
  {
    "url": "https://arxiv.org/abs/2211.11136",
    "title": "Wood traceability system using blockchain and zero-knowledge proof",
    "published_date": "2022-11-21",
    "abstract": "Summary: The system proposed in this study uses zero-knowledge proof (ZKP) to verify the traceability of wood recorded in a public blockchain. Wood is a byproduct of several states, ranging from standing trees to logs, lumber, and wood products (hereinafter “wood objects”) . The advantage of using the blockchain for record keeping is that participants can freely record the information at their discretion, without any restrictions. However, the openness of the blockchain may allow a malicious third party to introduce disinformation. In this study, we employ ZKP and near-field communication (NFC) chips to eliminate the possibility of disinformation introduction. ZKP is used to prove/validate changes in the state of wood objects, and the unique nonce associated with that state is encrypted and recorded on an NFC chip. The nonce is concealed and id of the wood object is defined as hash value of this nonce. We developed a prototype system based on an Android application and an Ethereum smart contract. We confirm that wood traceability and verification can be performed using the prototype system.",
    "citation_count": 1,
    "summary": "This paper proposes a wood traceability system using a public blockchain, enhanced by zero-knowledge proofs and NFC chips to verify wood object transitions while preventing data manipulation. A prototype Android application and Ethereum smart contract demonstrate the system's functionality."
  }
]