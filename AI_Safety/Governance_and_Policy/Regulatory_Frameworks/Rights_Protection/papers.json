[
  {
    "url": "https://arxiv.org/abs/2210.02667",
    "title": "A Human Rights-Based Approach to Responsible AI",
    "published_date": "2022-10-06",
    "abstract": "Research on fairness, accountability, transparency and ethics of AI-based interventions in society has gained much-needed momen-tum in recent years. However it lacks an explicit alignment with a set of normative values and principles that guide this research and interventions. Rather, an implicit consensus is often assumed to hold for the values we impart into our models – something that is at odds with the pluralistic world we live in. In this paper, we put forth the doctrine of universal human rights as a set of globally salient and cross-culturally recognized set of values that can serve as a grounding framework for explicit value alignment in responsible AI – and discuss its eﬃcacy as a framework for civil society partnership and participation. We argue that a human rights framework orients the research in this space away from the machines and the risks of their biases, and towards humans and the risks to their rights, essentially helping to center the conversation around who is harmed, what harms they face, and how those harms may be mitigated.",
    "citation_count": 30,
    "summary": "This paper advocates for a human rights-based framework for responsible AI development, arguing that universal human rights offer a globally applicable and ethically robust set of values to guide research and mitigate potential harms to individuals. This approach shifts the focus from technical biases in AI to the protection of human rights."
  },
  {
    "url": "https://arxiv.org/abs/2410.02769",
    "title": "Fundamentals of legislation for autonomous artificial intelligence systems",
    "published_date": "2024-09-14",
    "abstract": "The paper proposes a method for defining a dedicated operational context as part of the development and deployment of autonomous corporate governance systems. The case study of autonomous board of directors systems is examined. A significant part of the operational context for the autonomous corporate governance systems consists of the regulatory and legal framework that regulates the company's operations. A special operational context for autonomous artificial intelligence systems can be defined by simultaneously formulating local regulatory documents in two versions, i.e., to be used by people and by autonomous systems. In such a case, the artificial intelligence system receives a clearly defined operational context that allows such a system to perform its functions with a required operational quality. Local regulations that take into account the specificity of operations involving individuals and autonomous artificial intelligence systems can become the foundation of the relevant legislation that would regulate the development and deployment of autonomous systems.",
    "summary": "This paper suggests creating dual-version local regulations—one for humans, one for AI—to establish a clear operational context for autonomous corporate governance systems, arguing this approach forms a foundation for legislation governing AI development and deployment."
  },
  {
    "url": "https://arxiv.org/abs/2412.17114",
    "title": "Decentralized Governance of Autonomous AI Agents",
    "published_date": "2024-12-22",
    "abstract": "Autonomous AI agents present transformative opportunities and significant governance challenges. Existing frameworks, such as the EU AI Act and the NIST AI Risk Management Framework, fall short of addressing the complexities of these agents, which are capable of independent decision-making, learning, and adaptation. To bridge these gaps, we propose the ETHOS (Ethical Technology and Holistic Oversight System) framework, a decentralized governance (DeGov) model leveraging Web3 technologies, including blockchain, smart contracts, and decentralized autonomous organizations (DAOs). ETHOS establishes a global registry for AI agents, enabling dynamic risk classification, proportional oversight, and automated compliance monitoring through tools like soulbound tokens and zero-knowledge proofs. Furthermore, the framework incorporates decentralized justice systems for transparent dispute resolution and introduces AI specific legal entities to manage limited liability, supported by mandatory insurance to ensure financial accountability and incentivize ethical design. By integrating philosophical principles of rationality, ethical grounding, and goal alignment, ETHOS aims to create a robust research agenda for promoting trust, transparency, and participatory governance. This innovative framework offers a scalable and inclusive strategy for regulating AI agents, balancing innovation with ethical responsibility to meet the demands of an AI-driven future.",
    "summary": "The paper proposes ETHOS, a decentralized governance framework using Web3 technologies to manage the risks of autonomous AI agents, focusing on a global registry, dynamic risk classification, and decentralized dispute resolution mechanisms. This approach aims to balance innovation with ethical responsibility through transparent and participatory governance."
  },
  {
    "url": "https://www.lesswrong.com/posts/ECnLBSxw4TvpWPnae/ai-model-registries-a-regulatory-review",
    "author": "Deric Cheng, Elliot_Mckernon",
    "title": "AI Model Registries: A Regulatory Review",
    "published_date": "2024-03-22",
    "summary": "This article, part of a series reviewing the 2024 AI regulatory landscape, examines AI model registries—centralized databases tracking AI systems—as a key element of AI governance. These registries, drawing parallels with pharmaceutical regulations, vary widely in implementation across countries like China and the US, mandating varying levels of model information and pre-release safety assessments."
  },
  {
    "url": "https://www.alignmentforum.org/posts/XSqntCNMafhcy9irf/third-party-testing-as-a-key-ingredient-of-ai-policy",
    "author": "Zac Hatfield-Dodds",
    "title": "Third-party testing as a key ingredient of AI policy",
    "published_date": "2024-03-25",
    "summary": "The article advocates for third-party testing of large-scale AI systems to mitigate societal harm from misuse or accidents. This testing regime, crucial for managing current and future AI capabilities, should be narrowly focused on high-impact systems while fostering international cooperation and minimizing burdens on smaller companies."
  },
  {
    "url": "https://www.lesswrong.com/posts/5bd2ChzKKr2Ph5fnL/what-is-compute-governance",
    "author": "Vishakha",
    "title": "What is compute governance?",
    "published_date": "2024-12-23",
    "summary": "Compute governance, focusing on controlling access to AI development hardware, is a promising but underdeveloped AI safety strategy. While current measures are limited, proposed methods aim to improve visibility into AI development, allocate compute resources strategically, and enforce regulations on its use to mitigate existential risks."
  },
  {
    "url": "https://www.lesswrong.com/posts/vzGC4zh73dfcqnFgf/open-source-ai-a-regulatory-review",
    "author": "Elliot_Mckernon, Deric Cheng",
    "title": "Open-Source AI: A Regulatory Review",
    "published_date": "2024-04-29",
    "summary": "This article examines the implications of open-sourcing AI models, focusing on the trade-offs between collaboration and potential harms. While open-source models foster innovation, the unrestricted availability of powerful AI, even with partial open-sourcing, raises concerns about misuse for malicious purposes, highlighting the challenge of balancing accessibility with safety."
  },
  {
    "url": "https://www.alignmentforum.org/tag/regulation-and-ai-risk",
    "author": "KatjaGrace",
    "title": "Regulation and AI Risk - AI Alignment Forum",
    "published_date": "2023-02-07",
    "summary": "The debate on AI regulation centers on whether it can mitigate the risks of unfriendly AI, with proposals ranging from global oversight boards to government funding of safe AI research. However, challenges include the difficulty of global cooperation, the potential for an AI arms race, and the relative ease of concealing AI research."
  },
  {
    "url": "https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research",
    "author": "Roman Leventov",
    "title": "A multi-disciplinary view on AI safety research",
    "published_date": "2023-02-08",
    "summary": "The article advocates for a multidisciplinary approach to AI safety research, arguing that focusing solely on technical aspects is insufficient and potentially dangerous. It emphasizes the need for a holistic, top-down design of \"civilizational intelligence,\" integrating diverse fields like cognitive science, social sciences, and engineering to ensure the safe development of advanced AI."
  },
  {
    "url": "https://www.lesswrong.com/posts/5nDxmAvZ9w5CPa9gR/ai-12-the-quest-for-sane-regulations",
    "author": "Zvi",
    "title": "AI #12:The Quest for Sane Regulations",
    "published_date": "2023-05-18",
    "summary": "Recent advancements in AI, particularly expanded context windows for models like Claude, are overshadowed by intense regulatory discussions. US Senate hearings and proposed EU regulations highlight the global push for AI oversight, creating uncertainty around future model development and accessibility."
  }
]