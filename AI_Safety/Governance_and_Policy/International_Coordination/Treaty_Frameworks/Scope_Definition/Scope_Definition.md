### Mini Description

Frameworks for precisely defining which AI systems and activities fall under treaty governance, including technical criteria and capability thresholds.

### Description

Scope definition in AI treaty frameworks addresses the fundamental challenge of precisely specifying which AI systems, capabilities, and development activities fall under international governance mechanisms. This involves creating clear, technically-grounded criteria that can distinguish regulated systems from unregulated ones, while remaining robust to technological advancement and resistant to exploitation through creative interpretation.

A key challenge is balancing specificity with adaptability - definitions must be precise enough to enable consistent enforcement but flexible enough to remain relevant as technology evolves. Current approaches range from capability-based definitions (e.g., performance on specific benchmarks) to architectural criteria (e.g., model size or training compute) to impact-based measures (e.g., potential for catastrophic harm). Researchers also explore hybrid approaches that combine multiple criteria to create more robust definitions.

Active research questions include developing formal methods for capability assessment, establishing meaningful thresholds that trigger different levels of oversight, and creating definitions that account for emergent properties in AI systems. There is particular focus on addressing edge cases and potential loopholes, such as distributed systems that individually fall below thresholds but collectively exceed them, or systems that can rapidly scale in capability through learning or self-improvement.

### Order

1. Technical_Criteria
2. Impact_Assessment
3. Edge_Case_Analysis
4. Classification_Procedures
5. Definitional_Robustness
