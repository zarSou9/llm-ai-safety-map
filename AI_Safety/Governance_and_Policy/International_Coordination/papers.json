[
  {
    "url": "https://arxiv.org/abs/2307.04699",
    "title": "International Institutions for Advanced AI",
    "published_date": "2023-07-10",
    "abstract": "International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.",
    "citation_count": 12,
    "summary": "This paper proposes four international institutional models—a commission, governance organization, collaborative, and safety project—to address the opportunities and risks of advanced AI, focusing on fostering beneficial development and mitigating potential harms through global cooperation and standard-setting. These models build upon existing organizational precedents and aim to facilitate responsible AI innovation and deployment."
  },
  {
    "url": "https://arxiv.org/abs/2308.15514",
    "title": "International Governance of Civilian AI: A Jurisdictional Certification Approach",
    "published_date": "2023-08-29",
    "abstract": "This report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (AI) and presents one approach in detail. This approach represents the extension of a standards, licensing, and liability regime to the global level. We propose that states establish an International AI Organization (IAIO) to certify state jurisdictions (not firms or AI projects) for compliance with international oversight standards. States can give force to these international standards by adopting regulations prohibiting the import of goods whose supply chains embody AI from non-IAIO-certified jurisdictions. This borrows attributes from models of existing international organizations, such as the International Civilian Aviation Organization (ICAO), the International Maritime Organization (IMO), and the Financial Action Task Force (FATF). States can also adopt multilateral controls on the export of AI product inputs, such as specialized hardware, to non-certified jurisdictions. Indeed, both the import and export standards could be required for certification. As international actors reach consensus on risks of and minimum standards for advanced AI, a jurisdictional certification regime could mitigate a broad range of potential harms, including threats to public safety.",
    "citation_count": 19,
    "summary": "This report proposes an international AI governance model where an International AI Organization certifies state jurisdictions' compliance with AI safety standards, leveraging import/export controls to incentivize global adoption and mitigate potential harms. This approach extends existing international organizational models to the AI domain."
  },
  {
    "url": "https://arxiv.org/abs/2409.02779",
    "title": "Governing dual-use technologies: Case studies of international security agreements and lessons for AI governance",
    "published_date": "2024-09-04",
    "abstract": "International AI governance agreements and institutions may play an important role in reducing global security risks from advanced AI. To inform the design of such agreements and institutions, we conducted case studies of historical and contemporary international security agreements. We focused specifically on those arrangements around dual-use technologies, examining agreements in nuclear security, chemical weapons, biosecurity, and export controls. For each agreement, we examined four key areas: (a) purpose, (b) core powers, (c) governance structure, and (d) instances of non-compliance. From these case studies, we extracted lessons for the design of international AI agreements and governance institutions. We discuss the importance of robust verification methods, strategies for balancing power between nations, mechanisms for adapting to rapid technological change, approaches to managing trade-offs between transparency and security, incentives for participation, and effective enforcement mechanisms.",
    "summary": "This paper analyzes international security agreements governing dual-use technologies (nuclear, chemical, biological) to identify effective governance strategies for AI, focusing on factors such as verification, power balance, adaptability, transparency, incentives, and enforcement. Lessons learned from these case studies inform the design of future international AI governance frameworks."
  },
  {
    "url": "https://arxiv.org/abs/2409.10536",
    "title": "The potential functions of an international institution for AI safety. Insights from adjacent policy areas and recent trends",
    "published_date": "2024-08-31",
    "abstract": "Governments, industry, and other actors involved in governing AI technologies around the world agree that, while AI offers tremendous promise to benefit the world, appropriate guardrails are required to mitigate risks. Global institutions, including the OECD, the G7, the G20, UNESCO, and the Council of Europe, have already started developing frameworks for ethical and responsible AI governance. While these are important initial steps, they alone fall short of addressing the need for institutionalised international processes to identify and assess potentially harmful AI capabilities. Contributing to the relevant conversation on how to address this gap, this chapter reflects on what functions an international AI safety institute could perform. Based on the analysis of both existing international governance models addressing safety considerations in adjacent policy areas and the newly established national AI safety institutes in the UK and US, the chapter identifies a list of concrete functions that could be performed at the international level. While creating a new international body is not the only way forward, understanding the structure of these bodies from a modular perspective can help us to identify the tools at our disposal. These, we suggest, can be categorised under three functional domains: a) technical research and cooperation, b) safeguards and evaluations, c) policymaking and governance support.",
    "summary": "This paper explores the potential functions of a new international institution dedicated to AI safety, drawing insights from existing international governance models and recent national initiatives. It proposes three key functional domains for such an institution: technical research and cooperation, safeguards and evaluations, and policymaking and governance support."
  },
  {
    "url": "https://www.lesswrong.com/posts/zKGyznvDB94aoJgx4/towards-mutually-assured-cooperation",
    "author": "mikko",
    "title": "Towards mutually assured cooperation",
    "published_date": "2024-12-22",
    "summary": "Uncontrolled AI development poses a significant risk of global nuclear war, as nations may preemptively strike to prevent another from achieving total dominance through weaponized AI. International cooperation in AI development is crucial to establishing a safe equilibrium and mitigating this existential threat."
  },
  {
    "url": "https://arxiv.org/pdf/2001.03573.pdf",
    "title": "Should Artificial Intelligence Governance be Centralised?: Design Lessons from History",
    "published_date": "2020-01-10",
    "abstract": "Can effective international governance for artificial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difficulty in securing participation while creating stringent rules. Other considerations depend on the specific design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate.",
    "citation_count": 36,
    "summary": "This paper examines whether centralized or fragmented governance is more effective for international AI regulation, drawing historical parallels to highlight the trade-offs between efficiency and flexibility, concluding that a well-designed centralized system could be beneficial but that fragmentation may persist and requires monitoring."
  },
  {
    "title": "Should Artificial Intelligence Governance be Centralised?: Design Lessons from History",
    "abstract": "Can effective international governance for artificial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difficulty in securing participation while creating stringent rules. Other considerations depend on the specific design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate.",
    "published_date": "2020-01-10",
    "citation_count": 36,
    "url": "https://dl.acm.org/doi/10.1145/3375627.3375857",
    "summary": "This paper examines whether centralized or fragmented governance is more effective for international AI regulation, drawing historical parallels to highlight the trade-offs between efficiency and adaptability. It concludes that while a well-designed centralized body could be beneficial, fragmentation is likely to persist, requiring close monitoring for either self-organization or inadequacy."
  },
  {
    "url": "https://arxiv.org/abs/2409.11314",
    "title": "The Role of AI Safety Institutes in Contributing to International Standards for Frontier AI Safety",
    "published_date": "2024-09-17",
    "abstract": "International standards are crucial for ensuring that frontier AI systems are developed and deployed safely around the world. Since the AI Safety Institutes (AISIs) possess in-house technical expertise, mandate for international engagement, and convening power in the national AI ecosystem while being a government institution, we argue that they are particularly well-positioned to contribute to the international standard-setting processes for AI safety. In this paper, we propose and evaluate three models for AISI involvement: 1. Seoul Declaration Signatories, 2. US (and other Seoul Declaration Signatories) and China, and 3. Globally Inclusive. Leveraging their diverse strengths, these models are not mutually exclusive. Rather, they offer a multi-track system solution in which the central role of AISIs guarantees coherence among the different tracks and consistency in their AI safety focus.",
    "citation_count": 1,
    "summary": "AI Safety Institutes (AISIs), possessing technical expertise and international engagement mandates, are uniquely positioned to contribute to global AI safety standards. The paper proposes and analyzes three models for AISI involvement in international standard-setting processes, advocating for a multi-track approach leveraging their diverse strengths."
  },
  {
    "url": "https://arxiv.org/abs/2407.16903",
    "title": "US-China perspectives on extreme AI risks and global governance",
    "published_date": "2024-06-23",
    "abstract": "The United States and China will play an important role in navigating safety and security challenges relating to advanced artificial intelligence. We sought to better understand how experts in each country describe safety and security threats from advanced artificial intelligence, extreme risks from AI, and the potential for international cooperation. Specifically, we compiled publicly-available statements from major technical and policy leaders in both the United States and China. We focused our analysis on advanced forms of artificial intelligence, such as artificial general intelligence (AGI), that may have the most significant impacts on national and global security. Experts in both countries expressed concern about risks from AGI, risks from intelligence explosions, and risks from AI systems that escape human control. Both countries have also launched early efforts designed to promote international cooperation around safety standards and risk management practices. Notably, our findings only reflect information from publicly available sources. Nonetheless, our findings can inform policymakers and researchers about the state of AI discourse in the US and China. We hope such work can contribute to policy discussions around advanced AI, its global security threats, and potential international dialogues or agreements to mitigate such threats.",
    "summary": "This paper analyzes publicly available statements from US and Chinese AI experts, revealing shared concerns about the extreme risks posed by advanced AI, including uncontrolled AI systems and intelligence explosions, and early efforts toward international cooperation on safety and risk management. The findings aim to inform policymakers and researchers on the current state of AI discourse in both countries."
  },
  {
    "url": "https://arxiv.org/abs/2410.21279",
    "title": "Comparative Global AI Regulation: Policy Perspectives from the EU, China, and the US",
    "published_date": "2024-10-05",
    "abstract": "As a powerful and rapidly advancing dual-use technology, AI offers both immense benefits and worrisome risks. In response, governing bodies around the world are developing a range of regulatory AI laws and policies. This paper compares three distinct approaches taken by the EU, China and the US. Within the US, we explore AI regulation at both the federal and state level, with a focus on California's pending Senate Bill 1047. Each regulatory system reflects distinct cultural, political and economic perspectives. Each also highlights differing regional perspectives on regulatory risk-benefit tradeoffs, with divergent judgments on the balance between safety versus innovation and cooperation versus competition. Finally, differences between regulatory frameworks reflect contrastive stances in regards to trust in centralized authority versus trust in a more decentralized free market of self-interested stakeholders. Taken together, these varied approaches to AI innovation and regulation influence each other, the broader international community, and the future of AI regulation.",
    "summary": "This paper compares AI regulatory approaches in the EU, China, and the US, highlighting the differing cultural, political, and economic perspectives that shape their risk-benefit assessments, balancing safety and innovation, and approaches to centralized versus decentralized regulation. These distinct frameworks influence each other and the global landscape of AI governance."
  },
  {
    "url": "https://arxiv.org/abs/2410.22151",
    "title": "Standardization Trends on Safety and Trustworthiness Technology for Advanced AI",
    "published_date": "2024-10-29",
    "abstract": "Artificial Intelligence (AI) has rapidly evolved over the past decade and has advanced in areas such as language comprehension, image and video recognition, programming, and scientific reasoning. Recent AI technologies based on large language models and foundation models are approaching or surpassing artificial general intelligence. These systems demonstrate superior performance in complex problem solving, natural language processing, and multi-domain tasks, and can potentially transform fields such as science, industry, healthcare, and education. However, these advancements have raised concerns regarding the safety and trustworthiness of advanced AI, including risks related to uncontrollability, ethical conflicts, long-term socioeconomic impacts, and safety assurance. Efforts are being expended to develop internationally agreed-upon standards to ensure the safety and reliability of AI. This study analyzes international trends in safety and trustworthiness standardization for advanced AI, identifies key areas for standardization, proposes future directions and strategies, and draws policy implications. The goal is to support the safe and trustworthy development of advanced AI and enhance international competitiveness through effective standardization.",
    "citation_count": 1,
    "summary": "This paper analyzes international standardization trends for ensuring the safety and trustworthiness of advanced AI systems, identifying key areas needing standardization and proposing future strategies to promote safe AI development and global competitiveness."
  },
  {
    "url": "https://arxiv.org/abs/2410.01819",
    "title": "Strategic AI Governance: Insights from Leading Nations",
    "published_date": "2024-09-16",
    "abstract": "Artificial Intelligence (AI) has the potential to revolutionize various sectors, yet its adoption is often hindered by concerns about data privacy, security, and the understanding of AI capabilities. This paper synthesizes AI governance approaches, strategic themes, and enablers and challenges for AI adoption by reviewing national AI strategies from leading nations. The key contribution is the development of an EPIC (Education, Partnership, Infrastructure, Community) framework, which maps AI implementation requirements to fully realize social impacts and public good from successful and sustained AI deployment. Through a multi-perspective content analysis of the latest AI strategy documents, this paper provides a structured comparison of AI governance strategies across nations. The findings offer valuable insights for governments, academics, industries, and communities to enable responsible and trustworthy AI deployments. Future work should focus on incorporating specific requirements for developing countries and applying the strategies to specific AI applications, industries, and the public sector.",
    "summary": "This paper analyzes national AI strategies from leading nations to identify common themes and challenges in AI governance, proposing an EPIC framework (Education, Partnership, Infrastructure, Community) for responsible AI implementation. The framework aims to facilitate the realization of societal benefits from successful and sustained AI deployment."
  },
  {
    "url": "https://www.alignmentforum.org/posts/6nNwMbdRXZDuNd4Gx/analysis-of-global-ai-governance-strategies",
    "author": "Sammy Martin, Justin Bullock, Corin Katzke",
    "title": "Analysis of Global AI Governance Strategies",
    "published_date": "2024-12-04",
    "summary": "The article analyzes three strategies for governing transformative AI: Cooperative Development, Strategic Advantage, and Global Moratorium. The effectiveness of each strategy depends heavily on the difficulty of aligning AI and the projected timeline for its development, with preferences shifting significantly based on these factors."
  },
  {
    "url": "https://www.alignmentforum.org/posts/82f3o2SuS3pwaZt8Y/paper-in-science-managing-extreme-ai-risks-amid-rapid",
    "author": "JanB",
    "title": "Paper in Science: Managing extreme AI risks amid rapid progress",
    "published_date": "2024-05-23",
    "summary": "Leading AI researchers urge immediate action to address the growing risks of advanced AI systems, proposing a comprehensive plan combining enhanced safety research with proactive governance mechanisms to mitigate potential large-scale harms and loss of human control. This plan draws on lessons from other safety-critical technologies to foster a more commensurate societal response to rapid AI advancements."
  },
  {
    "url": "https://www.lesswrong.com/posts/324pQjqoHEHeF2vPs/ai-clarity-an-initial-research-agenda",
    "author": "Justin Bullock, Corin Katzke, Zershaaneh Qureshi, David_Kristoffersson",
    "title": "AI Clarity: An Initial Research Agenda",
    "published_date": "2024-05-03",
    "summary": "The AI Clarity research program uses scenario planning to analyze potential trajectories of transformative AI (TAI), focusing on scenarios where TAI emerges within the next decade. This research aims to identify existential hazards posed by TAI, evaluate strategies for mitigating those risks, and ultimately recommend effective AI safety and governance approaches."
  }
]