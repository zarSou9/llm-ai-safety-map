[
  {
    "url": "https://arxiv.org/abs/2412.17618",
    "title": "Dynamic safety cases for frontier AI",
    "published_date": "2024-12-23",
    "abstract": "Frontier artificial intelligence (AI) systems present both benefits and risks to society. Safety cases - structured arguments supported by evidence - are one way to help ensure the safe development and deployment of these systems. Yet the evolving nature of AI capabilities, as well as changes in the operational environment and understanding of risk, necessitates mechanisms for continuously updating these safety cases. Typically, in other sectors, safety cases are produced pre-deployment and do not require frequent updates post-deployment, which can be a manual, costly process. This paper proposes a Dynamic Safety Case Management System (DSCMS) to support both the initial creation of a safety case and its systematic, semi-automated revision over time. Drawing on methods developed in the autonomous vehicles (AV) sector - state-of-the-art Checkable Safety Arguments (CSA) combined with Safety Performance Indicators (SPIs) recommended by UL 4600, a DSCMS helps developers maintain alignment between system safety claims and the latest system state. We demonstrate this approach on a safety case template for offensive cyber capabilities and suggest ways it can be integrated into governance structures for safety-critical decision-making. While the correctness of the initial safety argument remains paramount - particularly for high-severity risks - a DSCMS provides a framework for adapting to new insights and strengthening incident response. We outline challenges and further work towards development and implementation of this approach as part of continuous safety assurance of frontier AI systems.",
    "summary": "This paper proposes a Dynamic Safety Case Management System (DSCMS) for continuously updating safety cases of evolving frontier AI systems, leveraging methods from autonomous vehicles to ensure ongoing alignment between safety claims and system performance. The DSCMS facilitates semi-automated revisions, improving incident response and adapting to new insights while maintaining the importance of a sound initial safety argument."
  },
  {
    "url": "https://arxiv.org/abs/2409.16425",
    "title": "Lessons for Editors of AI Incidents from the AI Incident Database",
    "published_date": "2024-09-24",
    "abstract": "As artificial intelligence (AI) systems become increasingly deployed across the world, they are also increasingly implicated in AI incidents - harm events to individuals and society. As a result, industry, civil society, and governments worldwide are developing best practices and regulations for monitoring and analyzing AI incidents. The AI Incident Database (AIID) is a project that catalogs AI incidents and supports further research by providing a platform to classify incidents for different operational and research-oriented goals. This study reviews the AIID's dataset of 750+ AI incidents and two independent taxonomies applied to these incidents to identify common challenges to indexing and analyzing AI incidents. We find that certain patterns of AI incidents present structural ambiguities that challenge incident databasing and explore how epistemic uncertainty in AI incident reporting is unavoidable. We therefore report mitigations to make incident processes more robust to uncertainty related to cause, extent of harm, severity, or technical details of implicated systems. With these findings, we discuss how to develop future AI incident reporting practices.",
    "citation_count": 1,
    "summary": "Analyzing 750+ AI incidents from the AI Incident Database reveals structural ambiguities hindering accurate classification and reporting, highlighting the need for improved incident reporting practices to mitigate epistemic uncertainty regarding cause, harm, and technical details. This study offers recommendations for more robust incident processes."
  },
  {
    "url": "https://arxiv.org/abs/2409.11314",
    "title": "The Role of AI Safety Institutes in Contributing to International Standards for Frontier AI Safety",
    "published_date": "2024-09-17",
    "abstract": "International standards are crucial for ensuring that frontier AI systems are developed and deployed safely around the world. Since the AI Safety Institutes (AISIs) possess in-house technical expertise, mandate for international engagement, and convening power in the national AI ecosystem while being a government institution, we argue that they are particularly well-positioned to contribute to the international standard-setting processes for AI safety. In this paper, we propose and evaluate three models for AISI involvement: 1. Seoul Declaration Signatories, 2. US (and other Seoul Declaration Signatories) and China, and 3. Globally Inclusive. Leveraging their diverse strengths, these models are not mutually exclusive. Rather, they offer a multi-track system solution in which the central role of AISIs guarantees coherence among the different tracks and consistency in their AI safety focus.",
    "citation_count": 1,
    "summary": "AI Safety Institutes (AISIs), possessing unique expertise and influence, are well-suited to contribute to international AI safety standards. The paper proposes and analyzes three models for AISI involvement in this process, advocating for a multi-track approach leveraging their diverse strengths."
  },
  {
    "url": "https://arxiv.org/abs/2409.07985",
    "title": "Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols",
    "published_date": "2024-09-12",
    "abstract": "To evaluate the safety and usefulness of deployment protocols for untrusted AIs, AI Control uses a red-teaming exercise played between a protocol designer and an adversary. This paper introduces AI-Control Games, a formal decision-making model of the red-teaming exercise as a multi-objective, partially observable, stochastic game. We also introduce methods for finding optimal protocols in AI-Control Games, by reducing them to a set of zero-sum partially observable stochastic games. We apply our formalism to model, evaluate and synthesise protocols for deploying untrusted language models as programming assistants, focusing on Trusted Monitoring protocols, which use weaker language models and limited human assistance. Finally, we demonstrate the utility of our formalism by showcasing improvements over empirical studies in existing settings, evaluating protocols in new settings, and analysing how modelling assumptions affect the safety and usefulness of protocols.",
    "citation_count": 2,
    "summary": "This paper models the safety evaluation of AI deployment protocols as multi-objective, partially observable stochastic games, allowing for the formal analysis and synthesis of optimal protocols, particularly demonstrating improvements in trusted monitoring protocols for untrusted language models."
  },
  {
    "url": "https://arxiv.org/abs/2410.21572",
    "title": "Safety cases for frontier AI",
    "published_date": "2024-10-28",
    "abstract": "As frontier artificial intelligence (AI) systems become more capable, it becomes more important that developers can explain why their systems are sufficiently safe. One way to do so is via safety cases: reports that make a structured argument, supported by evidence, that a system is safe enough in a given operational context. Safety cases are already common in other safety-critical industries such as aviation and nuclear power. In this paper, we explain why they may also be a useful tool in frontier AI governance, both in industry self-regulation and government regulation. We then discuss the practicalities of safety cases, outlining how to produce a frontier AI safety case and discussing what still needs to happen before safety cases can substantially inform decisions.",
    "citation_count": 3,
    "summary": "This paper argues that safety cases, structured arguments supported by evidence demonstrating sufficient system safety, are a valuable tool for governing frontier AI, drawing parallels with other high-risk industries. It explores the practical application of safety cases in both industry and government regulation, highlighting current limitations and future needs."
  },
  {
    "url": "https://www.alignmentforum.org/posts/6nNwMbdRXZDuNd4Gx/analysis-of-global-ai-governance-strategies",
    "author": "Sammy Martin, Justin Bullock, Corin Katzke",
    "title": "Analysis of Global AI Governance Strategies",
    "published_date": "2024-12-04",
    "summary": "The article analyzes three AI governance strategies—Cooperative Development, Strategic Advantage, and Global Moratorium—evaluating their effectiveness based on the difficulty of aligning AI and development timelines. The optimal strategy shifts depending on these factors, with Cooperative Development preferred for longer timelines and easier alignment, Strategic Advantage for shorter timelines and moderate difficulty, and Global Moratorium as a last resort for extremely difficult alignment or short timelines."
  },
  {
    "url": "https://www.lesswrong.com/posts/324pQjqoHEHeF2vPs/ai-clarity-an-initial-research-agenda",
    "author": "Justin Bullock, Corin Katzke, Zershaaneh Qureshi, David_Kristoffersson",
    "title": "AI Clarity: An Initial Research Agenda",
    "published_date": "2024-05-03",
    "summary": "The AI Clarity research program uses scenario planning to explore potential pathways to existential risks from transformative AI (TAI), particularly focusing on short timelines (within a decade). The program will then evaluate strategies for AI safety and governance across these scenarios to mitigate existential risk."
  },
  {
    "url": "https://www.lesswrong.com/posts/zKGyznvDB94aoJgx4/towards-mutually-assured-cooperation",
    "author": "mikko",
    "title": "Towards mutually assured cooperation",
    "published_date": "2024-12-22",
    "summary": "Unfettered AI development risks escalating global power imbalances, potentially triggering nuclear war as nations preemptively strike to prevent AI-enabled total dominance. International cooperation in AGI development is crucial to establishing a safe equilibrium and avoiding catastrophic conflict."
  },
  {
    "url": "https://www.alignmentforum.org/posts/Z8bthnjW52uTCFGku/a-narrow-path-a-plan-to-deal-with-ai-extinction-risk",
    "author": "Andrea Miotti; Davekasten; Tolga",
    "title": "A Narrow Path: a plan to deal with AI extinction risk",
    "published_date": "2024-10-07",
    "summary": "The article presents \"A Narrow Path,\" a comprehensive plan to mitigate the existential risk posed by uncontrolled artificial superintelligence. The plan proposes a three-phase approach—focused on safety, stability, and flourishing—to prevent the development of uncontrollable AI for at least 20 years and establish a globally coordinated oversight system."
  },
  {
    "url": "https://arxiv.org/abs/2307.04699",
    "title": "International Institutions for Advanced AI",
    "published_date": "2023-07-10",
    "abstract": "International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.",
    "citation_count": 12,
    "summary": "This paper proposes four international institutional models—a commission, governance organization, collaborative, and safety project—to manage the risks and benefits of advanced AI, focusing on functions like setting safety standards, promoting access, and fostering expert consensus. These models aim to address global externalities and ensure AI benefits humanity."
  }
]