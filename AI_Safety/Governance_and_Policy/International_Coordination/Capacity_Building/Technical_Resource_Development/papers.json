[
  {
    "url": "https://arxiv.org/abs/2409.07878",
    "title": "Mapping Technical Safety Research at AI Companies: A literature review and incentives analysis",
    "published_date": "2024-09-12",
    "abstract": "As AI systems become more advanced, concerns about large-scale risks from misuse or accidents have grown. This report analyzes the technical research into safe AI development being conducted by three leading AI companies: Anthropic, Google DeepMind, and OpenAI. We define safe AI development as developing AI systems that are unlikely to pose large-scale misuse or accident risks. This encompasses a range of technical approaches aimed at ensuring AI systems behave as intended and do not cause unintended harm, even as they are made more capable and autonomous. We analyzed all papers published by the three companies from January 2022 to July 2024 that were relevant to safe AI development, and categorized the 80 included papers into nine safety approaches. Additionally, we noted two categories representing nascent approaches explored by academia and civil society, but not currently represented in any research papers by these leading AI companies. Our analysis reveals where corporate attention is concentrated and where potential gaps lie. Some AI research may stay unpublished for good reasons, such as to not inform adversaries about the details of security techniques they would need to overcome to misuse AI systems. Therefore, we also considered the incentives that AI companies have to research each approach, regardless of how much work they have published on the topic. We identified three categories where there are currently no or few papers and where we do not expect AI companies to become much more incentivized to pursue this research in the future. These are model organisms of misalignment, multi-agent safety, and safety by design. Our findings provide an indication that these approaches may be slow to progress without funding or efforts from government, civil society, philanthropists, or academia.",
    "summary": "This report analyzes published research on AI safety from Anthropic, Google DeepMind, and OpenAI, categorizing 80 papers into nine safety approaches and identifying three areas (model organisms of misalignment, multi-agent safety, and safety by design) where research is lacking and unlikely to increase due to current company incentives."
  },
  {
    "url": "https://arxiv.org/abs/2412.14870",
    "title": "Large-scale School Mapping using Weakly Supervised Deep Learning for Universal School Connectivity",
    "published_date": "2024-12-19",
    "abstract": "Improving global school connectivity is critical for ensuring inclusive and equitable quality education. To reliably estimate the cost of connecting schools, governments and connectivity providers require complete and accurate school location data - a resource that is often scarce in many low- and middle-income countries. To address this challenge, we propose a cost-effective, scalable approach to locating schools in high-resolution satellite images using weakly supervised deep learning techniques. Our best models, which combine vision transformers and convolutional neural networks, achieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging explainable AI techniques, our approach can approximate the precise geographical coordinates of the school locations using only low-cost, classification-level annotations. To demonstrate the scalability of our method, we generate nationwide maps of school location predictions in African countries and present a detailed analysis of our results, using Senegal as our case study. Finally, we demonstrate the immediate usability of our work by introducing an interactive web mapping tool to streamline human-in-the-loop model validation efforts by government partners. This work successfully showcases the real-world utility of deep learning and satellite images for planning regional infrastructure and accelerating universal school connectivity.",
    "summary": "This paper presents a weakly supervised deep learning model that accurately identifies schools in high-resolution satellite imagery, achieving high performance across multiple African countries. The resulting nationwide school location maps, accessible via an interactive web tool, are intended to facilitate cost-effective planning for improved school connectivity."
  },
  {
    "url": "http://arxiv.org/abs/2401.06171",
    "title": "Harnessing Artificial Intelligence for Sustainable Agricultural Development in Africa: Opportunities, Challenges, and Impact",
    "published_date": "2024-01-03",
    "abstract": "This paper explores the transformative potential of artificial intelligence (AI) in the context of sustainable agricultural development across diverse regions in Africa. Delving into opportunities, challenges, and impact, the study navigates through the dynamic landscape of AI applications in agriculture. Opportunities such as precision farming, crop monitoring, and climate-resilient practices are examined, alongside challenges related to technological infrastructure, data accessibility, and skill gaps. The article analyzes the impact of AI on smallholder farmers, supply chains, and inclusive growth. Ethical considerations and policy implications are also discussed, offering insights into responsible AI integration. By providing a nuanced understanding, this paper contributes to the ongoing discourse on leveraging AI for fostering sustainability in African agriculture.",
    "citation_count": 7,
    "summary": "This paper examines the potential of artificial intelligence to improve sustainable agriculture in Africa, analyzing its opportunities (e.g., precision farming), challenges (e.g., infrastructure limitations), and impacts on various stakeholders while considering ethical and policy implications."
  },
  {
    "url": "https://www.lesswrong.com/posts/324pQjqoHEHeF2vPs/ai-clarity-an-initial-research-agenda",
    "author": "Justin Bullock, Corin Katzke, Zershaaneh Qureshi, David_Kristoffersson",
    "title": "AI Clarity: An Initial Research Agenda",
    "published_date": "2024-05-03",
    "summary": "The AI Clarity research program uses scenario planning to explore potential pathways to existential risks from transformative AI (TAI), particularly focusing on short timelines (within a decade). The program will then evaluate strategies for AI safety and governance across these scenarios to identify those best mitigating existential risk."
  },
  {
    "url": "https://www.lesswrong.com/posts/QzQQvGJYDeaDE4Cfg/talent-needs-of-technical-ai-safety-teams",
    "author": "Yams; Carson Jones; McKennaFitzgerald; Ryan Kidd",
    "title": "Talent Needs of Technical AI Safety Teams",
    "published_date": "2024-05-24",
    "summary": "This report summarizes 31 interviews with AI safety experts to identify current talent needs and archetypes within the field. The authors propose three researcher archetypes—Connectors, Iterators, and Amplifiers—to better understand and address the evolving landscape of AI safety talent."
  },
  {
    "url": "https://arxiv.org/pdf/2304.11703.pdf",
    "title": "An Artificial Intelligence-based Framework to Achieve the Sustainable Development Goals in the Context of Bangladesh",
    "published_date": "2023-04-23",
    "abstract": "Sustainable development is a framework for achieving human development goals. It provides natural systems' ability to deliver natural resources and ecosystem services. Sustainable development is crucial for the economy and society. Artificial intelligence (AI) has attracted increasing attention in recent years, with the potential to have a positive influence across many domains. AI is a commonly employed component in the quest for long-term sustainability. In this study, we explore the impact of AI on three pillars of sustainable development: society, environment, and economy, as well as numerous case studies from which we may deduce the impact of AI in a variety of areas, i.e., agriculture, classifying waste, smart water management, and Heating, Ventilation, and Air Conditioning (HVAC) systems. Furthermore, we present AI-based strategies for achieving Sustainable Development Goals (SDGs) which are effective for developing countries like Bangladesh. The framework that we propose may reduce the negative impact of AI and promote the proactiveness of this technology.",
    "summary": "This paper proposes an AI-based framework to achieve the Sustainable Development Goals (SDGs) in Bangladesh, focusing on its application across societal, environmental, and economic pillars through case studies in agriculture, waste management, and smart resource management. The framework aims to maximize AI's positive impact while mitigating potential negative consequences."
  },
  {
    "url": "https://arxiv.org/abs/2307.04699",
    "title": "International Institutions for Advanced AI",
    "published_date": "2023-07-10",
    "abstract": "International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.",
    "citation_count": 12,
    "summary": "This paper proposes four international institutional models—a commission, a governance organization, a collaborative, and a research project—to address the benefits and risks of advanced AI, focusing on facilitating collaboration, setting safety standards, and promoting responsible development. These models aim to harness AI's potential while mitigating its potential harms through international cooperation."
  },
  {
    "url": "https://arxiv.org/abs/2312.04616",
    "title": "Can apparent bystanders distinctively shape an outcome? Global south countries and global catastrophic risk-focused governance of artificial intelligence",
    "published_date": "2023-12-07",
    "abstract": "Increasingly, there is well-grounded concern that through perpetual scaling-up of computation power and data, current deep learning techniques will create highly capable artificial intelligence that could pursue goals in a manner that is not aligned with human values. In turn, such AI could have the potential of leading to a scenario in which there is serious global-scale damage to human wellbeing. Against this backdrop, a number of researchers and public policy professionals have been developing ideas about how to govern AI in a manner that reduces the chances that it could lead to a global catastrophe. The jurisdictional focus of a vast majority of their assessments so far has been the United States, China, and Europe. That preference seems to reveal an assumption underlying most of the work in this field: That global south countries can only have a marginal role in attempts to govern AI development from a global catastrophic risk -focused perspective. Our paper sets out to undermine this assumption. We argue that global south countries like India and Singapore (and specific coalitions) could in fact be fairly consequential in the global catastrophic risk-focused governance of AI. We support our position using 4 key claims. 3 are constructed out of the current ways in which advanced foundational AI models are built and used while one is constructed on the strategic roles that global south countries and coalitions have historically played in the design and use of multilateral rules and institutions. As each claim is elaborated, we also suggest some ways through which global south countries can play a positive role in designing, strengthening and operationalizing global catastrophic risk-focused AI governance.",
    "summary": "This paper challenges the assumption that Global South countries have only a marginal role in AI governance focused on global catastrophic risks, arguing that nations like India and Singapore can be highly influential through their unique positions in AI development and international cooperation. The authors support this claim with four arguments focusing on AI model development and the historical influence of Global South nations in multilateral institutions."
  }
]