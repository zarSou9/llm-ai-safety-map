[
  {
    "url": "https://arxiv.org/abs/2409.11314",
    "title": "The Role of AI Safety Institutes in Contributing to International Standards for Frontier AI Safety",
    "published_date": "2024-09-17",
    "abstract": "International standards are crucial for ensuring that frontier AI systems are developed and deployed safely around the world. Since the AI Safety Institutes (AISIs) possess in-house technical expertise, mandate for international engagement, and convening power in the national AI ecosystem while being a government institution, we argue that they are particularly well-positioned to contribute to the international standard-setting processes for AI safety. In this paper, we propose and evaluate three models for AISI involvement: 1. Seoul Declaration Signatories, 2. US (and other Seoul Declaration Signatories) and China, and 3. Globally Inclusive. Leveraging their diverse strengths, these models are not mutually exclusive. Rather, they offer a multi-track system solution in which the central role of AISIs guarantees coherence among the different tracks and consistency in their AI safety focus.",
    "citation_count": 1,
    "summary": "AI Safety Institutes (AISIs), possessing unique expertise and influence, are well-suited to contribute to international AI safety standards. The paper proposes and analyzes three models for AISI involvement in this crucial standard-setting process, advocating for a multi-track approach leveraging their diverse strengths."
  },
  {
    "url": "https://arxiv.org/abs/2409.10536",
    "title": "The potential functions of an international institution for AI safety. Insights from adjacent policy areas and recent trends",
    "published_date": "2024-08-31",
    "abstract": "Governments, industry, and other actors involved in governing AI technologies around the world agree that, while AI offers tremendous promise to benefit the world, appropriate guardrails are required to mitigate risks. Global institutions, including the OECD, the G7, the G20, UNESCO, and the Council of Europe, have already started developing frameworks for ethical and responsible AI governance. While these are important initial steps, they alone fall short of addressing the need for institutionalised international processes to identify and assess potentially harmful AI capabilities. Contributing to the relevant conversation on how to address this gap, this chapter reflects on what functions an international AI safety institute could perform. Based on the analysis of both existing international governance models addressing safety considerations in adjacent policy areas and the newly established national AI safety institutes in the UK and US, the chapter identifies a list of concrete functions that could be performed at the international level. While creating a new international body is not the only way forward, understanding the structure of these bodies from a modular perspective can help us to identify the tools at our disposal. These, we suggest, can be categorised under three functional domains: a) technical research and cooperation, b) safeguards and evaluations, c) policymaking and governance support.",
    "summary": "This paper explores the potential functions of a new international institution dedicated to AI safety, drawing insights from similar organizations in related fields and recent national initiatives. It proposes three key functional domains for such an institution: technical research and cooperation, safety evaluations and safeguards, and policymaking and governance support."
  },
  {
    "url": "https://arxiv.org/abs/2410.01819",
    "title": "Strategic AI Governance: Insights from Leading Nations",
    "published_date": "2024-09-16",
    "abstract": "Artificial Intelligence (AI) has the potential to revolutionize various sectors, yet its adoption is often hindered by concerns about data privacy, security, and the understanding of AI capabilities. This paper synthesizes AI governance approaches, strategic themes, and enablers and challenges for AI adoption by reviewing national AI strategies from leading nations. The key contribution is the development of an EPIC (Education, Partnership, Infrastructure, Community) framework, which maps AI implementation requirements to fully realize social impacts and public good from successful and sustained AI deployment. Through a multi-perspective content analysis of the latest AI strategy documents, this paper provides a structured comparison of AI governance strategies across nations. The findings offer valuable insights for governments, academics, industries, and communities to enable responsible and trustworthy AI deployments. Future work should focus on incorporating specific requirements for developing countries and applying the strategies to specific AI applications, industries, and the public sector.",
    "summary": "This paper analyzes national AI strategies from leading countries to identify common themes and challenges in AI governance, proposing an EPIC framework (Education, Partnership, Infrastructure, Community) to guide responsible AI implementation and maximize societal benefit. The framework maps requirements for successful and sustained AI deployment, offering insights for stakeholders across sectors."
  },
  {
    "url": "https://arxiv.org/abs/2410.09219",
    "title": "Understanding the First Wave of AI Safety Institutes: Characteristics, Functions, and Challenges",
    "published_date": "2024-10-11",
    "abstract": "In November 2023, the UK and US announced the creation of their AI Safety Institutes (AISIs). Five other jurisdictions have followed in establishing AISIs or similar institutions, with more likely to follow. While there is considerable variation between these institutions, there are also key similarities worth identifying. This primer describes one cluster of similar AISIs, the\"first wave,\"consisting of the Japan, UK, and US AISIs. First-wave AISIs have several fundamental characteristics in common: they are technical government institutions, have a clear mandate related to the safety of advanced AI systems, and lack regulatory powers. Safety evaluations are at the center of first-wave AISIs. These techniques test AI systems across tasks to understand their behavior and capabilities on relevant risks, such as cyber, chemical, and biological misuse. They also share three core functions: research, standards, and cooperation. These functions are critical to AISIs' work on safety evaluations but also support other activities such as scientific consensus-building and foundational AI safety research. Despite its growing popularity as an institutional model, the AISI model is not free from challenges and limitations. Some analysts have criticized the first wave of AISIs for specializing too much in a sub-area and for being potentially redundant with existing institutions, for example. Future developments may rapidly change this landscape, and particularities of individual AISIs may not be captured by our broad-strokes description. This policy brief aims to outline the core elements of first-wave AISIs as a way of encouraging and improving conversations on this novel institutional model, acknowledging this is just a simplified snapshot rather than a timeless prescription.",
    "summary": "The first wave of AI Safety Institutes (AISIs) in Japan, the UK, and the US are government-funded, technically focused institutions dedicated to advanced AI safety evaluation through research, standards development, and international cooperation, but face challenges including potential redundancy and over-specialization. These institutes lack regulatory power, focusing instead on assessing AI system risks and promoting safety."
  },
  {
    "url": "https://www.lesswrong.com/posts/324pQjqoHEHeF2vPs/ai-clarity-an-initial-research-agenda",
    "author": "Justin Bullock, Corin Katzke, Zershaaneh Qureshi, David_Kristoffersson",
    "title": "AI Clarity: An Initial Research Agenda",
    "published_date": "2024-05-03",
    "summary": "The AI Clarity research program uses scenario planning to explore potential pathways to existential risks from transformative AI (TAI), particularly focusing on the possibility of TAI emerging within the next decade. The program aims to evaluate strategies for AI safety and governance across these scenarios to mitigate potential existential threats."
  },
  {
    "url": "https://www.alignmentforum.org/posts/6nNwMbdRXZDuNd4Gx/analysis-of-global-ai-governance-strategies",
    "author": "Sammy Martin, Justin Bullock, Corin Katzke",
    "title": "Analysis of Global AI Governance Strategies",
    "published_date": "2024-12-04",
    "summary": "The article analyzes three strategies for governing transformative AI: Cooperative Development, Strategic Advantage, and Global Moratorium. The effectiveness of each strategy depends heavily on the difficulty of aligning AI and the timeframe for its development, with preferences shifting based on these variables."
  },
  {
    "url": "https://www.lesswrong.com/posts/zKGyznvDB94aoJgx4/towards-mutually-assured-cooperation",
    "author": "mikko",
    "title": "Towards mutually assured cooperation",
    "published_date": "2024-12-22",
    "summary": "Uncontrolled AI development risks escalating global power struggles, potentially triggering nuclear war as nations preemptively strike to prevent AI-enabled total dominance. International cooperation in AGI development is proposed as the safest path to avoid this catastrophic outcome."
  },
  {
    "url": "https://arxiv.org/abs/2307.04699",
    "title": "International Institutions for Advanced AI",
    "published_date": "2023-07-10",
    "abstract": "International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.",
    "citation_count": 12,
    "summary": "This paper proposes four international institutional models—a commission, governance organization, collaborative, and safety project—to address the benefits and risks of advanced AI, focusing on facilitating cooperation, setting safety standards, and promoting responsible development. These models aim to balance fostering innovation with mitigating potential global harms from powerful AI systems."
  },
  {
    "url": "https://arxiv.org/abs/2312.04616",
    "title": "Can apparent bystanders distinctively shape an outcome? Global south countries and global catastrophic risk-focused governance of artificial intelligence",
    "published_date": "2023-12-07",
    "abstract": "Increasingly, there is well-grounded concern that through perpetual scaling-up of computation power and data, current deep learning techniques will create highly capable artificial intelligence that could pursue goals in a manner that is not aligned with human values. In turn, such AI could have the potential of leading to a scenario in which there is serious global-scale damage to human wellbeing. Against this backdrop, a number of researchers and public policy professionals have been developing ideas about how to govern AI in a manner that reduces the chances that it could lead to a global catastrophe. The jurisdictional focus of a vast majority of their assessments so far has been the United States, China, and Europe. That preference seems to reveal an assumption underlying most of the work in this field: That global south countries can only have a marginal role in attempts to govern AI development from a global catastrophic risk -focused perspective. Our paper sets out to undermine this assumption. We argue that global south countries like India and Singapore (and specific coalitions) could in fact be fairly consequential in the global catastrophic risk-focused governance of AI. We support our position using 4 key claims. 3 are constructed out of the current ways in which advanced foundational AI models are built and used while one is constructed on the strategic roles that global south countries and coalitions have historically played in the design and use of multilateral rules and institutions. As each claim is elaborated, we also suggest some ways through which global south countries can play a positive role in designing, strengthening and operationalizing global catastrophic risk-focused AI governance.",
    "summary": "This paper challenges the assumption that Global South countries have only a marginal role in AI governance, arguing that nations like India and Singapore can significantly influence global catastrophic risk-focused AI regulation through their strategic positions in AI development and international cooperation. The authors support this claim with four key arguments, offering suggestions for increased Global South participation in shaping AI governance."
  },
  {
    "url": "https://arxiv.org/abs/2308.15514",
    "title": "International Governance of Civilian AI: A Jurisdictional Certification Approach",
    "published_date": "2023-08-29",
    "abstract": "This report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (AI) and presents one approach in detail. This approach represents the extension of a standards, licensing, and liability regime to the global level. We propose that states establish an International AI Organization (IAIO) to certify state jurisdictions (not firms or AI projects) for compliance with international oversight standards. States can give force to these international standards by adopting regulations prohibiting the import of goods whose supply chains embody AI from non-IAIO-certified jurisdictions. This borrows attributes from models of existing international organizations, such as the International Civilian Aviation Organization (ICAO), the International Maritime Organization (IMO), and the Financial Action Task Force (FATF). States can also adopt multilateral controls on the export of AI product inputs, such as specialized hardware, to non-certified jurisdictions. Indeed, both the import and export standards could be required for certification. As international actors reach consensus on risks of and minimum standards for advanced AI, a jurisdictional certification regime could mitigate a broad range of potential harms, including threats to public safety.",
    "citation_count": 19,
    "summary": "This report proposes an international AI governance model where an International AI Organization certifies state jurisdictions' compliance with AI oversight standards, leveraging import/export controls to incentivize global adoption and mitigate potential harms. This approach extends existing international organizational models to regulate AI at a jurisdictional level rather than focusing on individual firms or projects."
  }
]