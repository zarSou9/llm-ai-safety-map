[
  {
    "url": "https://arxiv.org/abs/2407.01420",
    "title": "Coordinated Disclosure of Dual-Use Capabilities: An Early Warning System for Advanced AI",
    "published_date": "2024-07-01",
    "abstract": "Advanced AI systems may be developed which exhibit capabilities that present significant risks to public safety or security. They may also exhibit capabilities that may be applied defensively in a wide set of domains, including (but not limited to) developing societal resilience against AI threats. We propose Coordinated Disclosure of Dual-Use Capabilities (CDDC) as a process to guide early information-sharing between advanced AI developers, US government agencies, and other private sector actors about these capabilities. The process centers around an information clearinghouse (the\"coordinator\") which receives evidence of dual-use capabilities from finders via mandatory and/or voluntary reporting pathways, and passes noteworthy reports to defenders for follow-up (i.e., further analysis and response). This aims to provide the US government, dual-use foundation model developers, and other actors with an overview of AI capabilities that could significantly impact public safety and security, as well as maximal time to respond.",
    "summary": "The paper proposes a Coordinated Disclosure of Dual-Use Capabilities (CDDC) system for advanced AI, involving an information clearinghouse to facilitate early warning of potentially dangerous or beneficial AI capabilities among developers, government agencies, and other stakeholders. This aims to maximize response time to both threats and opportunities presented by advanced AI."
  },
  {
    "url": "https://arxiv.org/abs/2409.11314",
    "title": "The Role of AI Safety Institutes in Contributing to International Standards for Frontier AI Safety",
    "published_date": "2024-09-17",
    "abstract": "International standards are crucial for ensuring that frontier AI systems are developed and deployed safely around the world. Since the AI Safety Institutes (AISIs) possess in-house technical expertise, mandate for international engagement, and convening power in the national AI ecosystem while being a government institution, we argue that they are particularly well-positioned to contribute to the international standard-setting processes for AI safety. In this paper, we propose and evaluate three models for AISI involvement: 1. Seoul Declaration Signatories, 2. US (and other Seoul Declaration Signatories) and China, and 3. Globally Inclusive. Leveraging their diverse strengths, these models are not mutually exclusive. Rather, they offer a multi-track system solution in which the central role of AISIs guarantees coherence among the different tracks and consistency in their AI safety focus.",
    "citation_count": 1,
    "summary": "AI Safety Institutes (AISIs), possessing technical expertise and international engagement capabilities, are uniquely positioned to contribute to global AI safety standards. The paper proposes and analyzes three models for AISI involvement in international standard-setting processes, advocating for a multi-track approach coordinated by AISIs."
  },
  {
    "url": "https://arxiv.org/abs/2409.02779",
    "title": "Governing dual-use technologies: Case studies of international security agreements and lessons for AI governance",
    "published_date": "2024-09-04",
    "abstract": "International AI governance agreements and institutions may play an important role in reducing global security risks from advanced AI. To inform the design of such agreements and institutions, we conducted case studies of historical and contemporary international security agreements. We focused specifically on those arrangements around dual-use technologies, examining agreements in nuclear security, chemical weapons, biosecurity, and export controls. For each agreement, we examined four key areas: (a) purpose, (b) core powers, (c) governance structure, and (d) instances of non-compliance. From these case studies, we extracted lessons for the design of international AI agreements and governance institutions. We discuss the importance of robust verification methods, strategies for balancing power between nations, mechanisms for adapting to rapid technological change, approaches to managing trade-offs between transparency and security, incentives for participation, and effective enforcement mechanisms.",
    "summary": "This paper analyzes international security agreements governing dual-use technologies (nuclear, chemical, biological) to derive lessons for AI governance, focusing on agreement purpose, power dynamics, structure, and enforcement to mitigate AI-related global security risks. Key takeaways include the need for robust verification, adaptable structures, and effective incentives for participation."
  },
  {
    "url": "https://arxiv.org/abs/2407.16903",
    "title": "US-China perspectives on extreme AI risks and global governance",
    "published_date": "2024-06-23",
    "abstract": "The United States and China will play an important role in navigating safety and security challenges relating to advanced artificial intelligence. We sought to better understand how experts in each country describe safety and security threats from advanced artificial intelligence, extreme risks from AI, and the potential for international cooperation. Specifically, we compiled publicly-available statements from major technical and policy leaders in both the United States and China. We focused our analysis on advanced forms of artificial intelligence, such as artificial general intelligence (AGI), that may have the most significant impacts on national and global security. Experts in both countries expressed concern about risks from AGI, risks from intelligence explosions, and risks from AI systems that escape human control. Both countries have also launched early efforts designed to promote international cooperation around safety standards and risk management practices. Notably, our findings only reflect information from publicly available sources. Nonetheless, our findings can inform policymakers and researchers about the state of AI discourse in the US and China. We hope such work can contribute to policy discussions around advanced AI, its global security threats, and potential international dialogues or agreements to mitigate such threats.",
    "summary": "This paper analyzes publicly available statements from US and Chinese AI experts, revealing shared concerns about extreme risks from advanced AI, particularly artificial general intelligence (AGI), including uncontrolled systems and intelligence explosions; both countries also show initial interest in international cooperation to manage these risks."
  },
  {
    "url": "https://arxiv.org/abs/2408.16074",
    "title": "Verification methods for international AI agreements",
    "published_date": "2024-08-28",
    "abstract": "What techniques can be used to verify compliance with international agreements about advanced AI development? In this paper, we examine 10 verification methods that could detect two types of potential violations: unauthorized AI training (e.g., training runs above a certain FLOP threshold) and unauthorized data centers. We divide the verification methods into three categories: (a) national technical means (methods requiring minimal or no access from suspected non-compliant nations), (b) access-dependent methods (methods that require approval from the nation suspected of unauthorized activities), and (c) hardware-dependent methods (methods that require rules around advanced hardware). For each verification method, we provide a description, historical precedents, and possible evasion techniques. We conclude by offering recommendations for future work related to the verification and enforcement of international AI governance agreements.",
    "summary": "This paper analyzes ten verification methods for ensuring compliance with international AI agreements, categorizing them by required access level (national technical means, access-dependent, hardware-dependent), and evaluating their effectiveness against potential evasion strategies. The study focuses on detecting unauthorized AI training exceeding specified thresholds and unauthorized data centers."
  },
  {
    "url": "https://arxiv.org/abs/2410.22151",
    "title": "Standardization Trends on Safety and Trustworthiness Technology for Advanced AI",
    "published_date": "2024-10-29",
    "abstract": "Artificial Intelligence (AI) has rapidly evolved over the past decade and has advanced in areas such as language comprehension, image and video recognition, programming, and scientific reasoning. Recent AI technologies based on large language models and foundation models are approaching or surpassing artificial general intelligence. These systems demonstrate superior performance in complex problem solving, natural language processing, and multi-domain tasks, and can potentially transform fields such as science, industry, healthcare, and education. However, these advancements have raised concerns regarding the safety and trustworthiness of advanced AI, including risks related to uncontrollability, ethical conflicts, long-term socioeconomic impacts, and safety assurance. Efforts are being expended to develop internationally agreed-upon standards to ensure the safety and reliability of AI. This study analyzes international trends in safety and trustworthiness standardization for advanced AI, identifies key areas for standardization, proposes future directions and strategies, and draws policy implications. The goal is to support the safe and trustworthy development of advanced AI and enhance international competitiveness through effective standardization.",
    "citation_count": 1,
    "summary": "This paper examines international standardization trends for ensuring the safety and trustworthiness of advanced AI systems, identifying key areas needing standardization and proposing future strategies to support safe AI development and global competitiveness."
  },
  {
    "url": "https://arxiv.org/abs/2410.01819",
    "title": "Strategic AI Governance: Insights from Leading Nations",
    "published_date": "2024-09-16",
    "abstract": "Artificial Intelligence (AI) has the potential to revolutionize various sectors, yet its adoption is often hindered by concerns about data privacy, security, and the understanding of AI capabilities. This paper synthesizes AI governance approaches, strategic themes, and enablers and challenges for AI adoption by reviewing national AI strategies from leading nations. The key contribution is the development of an EPIC (Education, Partnership, Infrastructure, Community) framework, which maps AI implementation requirements to fully realize social impacts and public good from successful and sustained AI deployment. Through a multi-perspective content analysis of the latest AI strategy documents, this paper provides a structured comparison of AI governance strategies across nations. The findings offer valuable insights for governments, academics, industries, and communities to enable responsible and trustworthy AI deployments. Future work should focus on incorporating specific requirements for developing countries and applying the strategies to specific AI applications, industries, and the public sector.",
    "summary": "This paper analyzes national AI strategies from leading countries to identify common themes and challenges in AI governance, proposing an EPIC framework (Education, Partnership, Infrastructure, Community) to guide responsible AI implementation and maximize societal benefit. The framework maps implementation requirements to achieve successful and sustained AI deployment."
  },
  {
    "url": "https://arxiv.org/abs/2410.18114",
    "title": "Bridging Today and the Future of Humanity: AI Safety in 2024 and Beyond",
    "published_date": "2024-10-09",
    "abstract": "The advancements in generative AI inevitably raise concerns about their risks and safety implications, which, in return, catalyzes significant progress in AI safety. However, as this field continues to evolve, a critical question arises: are our current efforts on AI safety aligned with the advancements of AI as well as the long-term goal of human civilization? This paper presents a blueprint for an advanced human society and leverages this vision to guide current AI safety efforts. It outlines a future where the Internet of Everything becomes reality, and creates a roadmap of significant technological advancements towards this envisioned future. For each stage of the advancements, this paper forecasts potential AI safety issues that humanity may face. By projecting current efforts against this blueprint, this paper examines the alignment between the current efforts and the long-term needs, and highlights unique challenges and missions that demand increasing attention from AI safety practitioners in the 2020s. This vision paper aims to offer a broader perspective on AI safety, emphasizing that our current efforts should not only address immediate concerns but also anticipate potential risks in the expanding AI landscape, thereby promoting a safe and sustainable future of AI and human civilization.",
    "summary": "This paper argues that current AI safety efforts must be re-evaluated against a long-term vision of a technologically advanced future, anticipating future risks posed by increasingly powerful AI systems to ensure a safe and sustainable human-AI coexistence. It provides a roadmap of future technological advancements and corresponding AI safety challenges to guide current research and development."
  },
  {
    "url": "https://www.lesswrong.com/posts/zKGyznvDB94aoJgx4/towards-mutually-assured-cooperation",
    "author": "mikko",
    "title": "Towards mutually assured cooperation",
    "published_date": "2024-12-22",
    "summary": "Unfettered AI development risks triggering a global nuclear arms race, as nations fear an adversary achieving total dominance through AI. International cooperation in AGI development is proposed as the safest path, maximizing benefits while minimizing the catastrophic risk of nuclear war."
  },
  {
    "url": "https://www.alignmentforum.org/posts/6nNwMbdRXZDuNd4Gx/analysis-of-global-ai-governance-strategies",
    "author": "Sammy Martin, Justin Bullock, Corin Katzke",
    "title": "Analysis of Global AI Governance Strategies",
    "published_date": "2024-12-04",
    "summary": "The article analyzes three AI governance strategies—Cooperative Development, Strategic Advantage, and Global Moratorium—evaluating their effectiveness based on the difficulty of aligning AI and the timeframe for its development. The optimal strategy shifts depending on these factors, with Cooperative Development favored for longer timelines and easier alignment, Strategic Advantage for shorter timelines and moderate alignment difficulty, and Global Moratorium reserved for scenarios with extremely short timelines or difficult alignment."
  }
]