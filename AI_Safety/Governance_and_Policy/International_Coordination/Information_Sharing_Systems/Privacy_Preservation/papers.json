[
  {
    "url": "https://www.lesswrong.com/posts/SacB4mg6nJWATKB3Y/a-proposal-for-a-cryogenic-grave-for-cryonics",
    "author": "Roko",
    "title": "A proposal for a cryogenic grave for cryonics",
    "published_date": "2024-02-01",
    "summary": "This article proposes a solution to the risk of cryonics organizations failing: individually insulated cryogenic graves, large enough to maintain cryogenic temperatures for centuries without human intervention. The author explores the engineering feasibility and cost of such a system, suggesting smaller, distributed units are preferable to a single large facility."
  },
  {
    "url": "https://www.lesswrong.com/posts/XCuwfWFuiGxCWXFtW/homomorphic-encryption-and-bitcoin?commentId=bjcL75iWLnAAifS6k",
    "author": "Jimrandomh",
    "title": "Homomorphic encryption and Bitcoin - LessWrong",
    "published_date": "2023-02-07",
    "summary": "The author proposes a Bitcoin security improvement using homomorphic encryption to split private keys across two devices, requiring both for transaction signing, thus mitigating the risk of single-point compromise. This multi-device system would enhance security without altering the Bitcoin protocol."
  },
  {
    "url": "https://arxiv.org/pdf/2103.03432v1.pdf",
    "title": "Network Consensus with Privacy: A Secret Sharing Method",
    "published_date": "2021-03-05",
    "abstract": "In this work, inspired by secret sharing schemes, we introduce a privacy-preserving approach for network consensus, by which all nodes in a network can reach an agreement on their states without exposing the individual state to neighbors. With the privacy degree defined for the agents, the proposed method makes the network resistant to the collusion of any given number of neighbors, and protects the consensus procedure from communication eavesdropping. Unlike existing works, the proposed privacy-preserving algorithm is resilient to node failures. When a node fails, the method offers the possibility of rebuilding the lost node via the information kept in its neighbors, even though none of the neighbors knows the exact state of the failing node. Moreover, it is shown that the proposed method can achieve consensus and average consensus almost surely, when the agents have arbitrary privacy degrees and a common privacy degree, respectively. To illustrate the theory, two numerical examples are presented.",
    "citation_count": 2,
    "summary": "This paper presents a novel privacy-preserving network consensus algorithm based on secret sharing, enabling nodes to agree on a state without revealing individual states to neighbors, even against colluding nodes or eavesdropping. The algorithm is resilient to node failures, allowing for reconstruction of failed nodes using information from neighbors, and guarantees almost sure consensus under various conditions."
  },
  {
    "title": "PRICURE: Privacy-Preserving Collaborative Inference in a Multi-Party Setting",
    "abstract": "When multiple parties that deal with private data aim for a collaborative prediction task such as medical image classification, they are often constrained by data protection regulations and lack of trust among collaborating parties. If done in a privacy-preserving manner, predictive analytics can benefit from the collective prediction capability of multiple parties holding complementary datasets on the same machine learning task. This paper presents PRICURE, a system that combines complementary strengths of secure multi-party computation (SMPC) and differential privacy (DP) to enable privacy-preserving collaborative prediction among multiple model owners. SMPC enables secret-sharing of private models and client inputs with non-colluding secure servers to compute predictions without leaking model parameters and inputs. DP masks true prediction results via noisy aggregation so as to deter a semi-honest client who may mount membership inference attacks. We evaluate PRICURE on neural networks across four datasets including benchmark medical image classification datasets. Our results suggest PRICURE guarantees privacy for tens of model owners and clients with acceptable accuracy loss. We also show that DP reduces membership inference attack exposure without hurting accuracy.",
    "published_date": "2021-02-19",
    "citation_count": 18,
    "url": "https://dl.acm.org/doi/10.1145/3445970.3451156",
    "summary": "PRICURE is a system for privacy-preserving collaborative prediction using secure multi-party computation (SMPC) to protect model parameters and inputs, and differential privacy (DP) to further mask predictions and prevent membership inference attacks. Evaluation on neural networks demonstrates its effectiveness for numerous model owners and clients with reasonable accuracy."
  },
  {
    "url": "https://arxiv.org/abs/2101.04766",
    "title": "Privacy-Preserving Randomized Controlled Trials: A Protocol for Industry Scale Deployment",
    "published_date": "2021-01-12",
    "abstract": "Randomized Controlled Trials, when feasible, give the strongest and most trustworthy empirical measures of causal effects. They are the gold standard in many clinical, social, and behavioral fields of study. However, the most important settings often involve the most sensitive data, therefore cause privacy concerns. In this paper, we outline a way to deploy an end-to-end privacy-preserving protocol for learning causal effects from Randomized Controlled Trials (RCTs). We are particularly focused on the difficult and important case where one party determines which treatment an individual receives, and another party measures outcomes on individuals, and these parties do not want to leak any of their information to each other, but still want to collectively learn a true causal effect in the world. Moreover, we show how such a protocol can be scaled to 500 million rows of data and more than a billion gates. We also offer an open source deployment of this protocol. We accomplish this by a three-stage solution, interconnecting and blending three privacy technologies--private set intersection, multiparty computation, and differential privacy--to address core points of privacy leakage, at the join, at the point of computation, and at the release, respectively. The first stage uses the Private-ID protocol[8] to create a private encrypted join of the users. The second stage utilizes the encrypted join to run multiple instances of a general purpose MPC over a sharded database to aggregate statistics about each experimental group while discarding individuals who took an action before they received treatment. The third stage adds distributed and calibrated Differential Privacy (DP) noise within the final MPC computations to the released aggregate statistical estimates of causal effects and their uncertainty measures, providing formal two-sided privacy guarantees. We also evaluate the performance of multiple open source general purpose MPC libraries for this task. We additionally demonstrate how we have used this to create a working ads effectiveness measurement product capable of measuring hundreds of millions of individuals per experiment.",
    "citation_count": 12,
    "summary": "This paper presents a privacy-preserving protocol for conducting large-scale randomized controlled trials, using a three-stage approach combining private set intersection, multi-party computation, and differential privacy to protect sensitive data while accurately estimating causal effects. The protocol is scalable to hundreds of millions of individuals and has been deployed in a real-world advertising effectiveness measurement product."
  },
  {
    "url": "https://arxiv.org/pdf/2106.14643v1.pdf",
    "title": "Privacy-Utility Tradeoffs Against Limited Adversaries",
    "published_date": "2021-06-18",
    "abstract": "In this article, we study privacy-utility tradeoffs where users share privacy-correlated useful information with a service provider to obtain some utility. The service provider is adversarial in the sense that it can infer the users' private information based on the shared useful information. To minimize the privacy leakage while maintaining a desired level of utility, the users carefully perturb the useful information via a probabilistic privacy mapping before sharing it. We focus on the setting in which the adversary attempting an inference attack on the users' privacy has biased information about the statistical correlation between the private and useful variables. This information asymmetry between the users and the limited adversary is shown to lead to better privacy guarantees. We first identify assumptions on the adversary's information so that the inference costs are well-defined. Then, we characterize the impact of the information asymmetry and show that it increases the inference costs for the adversary. We further formulate the design of the privacy mapping against a limited adversary as a difference of convex functions program and solve it via the concaveâ€“convex procedure. When the adversary's information is not precisely available, we adopt a Bayesian view and represent the adversary's information by a probability distribution. In this case, the expected cost for the adversary does not admit a closed-form expression, and we establish and maximize a lower bound of it. We provide a numerical example to illustrate the theoretical results.",
    "citation_count": 1,
    "summary": "This paper analyzes privacy-utility tradeoffs when users share information with a service provider, focusing on scenarios where the adversary's knowledge of the data correlation is limited. The authors model this asymmetric information setting, demonstrating improved privacy guarantees and formulating the optimal privacy-preserving data perturbation as a solvable optimization problem."
  },
  {
    "title": "A Data Sharing Scheme Based on Blockchain System and Attribute-Based Encryption",
    "abstract": "Blockchain systems offer a decentralized, immutable and transparent architecture that can give the ownership and control of data back to users, empower trusted and accountable data sharing. However, there still exist different scalability, security and potential privacy issues in blockchain networks, such as on-chain data privacy, data origin authentication, or compliance with privacy regulations (e.g. GDPR). To address these challenges, we propose a privacy-preserving and user-controlled architecture for data sharing based on blockchain system and Ciphertext-Policy Attribute-Based Encryption (CP-ABE), called ThemisABE. The proposed scheme has properties including one-to-many data encrypting and fined-grained access control. In the ThemisABE, any party can be an authority and the algorithm of CP-ABE generates the secret key corresponding to attributes for data user through any chosen set of nodes in blockchain system. Moreover, in order to realize accountability of CP-ABE algorithm and avoid performance bottleneck incurred by smart contracts, a kind of data structure of transactions is designed to record in the ledger, following an approaches to off-chain computations. The results of security analysis and experimental simulation show that the proposed scheme can solves the problems of privacy security and localization of data sharing. At the same time, the data sharing scheme has better performance and could be applied in some scenarios, such as eHealth, eGovernment or cross-border paperless trade.",
    "published_date": "2021-03-26",
    "citation_count": 5,
    "url": "https://dl.acm.org/doi/10.1145/3460537.3460559",
    "summary": "ThemisABE, a novel data sharing architecture, combines blockchain's decentralized nature with Ciphertext-Policy Attribute-Based Encryption (CP-ABE) to enable fine-grained access control and privacy-preserving data sharing, addressing scalability and security limitations of blockchain-based data sharing solutions. Off-chain computations and a specialized transaction data structure improve performance and accountability."
  },
  {
    "title": "POSTER: SecretSVM -- Secret Sharing-Based SVM for Preventing Collusion in IoT Data Analysis",
    "abstract": "Support vector machine (SVM) is widely used because of its efficiency in data processing. Single source data may not train a nice SVM, since a single entity isn't having enough data with adequate attributes. Thus, multiple source data need to share data to combine a dataset with different attributes, and then jointly train a classifier. However, outsourcing data to a cloud for training induces two security concerns, data privacy and collusion with the cloud and providers. In this paper, we consider a distributed scenario without any centralized party in which IoT data providers will jointly serve as the leader to obtain some parameters. We propose a privacy-preserving and collusion-free SVM (so-called SecretSVM)built from secret sharing and distributed consensus. Participants train intermediate values to provide the necessary interaction andprevent against collusion attacks.",
    "published_date": "2020-10-05",
    "citation_count": 1,
    "url": "https://dl.acm.org/doi/10.1145/3320269.3405439",
    "summary": "SecretSVM is a privacy-preserving support vector machine (SVM) that uses secret sharing and distributed consensus to enable multiple IoT data providers to collaboratively train a model without revealing individual data or trusting a central party. This approach mitigates data privacy risks and prevents collusion attacks."
  }
]