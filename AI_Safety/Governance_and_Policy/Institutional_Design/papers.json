[
  {
    "url": "https://arxiv.org/abs/2307.04699",
    "title": "International Institutions for Advanced AI",
    "published_date": "2023-07-10",
    "abstract": "International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.",
    "citation_count": 12,
    "summary": "This paper proposes four international institutional models—a commission, governance organization, collaborative, and safety project—to manage the risks and benefits of advanced AI, addressing global externalities and promoting responsible development. These models focus on facilitating expert consensus, setting international standards, promoting access, and advancing AI safety research."
  },
  {
    "url": "https://arxiv.org/pdf/2001.03573v1.pdf",
    "title": "Should Artificial Intelligence Governance be Centralised?: Design Lessons from History",
    "published_date": "2020-01-10",
    "abstract": "Can effective international governance for artificial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difficulty in securing participation while creating stringent rules. Other considerations depend on the specific design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate.",
    "citation_count": 36,
    "summary": "This paper examines whether centralized or fragmented governance is more effective for international AI regulation, drawing historical parallels to highlight the trade-offs between efficiency and adaptability. It concludes that while a well-designed centralized body could be beneficial, current fragmentation should be closely monitored for signs of self-organization or inadequacy."
  },
  {
    "url": "https://arxiv.org/abs/2409.10536",
    "title": "The potential functions of an international institution for AI safety. Insights from adjacent policy areas and recent trends",
    "published_date": "2024-08-31",
    "abstract": "Governments, industry, and other actors involved in governing AI technologies around the world agree that, while AI offers tremendous promise to benefit the world, appropriate guardrails are required to mitigate risks. Global institutions, including the OECD, the G7, the G20, UNESCO, and the Council of Europe, have already started developing frameworks for ethical and responsible AI governance. While these are important initial steps, they alone fall short of addressing the need for institutionalised international processes to identify and assess potentially harmful AI capabilities. Contributing to the relevant conversation on how to address this gap, this chapter reflects on what functions an international AI safety institute could perform. Based on the analysis of both existing international governance models addressing safety considerations in adjacent policy areas and the newly established national AI safety institutes in the UK and US, the chapter identifies a list of concrete functions that could be performed at the international level. While creating a new international body is not the only way forward, understanding the structure of these bodies from a modular perspective can help us to identify the tools at our disposal. These, we suggest, can be categorised under three functional domains: a) technical research and cooperation, b) safeguards and evaluations, c) policymaking and governance support.",
    "summary": "This chapter explores the potential functions of a new international institution dedicated to AI safety, drawing insights from existing international governance models and recent national initiatives. It proposes three key functional domains for such an institution: technical research and cooperation, safeguards and evaluations, and policymaking and governance support."
  },
  {
    "url": "https://arxiv.org/abs/2412.17114",
    "title": "Decentralized Governance of Autonomous AI Agents",
    "published_date": "2024-12-22",
    "abstract": "Autonomous AI agents present transformative opportunities and significant governance challenges. Existing frameworks, such as the EU AI Act and the NIST AI Risk Management Framework, fall short of addressing the complexities of these agents, which are capable of independent decision-making, learning, and adaptation. To bridge these gaps, we propose the ETHOS (Ethical Technology and Holistic Oversight System) framework, a decentralized governance (DeGov) model leveraging Web3 technologies, including blockchain, smart contracts, and decentralized autonomous organizations (DAOs). ETHOS establishes a global registry for AI agents, enabling dynamic risk classification, proportional oversight, and automated compliance monitoring through tools like soulbound tokens and zero-knowledge proofs. Furthermore, the framework incorporates decentralized justice systems for transparent dispute resolution and introduces AI specific legal entities to manage limited liability, supported by mandatory insurance to ensure financial accountability and incentivize ethical design. By integrating philosophical principles of rationality, ethical grounding, and goal alignment, ETHOS aims to create a robust research agenda for promoting trust, transparency, and participatory governance. This innovative framework offers a scalable and inclusive strategy for regulating AI agents, balancing innovation with ethical responsibility to meet the demands of an AI-driven future.",
    "summary": "The paper proposes ETHOS, a decentralized governance framework using Web3 technologies to manage autonomous AI agents, addressing limitations of existing regulatory frameworks by leveraging blockchain, DAOs, and novel mechanisms like soulbound tokens for risk classification and compliance. This framework aims to foster ethical AI development through transparent dispute resolution, accountability, and incentivized responsible design."
  },
  {
    "url": "https://arxiv.org/abs/2410.09219",
    "title": "Understanding the First Wave of AI Safety Institutes: Characteristics, Functions, and Challenges",
    "published_date": "2024-10-11",
    "abstract": "In November 2023, the UK and US announced the creation of their AI Safety Institutes (AISIs). Five other jurisdictions have followed in establishing AISIs or similar institutions, with more likely to follow. While there is considerable variation between these institutions, there are also key similarities worth identifying. This primer describes one cluster of similar AISIs, the\"first wave,\"consisting of the Japan, UK, and US AISIs. First-wave AISIs have several fundamental characteristics in common: they are technical government institutions, have a clear mandate related to the safety of advanced AI systems, and lack regulatory powers. Safety evaluations are at the center of first-wave AISIs. These techniques test AI systems across tasks to understand their behavior and capabilities on relevant risks, such as cyber, chemical, and biological misuse. They also share three core functions: research, standards, and cooperation. These functions are critical to AISIs' work on safety evaluations but also support other activities such as scientific consensus-building and foundational AI safety research. Despite its growing popularity as an institutional model, the AISI model is not free from challenges and limitations. Some analysts have criticized the first wave of AISIs for specializing too much in a sub-area and for being potentially redundant with existing institutions, for example. Future developments may rapidly change this landscape, and particularities of individual AISIs may not be captured by our broad-strokes description. This policy brief aims to outline the core elements of first-wave AISIs as a way of encouraging and improving conversations on this novel institutional model, acknowledging this is just a simplified snapshot rather than a timeless prescription.",
    "summary": "The first wave of AI Safety Institutes (AISIs) in Japan, the UK, and the US are government-funded, technically-focused institutions prioritizing AI safety evaluations through research, standards development, and international cooperation, but face challenges regarding specialization and potential redundancy with existing organizations."
  },
  {
    "url": "https://arxiv.org/abs/2410.02769",
    "title": "Fundamentals of legislation for autonomous artificial intelligence systems",
    "published_date": "2024-09-14",
    "abstract": "The paper proposes a method for defining a dedicated operational context as part of the development and deployment of autonomous corporate governance systems. The case study of autonomous board of directors systems is examined. A significant part of the operational context for the autonomous corporate governance systems consists of the regulatory and legal framework that regulates the company's operations. A special operational context for autonomous artificial intelligence systems can be defined by simultaneously formulating local regulatory documents in two versions, i.e., to be used by people and by autonomous systems. In such a case, the artificial intelligence system receives a clearly defined operational context that allows such a system to perform its functions with a required operational quality. Local regulations that take into account the specificity of operations involving individuals and autonomous artificial intelligence systems can become the foundation of the relevant legislation that would regulate the development and deployment of autonomous systems.",
    "summary": "This paper suggests creating dual-version local regulations—one for humans and one for AI—to define a clear operational context for autonomous corporate governance systems, arguing this approach forms the basis for effective legislation governing AI deployment."
  },
  {
    "url": "http://arxiv.org/abs/2401.13138",
    "title": "Visibility into AI Agents",
    "published_date": "2024-01-23",
    "abstract": "Increased delegation of commercial, scientific, governmental, and personal activities to AI agents—systems capable of pursuing complex goals with limited supervision—may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as visibility, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: agent identifiers, real-time monitoring, and activity logging. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for various actors in the supply chain including hardware and software service providers. Finally, we discuss the implications of our measures for privacy and concentration of power. Further work into understanding the measures and mitigating their negative impacts can help to build a foundation for the governance of AI agents.",
    "citation_count": 12,
    "summary": "This paper examines methods to increase transparency (\"visibility\") in the use of AI agents, focusing on agent identifiers, real-time monitoring, and activity logging, while considering implementation challenges, privacy implications, and power dynamics across diverse deployment contexts. The authors propose these measures as crucial for mitigating societal risks associated with increased AI agent autonomy."
  },
  {
    "url": "https://arxiv.org/abs/2411.15147",
    "title": "Delegating Responsibilities to Intelligent Autonomous Systems: Challenges and Benefits",
    "published_date": "2024-11-06",
    "abstract": "As AI systems increasingly operate with autonomy and adaptability, the traditional boundaries of moral responsibility in techno-social systems are being challenged. This paper explores the evolving discourse on the delegation of responsibilities to intelligent autonomous agents and the ethical implications of such practices. Synthesizing recent developments in AI ethics, including concepts of distributed responsibility and ethical AI by design, the paper proposes a functionalist perspective as a framework. This perspective views moral responsibility not as an individual trait but as a role within a socio-technical system, distributed among human and artificial agents. As an example of 'AI ethical by design,' we present Basti and Vitiello's implementation. They suggest that AI can act as artificial moral agents by learning ethical guidelines and using Deontic Higher-Order Logic to assess decisions ethically. Motivated by the possible speed and scale beyond human supervision and ethical implications, the paper argues for 'AI ethical by design', while acknowledging the distributed, shared, and dynamic nature of responsibility. This functionalist approach offers a practical framework for navigating the complexities of AI ethics in a rapidly evolving technological landscape.",
    "summary": "This paper examines the ethical challenges of delegating responsibilities to increasingly autonomous AI systems, proposing a functionalist framework that distributes moral responsibility across human and artificial agents within a socio-technical system. It advocates for \"AI ethical by design\" approaches, such as using Deontic Higher-Order Logic, to ensure ethical decision-making in AI."
  },
  {
    "url": "https://arxiv.org/pdf/2305.11528.pdf",
    "title": "The Global Governance of Artificial Intelligence: Next Steps for Empirical and Normative Research",
    "published_date": "2023-05-19",
    "abstract": "Artificial intelligence (AI) represents a technological upheaval with the potential to change human society. Because of its transformative potential, AI is increasingly becoming subject to regulatory initiatives at the global level. Yet, so far, scholarship in political science and international relations has focused more on AI applications than on the emerging architecture of global AI regulation. The purpose of this article is to outline an agenda for research into the global governance of AI. The article distinguishes between two broad perspectives: an empirical approach, aimed at mapping and explaining global AI governance; and a normative approach, aimed at developing and applying standards for appropriate global AI governance. The two approaches offer questions, concepts, and theories that are helpful in gaining an understanding of the emerging global governance of AI. Conversely, exploring AI as a regulatory issue offers a critical opportunity to refine existing general approaches to the study of global governance.",
    "citation_count": 25,
    "summary": "This paper proposes a research agenda for understanding the global governance of artificial intelligence, advocating for both empirical mapping of existing regulatory initiatives and normative development of appropriate global standards. It suggests that studying AI governance can refine existing theories of global governance, and vice versa."
  },
  {
    "url": "https://www.lesswrong.com/posts/GCMMPTCmGagcP2Bhd/ideas-for-ai-labs-reading-list",
    "author": "Zach Stein-Perlman",
    "title": "Ideas for AI labs: Reading list",
    "published_date": "2023-04-24",
    "summary": "This document surveys expert opinions and research on AI safety and governance, focusing on best practices for AI labs to mitigate existential risks. It compiles numerous suggestions, including improved model evaluation, transparency measures, and responsible publication practices to minimize the diffusion of potentially dangerous research."
  },
  {
    "url": "https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research",
    "author": "Roman Leventov",
    "title": "A multi-disciplinary view on AI safety research",
    "published_date": "2023-02-08",
    "summary": "The article advocates for a multidisciplinary approach to AI safety research, emphasizing the need to integrate theoretical and empirical research across numerous fields (including cognitive science, social sciences, and engineering) to design \"safe\" artificial general intelligence (AGI) systems within a framework of \"civilizational intelligence.\" This approach prioritizes pragmatism and collaboration over isolated theoretical development."
  },
  {
    "url": "https://www.lesswrong.com/posts/pHJtLHcWvfGbsW7LR/roadmap-for-a-collaborative-prototype-of-an-open-agency",
    "author": "Deger Turan",
    "title": "Roadmap for a collaborative prototype of an Open Agency Architecture",
    "published_date": "2023-05-10",
    "summary": "The AI Objectives Institute proposes the Open Agency Architecture (OAA), a framework for AI alignment that prioritizes human values through a three-stage process: world modeling, planning, and action. OAA aims to address AI alignment challenges by integrating human preferences with AI capabilities within a transparent and iterative development process."
  },
  {
    "url": "https://www.lesswrong.com/posts/2eaLH7zp6pxdQwYSH",
    "author": "Austin Witte",
    "title": "A Brief Overview of AI Safety/Alignment Orgs, Fields, Researchers, and Resources for ML Researchers",
    "published_date": "2023-02-02",
    "summary": "Two overview documents, a short and a long version, categorize AI safety research by keywords and fields to help machine learning researchers quickly assess alignment research areas aligning with their expertise. These resources list organizations, researchers, papers, and guides to facilitate entry into the field."
  },
  {
    "url": "https://www.alignmentforum.org/tag/organization-updates",
    "author": "Jesse Hoogland, Daniel Murfet, Stan van Wingerden, Alexander Gietelink Oldenziel",
    "title": "Organization Updates - AI Alignment Forum",
    "published_date": "2023-05-30",
    "summary": "Organization updates pertain to information changes or announcements specific to a particular group or organization. These updates may cover various aspects relevant to the group's members or activities."
  },
  {
    "url": "https://www.lesswrong.com/posts/AKBkDNeFLZxaMqjQG/a-practical-incremental-pathway-to-safe-tai-oaa-in-the-real",
    "author": "Roman Leventov, Rafael Kaufmann Nedal",
    "title": "Gaia Network: a practical, incremental pathway to Open Agency Architecture",
    "published_date": "2023-12-20",
    "summary": "The article proposes Gaia, a decentralized network leveraging existing technologies and economic mechanisms to create a detailed world simulation for aligning advanced AI with human values, addressing AI safety concerns within a broader framework of improved global governance and decision-making. This approach prioritizes a bottom-up, evolutionary design over top-down control, promoting collaboration and open development."
  }
]