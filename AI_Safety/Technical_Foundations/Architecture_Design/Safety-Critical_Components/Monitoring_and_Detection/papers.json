[
  {
    "url": "https://arxiv.org/abs/2405.13254",
    "title": "System Safety Monitoring of Learned Components Using Temporal Metric Forecasting",
    "published_date": "2024-05-21",
    "abstract": "In learning-enabled autonomous systems, safety monitoring of learned components is crucial to ensure their outputs do not lead to system safety violations, given the operational context of the system. However, developing a safety monitor for practical deployment in real-world applications is challenging. This is due to limited access to internal workings and training data of the learned component. Furthermore, safety monitors should predict safety violations with low latency, while consuming a reasonable computation resource amount.\n \n To address the challenges, we propose a safety monitoring method based on probabilistic time series forecasting. Given the learned component outputs and an operational context, we empirically investigate different Deep Learning (DL)-based probabilistic forecasting to predict the objective measure capturing the satisfaction or violation of a safety requirement (\n safety metric\n ). We empirically evaluate safety metric and violation prediction accuracy, and inference latency and resource usage of four state-of-the-art models, with varying horizons, using autonomous aviation and autonomous driving case studies. Our results suggest that probabilistic forecasting of safety metrics, given learned component outputs and scenarios, is effective for safety monitoring. Furthermore, for both case studies, the Temporal Fusion Transformer (TFT) was the most accurate model for predicting imminent safety violations, with acceptable latency and resource consumption.\n",
    "summary": "This paper proposes a safety monitoring method for learned components in autonomous systems using probabilistic time series forecasting, specifically evaluating the accuracy and efficiency of four deep learning models in predicting safety metric violations in aviation and driving scenarios. The Temporal Fusion Transformer (TFT) model demonstrated superior performance in predicting imminent safety violations with acceptable latency and resource use."
  },
  {
    "url": "https://arxiv.org/abs/2406.16220",
    "title": "Learning Run-time Safety Monitors for Machine Learning Components",
    "published_date": "2024-06-23",
    "abstract": "For machine learning components used as part of autonomous systems (AS) in carrying out critical tasks it is crucial that assurance of the models can be maintained in the face of post-deployment changes (such as changes in the operating environment of the system). A critical part of this is to be able to monitor when the performance of the model at runtime (as a result of changes) poses a safety risk to the system. This is a particularly difficult challenge when ground truth is unavailable at runtime. In this paper we introduce a process for creating safety monitors for ML components through the use of degraded datasets and machine learning. The safety monitor that is created is deployed to the AS in parallel to the ML component to provide a prediction of the safety risk associated with the model output. We demonstrate the viability of our approach through some initial experiments using publicly available speed sign datasets.",
    "summary": "This paper proposes a method for creating runtime safety monitors for machine learning components in autonomous systems, using degraded datasets and machine learning to predict safety risks even when ground truth is unavailable. These monitors run parallel to the ML component, providing a real-time assessment of its operational safety."
  },
  {
    "url": "https://arxiv.org/abs/2409.17630",
    "title": "System-Level Safety Monitoring and Recovery for Perception Failures in Autonomous Vehicles",
    "published_date": "2024-09-26",
    "abstract": "The safety-critical nature of autonomous vehicle (AV) operation necessitates development of task-relevant algorithms that can reason about safety at the system level and not just at the component level. To reason about the impact of a perception failure on the entire system performance, such task-relevant algorithms must contend with various challenges: complexity of AV stacks, high uncertainty in the operating environments, and the need for real-time performance. To overcome these challenges, in this work, we introduce a Q-network called SPARQ (abbreviation for Safety evaluation for Perception And Recovery Q-network) that evaluates the safety of a plan generated by a planning algorithm, accounting for perception failures that the planning process may have overlooked. This Q-network can be queried during system runtime to assess whether a proposed plan is safe for execution or poses potential safety risks. If a violation is detected, the network can then recommend a corrective plan while accounting for the perceptual failure. We validate our algorithm using the NuPlan-Vegas dataset, demonstrating its ability to handle cases where a perception failure compromises a proposed plan while the corrective plan remains safe. We observe an overall accuracy and recall of 90% while sustaining a frequency of 42Hz on the unseen testing dataset. We compare our performance to a popular reachability-based baseline and analyze some interesting properties of our approach in improving the safety properties of an AV pipeline.",
    "summary": "SPARQ, a Q-network-based system, monitors autonomous vehicle plans for safety, considering potential perception failures overlooked by the planning algorithm. Upon detecting unsafe plans, SPARQ recommends safe corrective plans, achieving 90% accuracy and recall at 42Hz on unseen data."
  },
  {
    "url": "https://arxiv.org/abs/2309.13475",
    "title": "Detecting and Mitigating System-Level Anomalies of Vision-Based Controllers",
    "published_date": "2023-09-23",
    "abstract": "Autonomous systems, such as self-driving cars and drones, have made significant strides in recent years by leveraging visual inputs and machine learning for decision-making and control. Despite their impressive performance, these vision-based controllers can make erroneous predictions when faced with novel or out-of-distribution inputs. Such errors can cascade to catastrophic system failures and compromise system safety. In this work, we introduce a run-time anomaly monitor to detect and mitigate such closed-loop, system-level failures. Specifically, we leverage a reachability-based framework to stress-test the vision-based controller offline and mine its system-level failures. This data is then used to train a classifier that is leveraged online to flag inputs that might cause system breakdowns. The anomaly detector highlights issues that transcend individual modules and pertain to the safety of the overall system. We also design a fallback controller that robustly handles these detected anomalies to preserve system safety. We validate the proposed approach on an autonomous aircraft taxiing system that uses a vision-based controller for taxiing. Our results show the efficacy of the proposed approach in identifying and handling system-level anomalies, outperforming methods such as prediction error-based detection, and ensembling, thereby enhancing the overall safety and robustness of autonomous systems. Website: phoenixrider12.github.io/FailureMitigation",
    "citation_count": 2,
    "summary": "This paper presents a runtime anomaly monitor for vision-based autonomous systems that detects and mitigates system-level failures by using a reachability-based framework to train a classifier that flags potentially catastrophic inputs, and a fallback controller to maintain safety. The approach outperforms existing anomaly detection methods, enhancing system robustness."
  },
  {
    "url": "https://arxiv.org/pdf/2309.08603.pdf",
    "title": "Closing the Loop on Runtime Monitors with Fallback-Safe MPC",
    "published_date": "2023-09-15",
    "abstract": "When we rely on deep-learned models for robotic perception, we must recognize that these models may behave unreliably on inputs dissimilar from the training data, compromising the closed-loop system's safety. This raises fundamental questions on how we can assess confidence in perception systems and to what extent we can take safety-preserving actions when external environmental changes degrade our perception model's performance. Therefore, we present a framework to certify the safety of a perception-enabled system deployed in novel contexts. To do so, we leverage robust model predictive control (MPC) to control the system using the perception estimates while maintaining the feasibility of a safety-preserving fallback plan that does not rely on the perception system. In addition, we calibrate a runtime monitor using recently proposed conformal prediction techniques to certifiably detect when the perception system degrades beyond the tolerance of the MPC controller, resulting in an end-to-end safety assurance. We show that this control framework and calibration technique allows us to certify the system's safety with orders of magnitudes fewer samples than required to retrain the perception network when we deploy in a novel context on a photo-realistic aircraft taxiing simulator. Furthermore, we illustrate the safety-preserving behavior of the MPC on simulated examples of a quadrotor. We open-source our simulation platform and provide videos of our results at our project page: https://tinyurl.com/fallback-safe-mpc.",
    "citation_count": 8,
    "summary": "This paper introduces a safety-certified control framework for perception-enabled robotic systems using robust model predictive control (MPC) and conformal prediction; the system incorporates a fallback plan and a runtime monitor to ensure safety even when the perception model's performance degrades in novel environments."
  },
  {
    "url": "https://www.lesswrong.com/posts/g5XLHKyApAFXi3fso/president-biden-issues-executive-order-on-safe-secure-and",
    "author": "Tristan Williams",
    "title": "President Biden Issues Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence",
    "published_date": "2023-10-30",
    "summary": "The U.S. government issued a sweeping executive order addressing AI risks, mandating safety testing transparency for high-risk AI systems and establishing standards to mitigate threats to national security, public health, and safety, including the development of AI safety and security boards and international collaborations."
  },
  {
    "url": "https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained",
    "author": "Jay Bailey",
    "title": "Deep Q-Networks Explained",
    "published_date": "2022-09-13",
    "summary": "This article explains Deep Q-Networks (DQN), a deep reinforcement learning algorithm, at varying levels of detail. It provides a high-level overview, a technical explanation with equations, and practical replication advice, allowing readers to choose the appropriate level of complexity based on their existing knowledge."
  },
  {
    "url": "https://www.lesswrong.com/posts/d9MkMeLWvoDEsqpQP/a-compilation-of-misuses-of-statistics",
    "author": "Younes Kamel",
    "title": "A compilation of misuses of statistics",
    "published_date": "2022-02-14",
    "summary": "The article highlights common statistical errors, primarily the false assumption of Gaussian distributions in many fields, leading to inaccurate predictions, and misinterpretations of p-values, base rates, and statistical power, often resulting in flawed studies and unreliable conclusions."
  }
]