[
  {
    "url": "https://www.alignmentforum.org/posts/LkECxpbjvSifPfjnb/towards-guaranteed-safe-ai-a-framework-for-ensuring-robust-1",
    "author": "Joar Skalse",
    "title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems",
    "published_date": "2024-05-17"
  },
  {
    "url": "https://www.lesswrong.com/posts/Z9P2m462wQ4qmH6uo/aspiration-based-q-learning",
    "author": "Cl√©ment Dumas, Jobst Heitzig",
    "title": "Aspiration-based Q-Learning",
    "published_date": "2023-10-27"
  },
  {
    "url": "https://www.alignmentforum.org/s/57bsaXbJXbzKqNkrf",
    "author": "Mark Xu",
    "title": "Intermittent Distllations - AI Alignment Forum",
    "published_date": "2021-04-14"
  },
  {
    "url": "https://www.alignmentforum.org/posts/r3NHPD3dLFNk9QE2Y/search-versus-design-1",
    "author": "Alex Flint",
    "title": "Search versus design",
    "published_date": "2020-08-16"
  },
  {
    "url": "https://www.lesswrong.com/posts/TQwXPHfyyQwr22NMh/box-inversion-hypothesis",
    "author": "Jan Kulveit",
    "title": "Box inversion hypothesis",
    "published_date": "2020-10-20"
  },
  {
    "url": "https://www.alignmentforum.org/tag/oracle-ai",
    "author": "Abram Demski",
    "title": "Oracle AI - AI Alignment Forum",
    "published_date": "2019-10-29"
  }
]