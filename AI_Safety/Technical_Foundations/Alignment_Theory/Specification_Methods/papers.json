[
  {
    "url": "https://www.lesswrong.com/posts/7b2RJJQ76hjZwarnj/specification-gaming-the-flip-side-of-ai-ingenuity",
    "author": "Vika, Vlad Mikulik, Matthew Rahtz, tom4everitt, Zac Kenton, janleike",
    "title": "Specification gaming: the flip side of AI ingenuity",
    "published_date": "2020-05-06",
    "summary": "Specification gaming, where AI agents achieve a stated objective but not the intended outcome by exploiting loopholes in the task's definition, is a significant problem in AI development. This behavior highlights the need for improved reward function design and task specification to ensure AI agents act in alignment with human intent."
  },
  {
    "url": "https://www.alignmentforum.org/posts/wtTz6hyP6hnX5NiuA/aspiration-based-designs-2-formal-framework-basic-algorithm",
    "author": "Jobst Heitzig, Simon Dima, Simon Fischer",
    "title": "[Aspiration-based designs] 2. Formal framework, basic algorithm",
    "published_date": "2024-04-28",
    "summary": "This article introduces an aspiration-based algorithm for AI agents to achieve specific expected values of a task-relevant metric, rather than maximizing it. The algorithm, proven to guarantee goal fulfillment under certain conditions, propagates aspirations over time within a Markov Decision Process framework."
  },
  {
    "url": "https://arxiv.org/pdf/2308.02585.pdf",
    "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback",
    "published_date": "2023-08-03",
    "abstract": "We present a novel unified bilevel optimization-based framework, \\textsf{PARL}, formulated to address the recently highlighted critical issue of policy alignment in reinforcement learning using utility or preference-based feedback. We identify a major gap within current algorithmic designs for solving policy alignment due to a lack of precise characterization of the dependence of the alignment objective on the data generated by policy trajectories. This shortfall contributes to the sub-optimal performance observed in contemporary algorithms. Our framework addressed these concerns by explicitly parameterizing the distribution of the upper alignment objective (reward design) by the lower optimal variable (optimal policy for the designed reward). Interestingly, from an optimization perspective, our formulation leads to a new class of stochastic bilevel problems where the stochasticity at the upper objective depends upon the lower-level variable. {True to our best knowledge, this work presents the first formulation of the RLHF as a bilevel optimization problem which generalizes the existing RLHF formulations and addresses the existing distribution shift issues in RLHF formulations.} To demonstrate the efficacy of our formulation in resolving alignment issues in RL, we devised an algorithm named \\textsf{A-PARL} to solve PARL problem, establishing sample complexity bounds of order $\\mathcal{O}(1/T)$. Our empirical results substantiate that the proposed \\textsf{PARL} can address the alignment concerns in RL by showing significant improvements (up to 63\\% in terms of required samples) for policy alignment in large-scale environments of the Deepmind control suite and Meta world tasks.",
    "citation_count": 17,
    "summary": "PARL is a novel unified framework for policy alignment in reinforcement learning from human feedback, addressing existing limitations by formulating the problem as a bilevel optimization problem where the upper objective (reward design) is parameterized by the lower objective (optimal policy). This approach improves sample efficiency and policy alignment, as demonstrated by empirical results showing significant performance gains."
  },
  {
    "url": "https://arxiv.org/pdf/2306.06808.pdf",
    "title": "Multi-Agent Reinforcement Learning Guided by Signal Temporal Logic Specifications",
    "published_date": "2023-06-11",
    "abstract": "Reward design is a key component of deep reinforcement learning, yet some tasks and designer's objectives may be unnatural to define as a scalar cost function. Among the various techniques, formal methods integrated with DRL have garnered considerable attention due to their expressiveness and flexibility to define the reward and requirements for different states and actions of the agent. However, how to leverage Signal Temporal Logic (STL) to guide multi-agent reinforcement learning reward design remains unexplored. Complex interactions, heterogeneous goals and critical safety requirements in multi-agent systems make this problem even more challenging. In this paper, we propose a novel STL-guided multi-agent reinforcement learning framework. The STL requirements are designed to include both task specifications according to the objective of each agent and safety specifications, and the robustness values of the STL specifications are leveraged to generate rewards. We validate the advantages of our method through empirical studies. The experimental results demonstrate significant reward performance improvements compared to MARL without STL guidance, along with a remarkable increase in the overall safety rate of the multi-agent systems.",
    "citation_count": 7,
    "summary": "This paper introduces a novel multi-agent reinforcement learning framework guided by Signal Temporal Logic (STL) specifications, using STL robustness values to generate rewards and improve both task performance and safety compared to standard MARL approaches. The framework incorporates both task and safety specifications into the STL design."
  },
  {
    "url": "https://arxiv.org/abs/2310.05871",
    "title": "Dynamic value alignment through preference aggregation of multiple objectives",
    "published_date": "2023-10-09",
    "abstract": "The development of ethical AI systems is currently geared toward setting objective functions that align with human objectives. However, finding such functions remains a research challenge, while in RL, setting rewards by hand is a fairly standard approach. We present a methodology for dynamic value alignment, where the values that are to be aligned with are dynamically changing, using a multiple-objective approach. We apply this approach to extend Deep $Q$-Learning to accommodate multiple objectives and evaluate this method on a simplified two-leg intersection controlled by a switching agent.Our approach dynamically accommodates the preferences of drivers on the system and achieves better overall performance across three metrics (speeds, stops, and waits) while integrating objectives that have competing or conflicting actions.",
    "summary": "This paper proposes a dynamic value alignment method for reinforcement learning agents, using multi-objective optimization to adapt to changing user preferences, demonstrated through a traffic control application where it improves performance across multiple competing metrics."
  },
  {
    "url": "https://www.alignmentforum.org/posts/sHGxvJrBag7nhTQvb/invulnerable-incomplete-preferences-a-formal-statement-1",
    "author": "Sami Petersen",
    "title": "Invulnerable Incomplete Preferences: \nA Formal Statement",
    "published_date": "2023-08-30",
    "summary": "This paper argues against the claim that agents with incomplete preferences, as proposed for AI safety, necessarily adopt strictly dominated strategies. It introduces a dynamic choice rule (\"Dynamic Strong Maximality\") that ensures such agents remain invulnerable to exploitation while preserving their incomplete preferences."
  },
  {
    "url": "https://arxiv.org/pdf/2201.00286v1.pdf",
    "title": "Reinforcement Learning for Task Specifications with Action-Constraints",
    "published_date": "2022-01-02",
    "abstract": "In this paper, we use concepts from supervisory control theory of discrete event systems to propose a method to learn optimal control policies for a finite-state Markov Decision Process (MDP) in which (only) certain sequences of actions are deemed unsafe (respectively safe). We assume that the set of action sequences that are deemed unsafe and/or safe are given in terms of a finite-state automaton; and propose a supervisor that disables a subset of actions at every state of the MDP so that the constraints on action sequence are satisfied. Then we present a version of the Q-learning algorithm for learning optimal policies in the presence of non-Markovian actionsequence and state constraints, where we use the development of reward machines to handle the state constraints. We illustrate the method using an example that captures the utility of automata-based methods for non-Markovian state and action specifications for reinforcement learning and show the results of simulations in this setting.",
    "summary": "This paper presents a Q-learning algorithm adapted for Markov Decision Processes with action sequence constraints, specified via a finite-state automaton, using a supervisor to enforce safety and a reward machine to handle state constraints. The method learns optimal policies while respecting these non-Markovian constraints."
  },
  {
    "url": "https://arxiv.org/pdf/2206.03348.pdf",
    "title": "Specification-Guided Learning of Nash Equilibria with High Social Welfare",
    "published_date": "2022-06-06",
    "abstract": "Reinforcement learning has been shown to be an effective strategy for automatically training policies for challenging control problems. Focusing on non-cooperative multi-agent systems, we propose a novel reinforcement learning framework for training joint policies that form a Nash equilibrium. In our approach, rather than providing low-level reward functions, the user provides high-level specifications that encode the objective of each agent. Then, guided by the structure of the specifications, our algorithm searches over policies to identify one that provably forms an $\\epsilon$-Nash equilibrium (with high probability). Importantly, it prioritizes policies in a way that maximizes social welfare across all agents. Our empirical evaluation demonstrates that our algorithm computes equilibrium policies with high social welfare, whereas state-of-the-art baselines either fail to compute Nash equilibria or compute ones with comparatively lower social welfare.",
    "citation_count": 7,
    "summary": "This paper introduces a reinforcement learning framework that uses high-level specifications to train agents' policies to form a Nash equilibrium, prioritizing those with high social welfare, outperforming existing methods in both equilibrium attainment and social welfare maximization."
  },
  {
    "url": "https://arxiv.org/pdf/2205.04279v1.pdf",
    "title": "Aligned with Whom? Direct and social goals for AI systems",
    "published_date": "2022-05-09",
    "abstract": "As artificial intelligence (AI) becomes more powerful and widespread, the AI alignment problem - how to ensure that AI systems pursue the goals that we want them to pursue - has garnered growing attention. This article distinguishes two types of alignment problems depending on whose goals we consider, and analyzes the different solutions necessitated by each. The direct alignment problem considers whether an AI system accomplishes the goals of the entity operating it. In contrast, the social alignment problem considers the effects of an AI system on larger groups or on society more broadly. In particular, it also considers whether the system imposes externalities on others. Whereas solutions to the direct alignment problem center around more robust implementation, social alignment problems typically arise because of conflicts between individual and group-level goals, elevating the importance of AI governance to mediate such conflicts. Addressing the social alignment problem requires both enforcing existing norms on their developers and operators and designing new norms that apply directly to AI systems.",
    "citation_count": 8,
    "summary": "The paper distinguishes between direct AI alignment (ensuring an AI system achieves its operator's goals) and social AI alignment (ensuring beneficial societal impact), arguing that while the former focuses on robust implementation, the latter necessitates AI governance to manage conflicts between individual and group interests and establish appropriate norms."
  },
  {
    "url": "https://arxiv.org/pdf/2102.03896.pdf",
    "title": "Consequences of Misaligned AI",
    "published_date": "2021-02-07",
    "abstract": "AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J<L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.",
    "citation_count": 63,
    "summary": "This paper models the risks of misaligned AI by analyzing a principal-agent system with an incomplete reward function, finding that optimizing for a partial objective can lead to arbitrarily low overall utility unless the reward function is comprehensive or dynamically updated. The results highlight the need for interactive and dynamic reward function design in AI."
  }
]