[
  {
    "url": "https://arxiv.org/abs/2412.16559",
    "title": "Metagoals Endowing Self-Modifying AGI Systems with Goal Stability or Moderated Goal Evolution: Toward a Formally Sound and Practical Approach",
    "published_date": "2024-12-21",
    "abstract": "We articulate here a series of specific metagoals designed to address the challenge of creating AGI systems that possess the ability to flexibly self-modify yet also have the propensity to maintain key invariant properties of their goal systems 1) a series of goal-stability metagoals aimed to guide a system to a condition in which goal-stability is compatible with reasonably flexible self-modification 2) a series of moderated-goal-evolution metagoals aimed to guide a system to a condition in which control of the pace of goal evolution is compatible with reasonably flexible self-modification The formulation of the metagoals is founded on fixed-point theorems from functional analysis, e.g. the Contraction Mapping Theorem and constructive approximations to Schauder's Theorem, applied to probabilistic models of system behavior We present an argument that the balancing of self-modification with maintenance of goal invariants will often have other interesting cognitive side-effects such as a high degree of self understanding Finally we argue for the practical value of a hybrid metagoal combining moderated-goal-evolution with pursuit of goal-stability -- along with potentially other metagoals relating to goal-satisfaction, survival and ongoing development -- in a flexible fashion depending on the situation",
    "summary": "This paper proposes metagoals, grounded in fixed-point theorems, to enable self-modifying AGI systems to maintain stable goals or control the rate of goal evolution. These metagoals aim to balance flexible self-modification with the preservation of crucial goal properties, potentially leading to enhanced self-understanding and practical adaptability."
  },
  {
    "url": "https://arxiv.org/abs/2405.13290",
    "title": "Theoretical Analysis of Meta Reinforcement Learning: Generalization Bounds and Convergence Guarantees",
    "published_date": "2024-05-17",
    "abstract": "This research delves deeply into Meta Reinforcement Learning (Meta RL) through a exploration focusing on defining generalization limits and ensuring convergence. By employing a approach this article introduces an innovative theoretical framework to meticulously assess the effectiveness and performance of Meta RL algorithms. We present an explanation of generalization limits measuring how well these algorithms can adapt to learning tasks while maintaining consistent results. Our analysis delves into the factors that impact the adaptability of Meta RL revealing the relationship, between algorithm design and task complexity. Additionally we establish convergence assurances by proving conditions under which Meta RL strategies are guaranteed to converge towards solutions. We examine the convergence behaviors of Meta RL algorithms across scenarios providing a comprehensive understanding of the driving forces behind their long term performance. This exploration covers both convergence and real time efficiency offering a perspective, on the capabilities of these algorithms.",
    "citation_count": 17,
    "summary": "This paper provides a theoretical framework for analyzing Meta Reinforcement Learning, establishing generalization bounds to quantify algorithm adaptability and proving convergence guarantees under specific conditions. The analysis explores the interplay between algorithm design, task complexity, and convergence behavior."
  },
  {
    "url": "https://arxiv.org/abs/2409.03682",
    "title": "A New First-Order Meta-Learning Algorithm with Convergence Guarantees",
    "published_date": "2024-09-05",
    "abstract": "Learning new tasks by drawing on prior experience gathered from other (related) tasks is a core property of any intelligent system. Gradient-based meta-learning, especially MAML and its variants, has emerged as a viable solution to accomplish this goal. One problem MAML encounters is its computational and memory burdens needed to compute the meta-gradients. We propose a new first-order variant of MAML that we prove converges to a stationary point of the MAML objective, unlike other first-order variants. We also show that the MAML objective does not satisfy the smoothness assumption assumed in previous works; we show instead that its smoothness constant grows with the norm of the meta-gradient, which theoretically suggests the use of normalized or clipped-gradient methods compared to the plain gradient method used in previous works. We validate our theory on a synthetic experiment.",
    "citation_count": 1,
    "summary": "This paper introduces a novel first-order meta-learning algorithm, proven to converge to a stationary point of the MAML objective, addressing the computational limitations of MAML while theoretically justifying the use of normalized or clipped gradients. The algorithm's convergence is guaranteed despite the non-smoothness of the MAML objective, a property not considered in prior work."
  },
  {
    "url": "https://arxiv.org/pdf/2109.14595v2.pdf",
    "title": "Generalization Bounds For Meta-Learning: An Information-Theoretic Analysis",
    "published_date": "2021-09-29",
    "abstract": "We derive a novel information-theoretic analysis of the generalization property of meta-learning algorithms. Concretely, our analysis proposes a generic understanding of both the conventional learning-to-learn framework and the modern model-agnostic meta-learning (MAML) algorithms. Moreover, we provide a data-dependent generalization bound for a stochastic variant of MAML, which is non-vacuous for deep few-shot learning. As compared to previous bounds that depend on the square norm of gradients, empirical validations on both simulated data and a well-known few-shot benchmark show that our bound is orders of magnitude tighter in most situations.",
    "citation_count": 39,
    "summary": "This paper presents a novel information-theoretic analysis of meta-learning generalization, offering a unified framework for understanding both traditional and MAML algorithms. The authors derive a data-dependent generalization bound for stochastic MAML, empirically demonstrating its superior tightness compared to existing bounds in few-shot learning scenarios."
  },
  {
    "url": "https://arxiv.org/pdf/2109.11792v1.pdf",
    "title": "Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability",
    "published_date": "2021-09-24",
    "abstract": "In the Bayesian reinforcement learning (RL) setting, a prior distribution over the unknown problem parameters -- the rewards and transitions -- is assumed, and a policy that optimizes the (posterior) expected return is sought. A common approximation, which has been recently popularized as meta-RL, is to train the agent on a sample of N problem instances from the prior, with the hope that for large enough N, good generalization behavior to an unseen test instance will be obtained. \n In this work, we study generalization in Bayesian RL under the probably approximately correct (PAC) framework, using the method of algorithmic stability. Our main contribution is showing that by adding regularization, the optimal policy becomes uniformly stable in an appropriate sense. Most stability results in the literature build on strong convexity of the regularized loss -- an approach that is not suitable for RL as Markov decision processes (MDPs) are not convex. Instead, building on recent results of fast convergence rates for mirror descent in regularized MDPs, we show that regularized MDPs satisfy a certain quadratic growth criterion, which is sufficient to establish stability. This result, which may be of independent interest, allows us to study the effect of regularization on generalization in the Bayesian RL setting.",
    "citation_count": 7,
    "summary": "This paper analyzes generalization in Bayesian reinforcement learning using algorithmic stability, demonstrating that regularization, leveraging a novel quadratic growth criterion instead of strong convexity, ensures uniform stability of the optimal policy and thus improves generalization. This result is achieved within a PAC framework."
  },
  {
    "url": "https://arxiv.org/abs/2106.04911v2",
    "title": "Memory-Based Optimization Methods for Model-Agnostic Meta-Learning and Personalized Federated Learning",
    "published_date": "2021-06-09",
    "abstract": "In recent years, model-agnostic meta-learning (MAML) has become a popular research area. However, the stochastic optimization of MAML is still underdeveloped. Existing MAML algorithms rely on the ``episode'' idea by sampling a few tasks and data points to update the meta-model at each iteration. Nonetheless, these algorithms either fail to guarantee convergence with a constant mini-batch size or require processing a large number of tasks at every iteration, which is unsuitable for continual learning or cross-device federated learning where only a small number of tasks are available per iteration or per round. To address these issues, this paper proposes memory-based stochastic algorithms for MAML that converge with vanishing error. The proposed algorithms require sampling a constant number of tasks and data samples per iteration, making them suitable for the continual learning scenario. Moreover, we introduce a communication-efficient memory-based MAML algorithm for personalized federated learning in cross-device (with client sampling) and cross-silo (without client sampling) settings. Our theoretical analysis improves the optimization theory for MAML, and our empirical results corroborate our theoretical findings. Interested readers can access our code at \\url{https://github.com/bokun-wang/moml}.",
    "citation_count": 8,
    "summary": "This paper introduces memory-based stochastic algorithms for model-agnostic meta-learning (MAML), guaranteeing convergence with a constant mini-batch size and enabling efficient continual and federated learning. These algorithms are theoretically analyzed and empirically validated, showing improved performance in resource-constrained settings."
  },
  {
    "url": "https://arxiv.org/abs/2106.06098v2",
    "title": "Meta-Adaptive Nonlinear Control: Theory and Algorithms",
    "published_date": "2021-06-11",
    "abstract": "We present an online multi-task learning approach for adaptive nonlinear control, which we call Online Meta-Adaptive Control (OMAC). The goal is to control a nonlinear system subject to adversarial disturbance and unknown $\\textit{environment-dependent}$ nonlinear dynamics, under the assumption that the environment-dependent dynamics can be well captured with some shared representation. Our approach is motivated by robot control, where a robotic system encounters a sequence of new environmental conditions that it must quickly adapt to. A key emphasis is to integrate online representation learning with established methods from control theory, in order to arrive at a unified framework that yields both control-theoretic and learning-theoretic guarantees. We provide instantiations of our approach under varying conditions, leading to the first non-asymptotic end-to-end convergence guarantee for multi-task nonlinear control. OMAC can also be integrated with deep representation learning. Experiments show that OMAC significantly outperforms conventional adaptive control approaches which do not learn the shared representation, in inverted pendulum and 6-DoF drone control tasks under varying wind conditions.",
    "citation_count": 39,
    "summary": "Online Meta-Adaptive Control (OMAC) is a novel multi-task learning approach for adaptive nonlinear control that leverages shared representations to handle environment-dependent dynamics, providing the first non-asymptotic convergence guarantee for multi-task nonlinear control and outperforming conventional methods in experiments."
  },
  {
    "url": "https://arxiv.org/pdf/2108.08770.pdf",
    "title": "Learning-to-learn non-convex piecewise-Lipschitz functions",
    "published_date": "2021-08-19",
    "abstract": "We analyze the meta-learning of the initialization and step-size of learning algorithms for piecewise-Lipschitz functions, a non-convex setting with applications to both machine learning and algorithms. Starting from recent regret bounds for the exponential forecaster on losses with dispersed discontinuities, we generalize them to be initialization-dependent and then use this result to propose a practical meta-learning procedure that learns both the initialization and the step-size of the algorithm from multiple online learning tasks. Asymptotically, we guarantee that the average regret across tasks scales with a natural notion of task-similarity that measures the amount of overlap between near-optimal regions of different tasks. Finally, we instantiate the method and its guarantee in two important settings: robust meta-learning and multi-task data-driven algorithm design.",
    "citation_count": 13,
    "summary": "This paper proposes a meta-learning method for optimizing the initialization and step size of algorithms designed for non-convex piecewise-Lipschitz functions, providing asymptotic regret bounds that depend on task similarity. The method is applied to robust meta-learning and multi-task algorithm design."
  }
]