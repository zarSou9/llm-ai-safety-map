[
  {
    "url": "https://arxiv.org/abs/2209.01501",
    "title": "Meta-Learning with Less Forgetting on Large-Scale Non-Stationary Task Distributions",
    "published_date": "2022-09-03",
    "abstract": ". The paradigm of machine intelligence moves from purely supervised learning to a more practical scenario when many loosely related unlabeled data are available and labeled data is scarce. Most existing algo-rithms assume that the underlying task distribution is stationary. Here we consider a more realistic and challenging setting in that task distributions evolve over time. We name this problem as S emi-supervised meta-learning with E volving T ask di S tributions, abbreviated as SETS . Two key challenges arise in this more realistic setting: (i) how to use unlabeled data in the presence of a large amount of unlabeled out-of-distribution (OOD) data; and (ii) how to prevent catastrophic forgetting on previously learned task distributions due to the task distribution shift. We propose an O OD R obust and knowle D ge pres E rved semi-supe R vised meta-learning approach ( ORDER ) â€¡ , to tackle these two major challenges. Specifically, our ORDER introduces a novel mutual information regularization to robustify the model with unlabeled OOD data and adopts an optimal transport regularization to remember previously learned knowledge in feature space. In addition, we test our method on a very challenging dataset: SETS on large-scale non-stationary semi-supervised task distributions consisting of (at least) 72K tasks. With extensive experiments, we demonstrate the proposed ORDER alleviates forgetting on evolving task distributions and is more robust to OOD data than related strong baselines.",
    "citation_count": 13,
    "summary": "The paper introduces ORDER, a semi-supervised meta-learning method addressing the challenges of evolving task distributions and out-of-distribution data in large-scale non-stationary settings. ORDER uses mutual information and optimal transport regularizations to improve robustness and prevent catastrophic forgetting."
  },
  {
    "url": "https://arxiv.org/abs/2103.01133v2",
    "title": "Posterior Meta-Replay for Continual Learning",
    "published_date": "2021-03-01",
    "abstract": "Continual Learning (CL) algorithms have recently received a lot of attention as they attempt to overcome the need to train with an i.i.d. sample from some unknown target data distribution. Building on prior work, we study principled ways to tackle the CL problem by adopting a Bayesian perspective and focus on continually learning a task-specific posterior distribution via a shared meta-model, a task-conditioned hypernetwork. This approach, which we term Posterior-replay CL, is in sharp contrast to most Bayesian CL approaches that focus on the recursive update of a single posterior distribution. The benefits of our approach are (1) an increased flexibility to model solutions in weight space and therewith less susceptibility to task dissimilarity, (2) access to principled task-specific predictive uncertainty estimates, that can be used to infer task identity during test time and to detect task boundaries during training, and (3) the ability to revisit and update task-specific posteriors in a principled manner without requiring access to past data. The proposed framework is versatile, which we demonstrate using simple posterior approximations (such as Gaussians) as well as powerful, implicit distributions modelled via a neural network. We illustrate the conceptual advance of our framework on low-dimensional problems and show performance gains on computer vision benchmarks.",
    "citation_count": 52,
    "summary": "Posterior Meta-Replay continually learns task-specific posterior distributions using a shared meta-model (hypernetwork), offering increased flexibility, principled uncertainty estimates for task inference, and the ability to update past tasks without requiring past data. This contrasts with single-posterior Bayesian CL methods, improving performance on computer vision benchmarks."
  },
  {
    "url": "https://arxiv.org/abs/2106.04911v2",
    "title": "Memory-Based Optimization Methods for Model-Agnostic Meta-Learning and Personalized Federated Learning",
    "published_date": "2021-06-09",
    "abstract": "In recent years, model-agnostic meta-learning (MAML) has become a popular research area. However, the stochastic optimization of MAML is still underdeveloped. Existing MAML algorithms rely on the ``episode'' idea by sampling a few tasks and data points to update the meta-model at each iteration. Nonetheless, these algorithms either fail to guarantee convergence with a constant mini-batch size or require processing a large number of tasks at every iteration, which is unsuitable for continual learning or cross-device federated learning where only a small number of tasks are available per iteration or per round. To address these issues, this paper proposes memory-based stochastic algorithms for MAML that converge with vanishing error. The proposed algorithms require sampling a constant number of tasks and data samples per iteration, making them suitable for the continual learning scenario. Moreover, we introduce a communication-efficient memory-based MAML algorithm for personalized federated learning in cross-device (with client sampling) and cross-silo (without client sampling) settings. Our theoretical analysis improves the optimization theory for MAML, and our empirical results corroborate our theoretical findings. Interested readers can access our code at \\url{https://github.com/bokun-wang/moml}.",
    "citation_count": 8,
    "summary": "This paper introduces novel memory-based stochastic algorithms for model-agnostic meta-learning (MAML), enabling convergence with a constant mini-batch size and thus suitability for continual and federated learning scenarios. These algorithms are theoretically analyzed and empirically validated, offering both improved optimization and communication efficiency for personalized federated learning."
  },
  {
    "url": "https://arxiv.org/pdf/2103.08233v2.pdf",
    "title": "Robust Maml: Prioritization Task Buffer with Adaptive Learning Process for Model-Agnostic Meta-Learning",
    "published_date": "2021-03-15",
    "abstract": "Model agnostic meta-learning (MAML) is a popular state-of-the-art meta-learning algorithm that provides good weight initialization of a model given a variety of learning tasks. The model initialized by provided weight can be fine-tuned to an unseen task despite only using a small amount of samples and within a few adaptation steps. MAML is simple and versatile but requires costly learning rate tuning and careful design of the task distribution which affects its scalability and generalization. This paper proposes a more robust MAML based on an adaptive learning scheme and a prioritization task buffer (PTB) referred to as Robust MAML (RMAML) for improving scalability of training process and alleviating the problem of distribution mismatch. RMAML uses gradient-based hyper-parameter optimization to automatically find the optimal learning rate and uses the PTB to gradually adjust training task distribution toward testing task distribution over the course of training. Experimental results on meta reinforcement learning environments demonstrate a substantial performance gain as well as being less sensitive to hyper-parameter choice and robust to distribution mismatch.",
    "citation_count": 10,
    "summary": "Robust MAML (RMAML) improves the scalability and robustness of Model-Agnostic Meta-Learning (MAML) by employing an adaptive learning rate and a prioritization task buffer that dynamically adjusts the training task distribution to better match the testing distribution. This leads to improved performance and reduced sensitivity to hyperparameter tuning."
  },
  {
    "title": "Learning to Balance Local Losses via Meta-Learning",
    "abstract": "The standard training for deep neural networks relies on a global and fixed loss function. For more effective training, dynamic loss functions have been recently proposed. However, the dynamic global loss function is not flexible to differentially train layers in complex deep neural networks. In this paper, we propose a general framework that learns to adaptively train each layer of deep neural networks via meta-learning. Our framework leverages the local error signals from layers and identifies which layer needs to be trained more at every iteration. Also, the proposed method improves the local loss function with our minibatch-wise dropout and cross-validation loop to alleviate meta-overfitting. The experiments show that our method achieved competitive performance compared to state-of-the-art methods on popular benchmark datasets for image classification: CIFAR-10 and CIFAR-100. Surprisingly, our method enables training deep neural networks without skip-connections using dynamically weighted local loss functions.",
    "published_date": "2021-01-01",
    "citation_count": 1,
    "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09541196.pdf",
    "summary": "This paper introduces a meta-learning framework that dynamically adjusts the training of individual layers in deep neural networks by learning to weight local loss functions, improving training efficiency and achieving competitive performance on image classification benchmarks. The method mitigates overfitting through minibatch-wise dropout and cross-validation."
  },
  {
    "title": "MetaStore: A Task-adaptative Meta-learning Model for Optimal Store Placement with Multi-city Knowledge Transfer",
    "abstract": "\n Optimal store placement aims to identify the optimal location for a new brick-and-mortar store that can maximize its sale by analyzing and mining users' preferences from large-scale urban data. In recent years, the expansion of chain enterprises in new cities brings some challenges because of two aspects: (1)\n data scarcity in new cities,\n so most existing models tend to not work (i.e., overfitting), because the superior performance of these works is conditioned on large-scale training samples; (2)\n data distribution discrepancy among different cities,\n so knowledge learned from other cities cannot be utilized directly in new cities. In this article, we propose a task-adaptative model-agnostic meta-learning framework, namely, MetaStore, to tackle these two challenges and improve the prediction performance in new cities with insufficient data for optimal store placement, by transferring prior knowledge learned from multiple data-rich cities. Specifically, we develop a task-adaptative meta-learning algorithm to learn city-specific prior initializations from multiple cities, which is capable of handling the multimodal data distribution and accelerating the adaptation in new cities compared to other methods. In addition, we design an effective learning strategy for MetaStore to promote faster convergence and optimization by sampling high-quality data for each training batch in view of noisy data in practical applications. The extensive experimental results demonstrate that our proposed method leads to state-of-the-art performance compared with various baselines.\n",
    "published_date": "2021-04-22",
    "citation_count": 12,
    "url": "https://dl.acm.org/doi/10.1145/3447271",
    "summary": "MetaStore is a task-adaptive meta-learning model for optimal store placement that addresses data scarcity and distribution discrepancies across cities by transferring knowledge from data-rich cities to data-poor ones. This improves prediction accuracy in new cities by learning city-specific initializations and employing a high-quality data sampling strategy."
  },
  {
    "url": "https://www.alignmentforum.org/s/xujLGRKFLKsPCTimd/p/y2XyxomuEpMaRYDQw",
    "author": "Stuart_Armstrong",
    "title": "AI, learn to be conservative, then learn to be less so: reducing side-effects, learning preserved features, and going beyond conservatism",
    "published_date": "2021-09-20",
    "summary": "Two research projects aim to improve AI safety by focusing on model splintering and learning irrational agent preferences. The core approach involves training AI agents in simulated environments to learn acceptable behavior by observing and mimicking the typical actions of \"human\" agents within those environments, thus avoiding undesirable side effects."
  },
  {
    "url": "https://arxiv.org/abs/2008.02219v1",
    "title": "Meta Continual Learning via Dynamic Programming",
    "published_date": "2020-08-05",
    "abstract": "Meta continual learning algorithms seek to train a model when faced with similar tasks observed in a sequential manner. Despite promising methodological advancements, there is a lack of theoretical frameworks that enable analysis of learning challenges such as generalization and catastrophic forgetting. To that end, we develop a new theoretical approach for meta continual learning~(MCL) where we mathematically model the learning dynamics using dynamic programming, and we establish conditions of optimality for the MCL problem. Moreover, using the theoretical framework, we derive a new dynamic-programming-based MCL method that adopts stochastic-gradient-driven alternating optimization to balance generalization and catastrophic forgetting. We show that, on MCL benchmark data sets, our theoretically grounded method achieves accuracy better than or comparable to that of existing state-of-the-art methods.",
    "citation_count": 8,
    "summary": "This paper introduces a novel theoretical framework for meta-continual learning (MCL) using dynamic programming, establishing optimality conditions and deriving a new algorithm that balances generalization and catastrophic forgetting. Empirical results demonstrate its competitive performance against state-of-the-art MCL methods."
  }
]