[
  {
    "url": "https://arxiv.org/abs/2408.07962",
    "title": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning",
    "published_date": "2024-08-15",
    "abstract": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.",
    "summary": "Meta SAC-Lag is a safe reinforcement learning algorithm that uses meta-gradient optimization to automatically tune safety hyperparameters, improving the reliability and efficiency of Lagrangian-based methods for real-world deployment, as demonstrated through simulations and a real-world robotic arm experiment."
  },
  {
    "url": "https://arxiv.org/pdf/2303.11183.pdf",
    "title": "Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning",
    "published_date": "2023-03-20",
    "abstract": "The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pretrained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PURER, which contains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta testing. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. The ECI adaptively increases the difficulty level of pseudo episodes according to the real-time feedback of the meta model. We formulate the optimization process of meta training with ECI as an adversarial form in an end-to-end manner. During meta testing, we further propose a simple plug-and-play supplement—ICFIL—only used during meta testing to narrow the gap between meta training and meta testing task distribution. Extensive experiments in various real-world scenarios show the superior performance of ours.",
    "citation_count": 5,
    "summary": "PURER, a novel data-free meta-learning framework, addresses limitations of existing methods by leveraging a pseudo-episode curriculum inversion for meta-training and an inversion calibration technique for meta-testing, enabling architecture and model-size agnostic adaptation to new tasks without access to original training data. This allows for superior performance across diverse real-world scenarios."
  },
  {
    "title": "Online Decentralized Multi-Agents Meta-Learning With Byzantine Resiliency",
    "abstract": "Meta-learning is a learning-to-learn paradigm that leverages past learning experiences for quick adaptation to new learning tasks. It has a wide application, such as in few-shot learning, reinforcement learning, neural architecture search, federated learning, etc. It has been extended to the online learning setting where task data distribution arrives sequentially. This provides continuous lifelong learning. However, in the online meta-learning setting, a single agent has to learn many varieties of related tasks. Yet, a single agent is limited to its local task data and must collaborate with neighboring agents to improve its learning performance. Therefore, online decentralized meta-learning algorithms are designed to allow an agent to collaborate with neighboring agents in order to improve learning performance. Despite their advantages, online decentralized meta-learning algorithms are susceptible to Byzantine attacks caused by the diffusion of poisonous information from unidentifiable Byzantine agents in the network. This is a serious problem where normal agents are unable to learn and convergence to the global meta-initializer is thwarted. State-of-the-art algorithms, such as BRIDGE, designed to provide robustness against Byzantine attacks are slow and cannot work in online learning settings. Therefore, we propose an online decentralized meta-learning algorithm that works with two Byzantine-resilient aggregation techniques, which are modified coordinate-wise screening and centerpoint aggregation. The proposed algorithm provides faster convergence speed and guarantees both resiliency and continuous lifelong learning. Our simulation results show that the proposed algorithm performs better than state-of-the-art algorithms.",
    "published_date": "2023-01-01",
    "citation_count": 1,
    "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10171341.pdf",
    "summary": "This paper introduces a novel online decentralized meta-learning algorithm robust to Byzantine attacks, achieving faster convergence than existing methods by employing modified coordinate-wise screening and centerpoint aggregation techniques for resilient information aggregation. The algorithm enables continuous lifelong learning while mitigating the impact of malicious agents."
  },
  {
    "url": "https://www.lesswrong.com/posts/2eaLH7zp6pxdQwYSH",
    "author": "Austin Witte",
    "title": "A Brief Overview of AI Safety/Alignment Orgs, Fields, Researchers, and Resources for ML Researchers",
    "published_date": "2023-02-02",
    "summary": "Two overview documents, a short and long version, have been created to help machine learning researchers quickly assess their existing skills' relevance to AI safety research. These resources list organizations, researchers, key papers, and keywords to facilitate efficient exploration of the field."
  },
  {
    "url": "https://www.alignmentforum.org/tag/ai",
    "author": "Evan Hubinger",
    "title": "AI - AI Alignment Forum",
    "published_date": "2023-02-06",
    "summary": "Artificial intelligence alignment focuses on ensuring powerful AI systems act in accordance with human values, addressing the existential risk of misaligned AI pursuing unintended goals. This field encompasses various approaches, from narrow tasks to achieving a beneficial future for humanity, and involves research across theoretical, engineering, and strategic domains."
  },
  {
    "url": "https://arxiv.org/abs/2212.14094",
    "title": "Wormhole MAML: Meta-Learning in Glued Parameter Space",
    "published_date": "2022-12-28",
    "abstract": "In this paper, we introduce a novel variation of model-agnostic meta-learning, where an extra multiplicative parameter is introduced in the inner-loop adaptation. Our variation creates a shortcut in the parameter space for the inner-loop adaptation and increases model expressivity in a highly controllable manner. We show both theoretically and numerically that our variation alleviates the problem of conflicting gradients and improves training dynamics. We conduct experiments on 3 distinctive problems, including a toy classification problem for threshold comparison, a regression problem for wavelet transform, and a classification problem on MNIST. We also discuss ways to generalize our method to a broader class of problems.",
    "summary": "Wormhole MAML introduces a multiplicative parameter to the inner loop of Model-Agnostic Meta-Learning (MAML), creating a shortcut in parameter space that improves expressivity and mitigates conflicting gradients, leading to enhanced meta-learning performance. Experimental results across diverse tasks support this improvement."
  },
  {
    "url": "https://arxiv.org/abs/2210.10485",
    "title": "Learning Transferable Adversarial Robust Representations via Multi-view Consistency",
    "published_date": "2022-10-19",
    "abstract": "Despite the success on few-shot learning problems, most meta-learned models only focus on achieving good performance on clean examples and thus easily break down when given adversarially perturbed samples. While some recent works have shown that a combination of adversarial learning and meta-learning could enhance the robustness of a meta-learner against adversarial attacks, they fail to achieve generalizable adversarial robustness to unseen domains and tasks, which is the ultimate goal of meta-learning. To address this challenge, we propose a novel meta-adversarial multi-view representation learning framework with dual encoders. Specifically, we introduce the discrepancy across the two differently augmented samples of the same data instance by first updating the encoder parameters with them and further imposing a novel label-free adversarial attack to maximize their discrepancy. Then, we maximize the consistency across the views to learn transferable robust representations across domains and tasks. Through experimental validation on multiple benchmarks, we demonstrate the effectiveness of our framework on few-shot learning tasks from unseen domains, achieving over 10\\% robust accuracy improvements against previous adversarial meta-learning baselines.",
    "summary": "This paper introduces a multi-view meta-learning framework that improves the adversarial robustness of few-shot learning models by leveraging dual encoders and a novel label-free adversarial attack to learn consistent, transferable representations across domains. This approach achieves significant improvements in robust accuracy on unseen domains compared to existing methods."
  },
  {
    "url": "https://arxiv.org/pdf/2109.01255v1.pdf",
    "title": "Provably Safe Model-Based Meta Reinforcement Learning: An Abstraction-Based Approach",
    "published_date": "2021-09-03",
    "abstract": "While conventional reinforcement learning focuses on designing agents that can perform one task, meta-learning aims, instead, to solve the problem of designing agents that can generalize to different tasks (e.g., environments, obstacles, and goals) that were not considered during the design or the training of these agents. In this paper, we consider the problem of training a provably safe Neural Network (NN) controller for uncertain nonlinear dynamical systems that can generalize to new tasks that were not present in the training data while preserving strong safety guarantees. Our approach is to learn a set of NNs during the training phase. When the task becomes available at runtime, our framework will carefully select a subset of NNs and compose them to form the final NN controller. Critical to our approach is the ability to compute a finite-state abstraction of the nonlinear dynamical system. This abstract model captures the behavior of the closed-loop system under all possible NN weights, and is used to train the NNs and compose them when the task becomes available. We provide theoretical guarantees on the correctness of the resulting NN controller and show the efficacy of our approach via simulations.",
    "citation_count": 3,
    "summary": "This paper presents a provably safe meta-reinforcement learning method for uncertain nonlinear systems, using a finite-state abstraction to train and compose neural network controllers that generalize to unseen tasks while maintaining safety guarantees. The approach leverages abstract models to select and combine pre-trained networks for optimal performance on new tasks."
  }
]