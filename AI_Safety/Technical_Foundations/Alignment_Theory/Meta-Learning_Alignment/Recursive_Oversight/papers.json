[
  {
    "url": "https://arxiv.org/abs/2412.16559",
    "title": "Metagoals Endowing Self-Modifying AGI Systems with Goal Stability or Moderated Goal Evolution: Toward a Formally Sound and Practical Approach",
    "published_date": "2024-12-21",
    "abstract": "We articulate here a series of specific metagoals designed to address the challenge of creating AGI systems that possess the ability to flexibly self-modify yet also have the propensity to maintain key invariant properties of their goal systems 1) a series of goal-stability metagoals aimed to guide a system to a condition in which goal-stability is compatible with reasonably flexible self-modification 2) a series of moderated-goal-evolution metagoals aimed to guide a system to a condition in which control of the pace of goal evolution is compatible with reasonably flexible self-modification The formulation of the metagoals is founded on fixed-point theorems from functional analysis, e.g. the Contraction Mapping Theorem and constructive approximations to Schauder's Theorem, applied to probabilistic models of system behavior We present an argument that the balancing of self-modification with maintenance of goal invariants will often have other interesting cognitive side-effects such as a high degree of self understanding Finally we argue for the practical value of a hybrid metagoal combining moderated-goal-evolution with pursuit of goal-stability -- along with potentially other metagoals relating to goal-satisfaction, survival and ongoing development -- in a flexible fashion depending on the situation",
    "summary": "This paper proposes metagoals, based on fixed-point theorems, to enable self-modifying AGI systems to maintain goal stability or control the rate of goal evolution. The authors argue that balancing self-modification with goal invariance may lead to enhanced self-understanding and suggest a hybrid approach combining both types of metagoals for practical application."
  },
  {
    "url": "https://arxiv.org/abs/2410.18636",
    "title": "Multi-agent cooperation through learning-aware policy gradients",
    "published_date": "2024-10-24",
    "abstract": "Self-interested individuals often fail to cooperate, posing a fundamental challenge for multi-agent learning. How can we achieve cooperation among self-interested, independent learning agents? Promising recent work has shown that in certain tasks cooperation can be established between learning-aware agents who model the learning dynamics of each other. Here, we present the first unbiased, higher-derivative-free policy gradient algorithm for learning-aware reinforcement learning, which takes into account that other agents are themselves learning through trial and error based on multiple noisy trials. We then leverage efficient sequence models to condition behavior on long observation histories that contain traces of the learning dynamics of other agents. Training long-context policies with our algorithm leads to cooperative behavior and high returns on standard social dilemmas, including a challenging environment where temporally-extended action coordination is required. Finally, we derive from the iterated prisoner's dilemma a novel explanation for how and when cooperation arises among self-interested learning-aware agents.",
    "summary": "This paper introduces a novel, unbiased policy gradient algorithm for multi-agent reinforcement learning that incorporates awareness of other agents' learning processes, enabling cooperation in social dilemmas through the use of efficient sequence models and long observation histories. The algorithm's effectiveness is demonstrated on standard social dilemmas, and a new explanation for the emergence of cooperation among self-interested agents is derived."
  },
  {
    "url": "https://arxiv.org/pdf/2305.12109.pdf",
    "title": "Meta Neural Coordination",
    "published_date": "2023-05-20",
    "abstract": "Meta-learning aims to develop algorithms that can learn from other learning algorithms to adapt to new and changing environments. This requires a model of how other learning algorithms operate and perform in different contexts, which is similar to representing and reasoning about mental states in the theory of mind. Furthermore, the problem of uncertainty in the predictions of conventional deep neural networks highlights the partial predictability of the world, requiring the representation of multiple predictions simultaneously. This is facilitated by coordination among neural modules, where different modules' beliefs and desires are attributed to others. The neural coordination among modular and decentralized neural networks is a fundamental prerequisite for building autonomous intelligence machines that can interact flexibly and adaptively. In this work, several pieces of evidence demonstrate a new avenue for tackling the problems above, termed Meta Neural Coordination. We discuss the potential advancements required to build biologically-inspired machine intelligence, drawing from both machine learning and cognitive science communities.",
    "summary": "Meta Neural Coordination proposes a novel approach to meta-learning, leveraging neural coordination among modules to represent uncertainty and adapt to changing environments, mirroring the theory of mind's representation of mental states. This framework aims to build more autonomous and adaptable artificial intelligence systems."
  },
  {
    "url": "https://www.lesswrong.com/posts/AKaf8zN2neXQEvLit/role-architectures-applying-llms-to-consequential-tasks",
    "author": "Eric Drexler",
    "title": "Role Architectures:\nApplying LLMs to consequential tasks",
    "published_date": "2023-03-30",
    "summary": "This article argues that large language models (LLMs), lacking inherent agency, can be safely integrated into complex tasks through \"role architectures,\" where diverse LLMs perform specialized roles, mitigating risks associated with powerful, self-directed AI. This approach leverages LLMs' capabilities while addressing concerns about alignment and safety."
  },
  {
    "url": "https://arxiv.org/abs/2109.04504",
    "title": "Bootstrapped Meta-Learning",
    "published_date": "2021-09-09",
    "abstract": "Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent, without backpropagating through the update rule.",
    "citation_count": 55,
    "summary": "Bootstrapped Meta-Learning addresses the meta-optimization challenge by having the meta-learner learn from a bootstrapped target, minimizing the distance to that target. This approach improves performance and efficiency in meta-learning, enabling state-of-the-art results on benchmarks and facilitating efficient exploration strategies."
  },
  {
    "url": "https://arxiv.org/pdf/2103.02265v2.pdf",
    "title": "Meta-Learning with Variational Bayes",
    "published_date": "2021-03-03",
    "abstract": "The field of meta-learning seeks to improve the ability of today's machine learning systems to adapt efficiently to small amounts of data. Typically this is accomplished by training a system with a parametrized update rule to improve a task-relevant objective based on supervision or a reward function. However, in many domains of practical interest, task data is unlabeled, or reward functions are unavailable. In this paper we introduce a new approach to address the more general problem of generative meta-learning, which we argue is an important prerequisite for obtaining human-level cognitive flexibility in artificial agents, and can benefit many practical applications along the way. Our contribution leverages the AEVB framework and mean-field variational Bayes, and creates fast-adapting latent-space generative models. At the heart of our contribution is a new result, showing that for a broad class of deep generative latent variable models, the relevant VB updates do not depend on any generative neural network. The theoretical merits of our approach are reflected in empirical experiments.",
    "summary": "This paper introduces a generative meta-learning approach using variational Bayes, enabling fast adaptation in latent-space generative models even without labeled data or reward functions. A key finding is that variational Bayes updates for a broad class of deep generative models are independent of the generative neural network itself."
  },
  {
    "url": "https://arxiv.org/pdf/2112.10859v1.pdf",
    "title": "Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning",
    "published_date": "2021-12-20",
    "abstract": "Critical sectors of human society are progressing toward the adoption of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents' behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the principle of online cross-validation, the incentive designer explicitly accounts for its impact on agents' learning and, through them, the impact on future social welfare. Experiments on didactic benchmark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a complex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis.",
    "citation_count": 19,
    "summary": "This paper proposes a model-free meta-gradient reinforcement learning method for adaptive incentive design in multi-agent systems, enabling a central planner to indirectly guide selfish agents towards socially optimal behavior by adjusting their payoffs. Experiments demonstrate its effectiveness in achieving near-optimal cooperation and improving societal outcomes, such as in simulated economic scenarios."
  },
  {
    "url": "https://arxiv.org/pdf/2103.06435v1.pdf",
    "title": "Population-Based Evolution Optimizes a Meta-Learning Objective",
    "published_date": "2021-03-11",
    "abstract": "Meta-learning models, or models that learn to learn, have been a long-desired target for their ability to quickly solve new tasks. Traditional meta-learning methods can require expensive inner and outer loops, thus there is demand for algorithms that discover strong learners without explicitly searching for them. We draw parallels to the study of evolvable genomes in evolutionary systems -- genomes with a strong capacity to adapt -- and propose that meta-learning and adaptive evolvability optimize for the same objective: high performance after a set of learning iterations. We argue that population-based evolutionary systems with non-static fitness landscapes naturally bias towards high-evolvability genomes, and therefore optimize for populations with strong learning ability. We demonstrate this claim with a simple evolutionary algorithm, Population-Based Meta Learning (PBML), that consistently discovers genomes which display higher rates of improvement over generations, and can rapidly adapt to solve sparse fitness and robotic control tasks.",
    "citation_count": 4,
    "summary": "The paper introduces Population-Based Meta-Learning (PBML), an evolutionary algorithm that optimizes a meta-learning objective by evolving populations of models, implicitly favoring those with high learning capacity and rapid adaptation to new tasks. PBML avoids expensive inner/outer loops of traditional meta-learning methods by leveraging the natural bias of evolutionary systems towards high evolvability."
  }
]