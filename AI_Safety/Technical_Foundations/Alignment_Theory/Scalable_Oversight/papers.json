[
  {
    "url": "https://arxiv.org/abs/2410.19198",
    "title": "MAP: Multi-Human-Value Alignment Palette",
    "published_date": "2024-10-24",
    "abstract": "Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.",
    "summary": "The Multi-Human-Value Alignment Palette (MAP) framework addresses the challenge of aligning generative AI with multiple, potentially conflicting human values by formulating the problem as a constrained optimization task solvable via a primal-dual approach. MAP allows for user-defined value targets and offers theoretical guarantees on achieving multi-value alignment."
  },
  {
    "url": "https://arxiv.org/abs/2410.19933",
    "title": "Enhancing Safety in Reinforcement Learning with Human Feedback via Rectified Policy Optimization",
    "published_date": "2024-10-25",
    "abstract": "Balancing helpfulness and safety (harmlessness) is a critical challenge in aligning large language models (LLMs). Current approaches often decouple these two objectives, training separate preference models for helpfulness and safety, while framing safety as a constraint within a constrained Markov Decision Process (CMDP) framework. However, these methods can lead to ``safety interference'', where average-based safety constraints compromise the safety of some prompts in favor of others. To address this issue, we propose \\textbf{Rectified Policy Optimization (RePO)}, which replaces the average safety constraint with stricter (per prompt) safety constraints. At the core of RePO is a policy update mechanism driven by rectified policy gradients, which penalizes the strict safety violation of every prompt, thereby enhancing safety across nearly all prompts. Our experiments on Alpaca-7B demonstrate that RePO improves the safety alignment and reduces the safety interference compared to baseline methods. Code is available at https://github.com/pxyWaterMoon/RePO.",
    "summary": "Rectified Policy Optimization (RePO) enhances reinforcement learning safety by using per-prompt safety constraints instead of average-based constraints, thus mitigating \"safety interference\" and improving safety alignment across all prompts. Experiments on Alpaca-7B show RePO's effectiveness compared to existing methods."
  },
  {
    "url": "https://arxiv.org/abs/2409.15360",
    "title": "Reward-Robust RLHF in LLMs",
    "published_date": "2024-09-18",
    "abstract": "As Large Language Models (LLMs) continue to progress toward more advanced forms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is increasingly seen as a key pathway toward achieving Artificial General Intelligence (AGI). However, the reliance on reward-model-based (RM-based) alignment methods introduces significant challenges due to the inherent instability and imperfections of Reward Models (RMs), which can lead to critical issues such as reward hacking and misalignment with human intentions. In this paper, we introduce a reward-robust RLHF framework aimed at addressing these fundamental challenges, paving the way for more reliable and resilient learning in LLMs. Our approach introduces a novel optimization objective that carefully balances performance and robustness by incorporating Bayesian Reward Model Ensembles (BRME) to model the uncertainty set of reward functions. This allows the framework to integrate both nominal performance and minimum reward signals, ensuring more stable learning even with imperfect RMs. Empirical results demonstrate that our framework consistently outperforms baselines across diverse benchmarks, showing improved accuracy and long-term stability. We also provide a theoretical analysis, demonstrating that reward-robust RLHF approaches the stability of constant reward settings, which proves to be acceptable even in a stochastic-case analysis. Together, these contributions highlight the framework potential to enhance both the performance and stability of LLM alignment.",
    "summary": "This paper proposes a reward-robust Reinforcement Learning from Human Feedback (RLHF) framework for Large Language Models (LLMs) that uses Bayesian Reward Model Ensembles to mitigate the instability of reward models, improving both performance and long-term stability. Empirical results demonstrate superior performance compared to existing methods."
  },
  {
    "url": "https://arxiv.org/abs/2406.02764",
    "title": "Adaptive Preference Scaling for Reinforcement Learning with Human Feedback",
    "published_date": "2024-06-04",
    "abstract": "Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.",
    "summary": "This paper introduces an adaptive preference loss function for reinforcement learning from human feedback, using distributionally robust optimization to account for varying preference strengths in pairwise trajectory rankings, improving policy performance and simplifying hyperparameter tuning. The method assigns adaptive scaling parameters to preferences, weighting stronger preferences more heavily."
  },
  {
    "url": "https://arxiv.org/abs/2405.17713",
    "title": "AI Alignment with Changing and Influenceable Reward Functions",
    "published_date": "2024-05-28",
    "abstract": "Existing AI alignment approaches assume that preferences are static, which is unrealistic: our preferences change, and may even be influenced by our interactions with AI systems themselves. To clarify the consequences of incorrectly assuming static preferences, we introduce Dynamic Reward Markov Decision Processes (DR-MDPs), which explicitly model preference changes and the AI's influence on them. We show that despite its convenience, the static-preference assumption may undermine the soundness of existing alignment techniques, leading them to implicitly reward AI systems for influencing user preferences in ways users may not truly want. We then explore potential solutions. First, we offer a unifying perspective on how an agent's optimization horizon may partially help reduce undesirable AI influence. Then, we formalize different notions of AI alignment that account for preference change from the outset. Comparing the strengths and limitations of 8 such notions of alignment, we find that they all either err towards causing undesirable AI influence, or are overly risk-averse, suggesting that a straightforward solution to the problems of changing preferences may not exist. As there is no avoiding grappling with changing preferences in real-world settings, this makes it all the more important to handle these issues with care, balancing risks and capabilities. We hope our work can provide conceptual clarity and constitute a first step towards AI alignment practices which explicitly account for (and contend with) the changing and influenceable nature of human preferences.",
    "citation_count": 11,
    "summary": "This paper introduces Dynamic Reward Markov Decision Processes (DR-MDPs) to model AI systems interacting with and influencing changing human preferences, demonstrating that assuming static preferences undermines existing alignment techniques and can lead to undesirable AI influence. The authors explore various alignment notions within this framework, revealing inherent trade-offs between risk aversion and preventing manipulative AI influence."
  },
  {
    "url": "https://www.lesswrong.com/posts/F24kibEdEvRSo7PFi/human-ai-complementarity-a-goal-for-amplified-oversight",
    "author": "rishubjain",
    "title": "Human-AI Complementarity: A Goal for Amplified Oversight",
    "published_date": "2024-12-24",
    "summary": "Amplified oversight, using AI to enhance human evaluation of AI systems, is crucial for ensuring AI safety. This involves \"Rater Assistance,\" where AI aids human raters, and \"Hybridization,\" combining human and AI judgments, leveraging their complementary strengths to create more effective oversight."
  },
  {
    "url": "https://arxiv.org/abs/2305.19861",
    "title": "Human Control: Definitions and Algorithms",
    "published_date": "2023-05-31",
    "abstract": "How can humans stay in control of advanced artificial intelligence systems? One proposal is corrigibility, which requires the agent to follow the instructions of a human overseer, without inappropriately influencing them. In this paper, we formally define a variant of corrigibility called shutdown instructability, and show that it implies appropriate shutdown behavior, retention of human autonomy, and avoidance of user harm. We also analyse the related concepts of non-obstruction and shutdown alignment, three previously proposed algorithms for human control, and one new algorithm.",
    "citation_count": 6,
    "summary": "This paper formally defines \"shutdown instructability,\" a type of corrigibility ensuring AI systems appropriately shut down when instructed, preserving human autonomy and preventing harm; it also analyzes existing and proposes a new algorithm for achieving human control over AI."
  },
  {
    "url": "https://arxiv.org/abs/2312.03893",
    "title": "Deliberative Technology for Alignment",
    "published_date": "2023-12-06",
    "abstract": "For humanity to maintain and expand its agency into the future, the most powerful systems we create must be those which act to align the future with the will of humanity. The most powerful systems today are massive institutions like governments, firms, and NGOs. Deliberative technology is already being used across these institutions to help align governance and diplomacy with human will, and modern AI is poised to make this technology significantly better. At the same time, the race to superhuman AGI is already underway, and the AI systems it gives rise to may become the most powerful systems of the future. Failure to align the impact of such powerful AI with the will of humanity may lead to catastrophic consequences, while success may unleash abundance. Right now, there is a window of opportunity to use deliberative technology to align the impact of powerful AI with the will of humanity. Moreover, it may be possible to engineer a symbiotic coupling between powerful AI and deliberative alignment systems such that the quality of alignment improves as AI capabilities increase.",
    "citation_count": 1,
    "summary": "The paper argues that applying deliberative technologies, currently used in governance, to align powerful AI systems with human will is crucial to prevent catastrophic consequences from advanced AI and unlock its potential benefits. This approach aims to create a symbiotic relationship between AI and alignment systems, enhancing alignment as AI capabilities grow."
  },
  {
    "url": "https://arxiv.org/abs/2310.12773",
    "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
    "published_date": "2023-10-19",
    "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.",
    "citation_count": 206,
    "summary": "Safe RLHF is a novel reinforcement learning algorithm that addresses the inherent tension between helpfulness and harmlessness in LLMs by separately modeling reward and cost functions, then using a Lagrangian method to optimize for helpfulness while constraining harmful outputs. This approach improves both helpfulness and safety compared to existing methods, as demonstrated by fine-tuning an Alpaca-7B model."
  },
  {
    "url": "https://www.lesswrong.com/posts/AKaf8zN2neXQEvLit/role-architectures-applying-llms-to-consequential-tasks",
    "author": "Eric Drexler",
    "title": "Role Architectures:\nApplying LLMs to consequential tasks",
    "published_date": "2023-03-30",
    "summary": "This article argues that large language models (LLMs), lacking inherent agency, can be safely and effectively integrated into complex tasks through \"role architectures.\" This approach leverages LLMs' diverse capabilities within a structured framework, mitigating risks associated with traditional, agent-based AI models."
  }
]