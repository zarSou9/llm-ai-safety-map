[
  {
    "url": "https://arxiv.org/abs/2410.04503",
    "title": "LRHP: Learning Representations for Human Preferences via Preference Pairs",
    "published_date": "2024-10-06",
    "abstract": "To improve human-preference alignment training, current research has developed numerous preference datasets consisting of preference pairs labeled as\"preferred\"or\"dispreferred\". These preference pairs are typically used to encode human preferences into a single numerical value through reward modeling, which acts as a reward signal during reinforcement learning from human feedback (RLHF). However, representing these human preferences as a numerical value complicates the analysis of these preferences and restricts their broader applications other than RLHF. In contrast, in this work, we introduce a preference representation learning task that aims to construct a richer and more structured representation of human preferences. We further develop a more generalizable framework, Learning Representations for Human Preferences via preference pairs (namely LRHP), which extends beyond traditional reward modeling to tackle this task. We verify the utility of preference representations in two downstream tasks: preference data selection and preference margin prediction. Building upon the human preferences in representations, we achieve strong performance in both tasks, significantly outperforming baselines.",
    "summary": "LRHP is a novel framework that learns richer, more structured representations of human preferences from preference pairs, moving beyond simple numerical reward modeling and enabling applications like preference data selection and margin prediction. This approach significantly improves performance on these downstream tasks compared to existing methods."
  },
  {
    "url": "https://arxiv.org/abs/2401.04056",
    "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback",
    "published_date": "2024-01-08",
    "abstract": "We present Self-Play Preference Optimization (SPO), an algorithm for reinforcement learning from human feedback. Our approach is minimalist in that it does not require training a reward model nor unstable adversarial training and is therefore rather simple to implement. Our approach is maximalist in that it provably handles non-Markovian, intransitive, and stochastic preferences while being robust to the compounding errors that plague offline approaches to sequential prediction. To achieve the preceding qualities, we build upon the concept of a Minimax Winner (MW), a notion of preference aggregation from the social choice theory literature that frames learning from preferences as a zero-sum game between two policies. By leveraging the symmetry of this game, we prove that rather than using the traditional technique of dueling two policies to compute the MW, we can simply have a single agent play against itself while maintaining strong convergence guarantees. Practically, this corresponds to sampling multiple trajectories from a policy, asking a preference or teacher model to compare them, and then using the proportion of wins as the reward for a particular trajectory. We demonstrate that on a suite of continuous control tasks, we are able to learn significantly more efficiently than reward-model based approaches while maintaining robustness to the intransitive and stochastic preferences that frequently occur in practice when aggregating human judgments.",
    "citation_count": 62,
    "summary": "Self-Play Preference Optimization (SPO) is a minimalist reinforcement learning algorithm using human feedback that avoids reward model training and adversarial training, achieving robustness to noisy preferences by framing preference aggregation as a zero-sum game between self-playing policies. This approach provably handles complex preferences and avoids compounding errors common in offline methods."
  },
  {
    "url": "https://arxiv.org/pdf/2111.03026.pdf",
    "title": "B-Pref: Benchmarking Preference-Based Reinforcement Learning",
    "published_date": "2021-11-04",
    "abstract": "Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.",
    "citation_count": 82,
    "summary": "B-Pref is a new benchmark for preference-based reinforcement learning that simulates human preference data with various irrationalities, enabling efficient evaluation of algorithms and their robustness. This benchmark facilitates systematic study of preference-based RL by providing standardized metrics and allowing analysis of design choices like informative query selection."
  },
  {
    "url": "https://arxiv.org/abs/2305.15363",
    "title": "Inverse Preference Learning: Preference-based RL without a Reward Function",
    "published_date": "2023-05-24",
    "abstract": "Reward functions are difficult to design and often hard to align with human intent. Preference-based Reinforcement Learning (RL) algorithms address these problems by learning reward functions from human feedback. However, the majority of preference-based RL methods na\\\"ively combine supervised reward models with off-the-shelf RL algorithms. Contemporary approaches have sought to improve performance and query complexity by using larger and more complex reward architectures such as transformers. Instead of using highly complex architectures, we develop a new and parameter-efficient algorithm, Inverse Preference Learning (IPL), specifically designed for learning from offline preference data. Our key insight is that for a fixed policy, the $Q$-function encodes all information about the reward function, effectively making them interchangeable. Using this insight, we completely eliminate the need for a learned reward function. Our resulting algorithm is simpler and more parameter-efficient. Across a suite of continuous control and robotics benchmarks, IPL attains competitive performance compared to more complex approaches that leverage transformer-based and non-Markovian reward functions while having fewer algorithmic hyperparameters and learned network parameters. Our code is publicly released.",
    "citation_count": 36,
    "summary": "Inverse Preference Learning (IPL) avoids the need for a learned reward function in reinforcement learning by directly leveraging the Q-function to infer preferences from offline data, resulting in a simpler, more parameter-efficient algorithm that achieves competitive performance compared to more complex methods. IPL's key innovation is its ability to bypass explicit reward model learning, relying instead on the inherent reward information within the Q-function for a given policy."
  },
  {
    "url": "https://www.alignmentforum.org/s/dT7CKGXwq9vt76CeX/p/7bNXqdDPYpnfCNQhA",
    "author": "Rohin Shah",
    "title": "[AN #89]: A unifying formalism for preference learning algorithms",
    "published_date": "2020-03-04",
    "summary": "This Alignment Newsletter discusses two papers on AI alignment. The first proposes a unifying formalism for reward learning from various human feedback types, while the second explores using goal-conditioned imitation learning to improve performance by leveraging expert demonstrations."
  },
  {
    "url": "https://arxiv.org/abs/2409.17534",
    "title": "Just say what you want: only-prompting self-rewarding online preference optimization",
    "published_date": "2024-09-26",
    "abstract": "We address the challenge of online Reinforcement Learning from Human Feedback (RLHF) with a focus on self-rewarding alignment methods. In online RLHF, obtaining feedback requires interaction with the environment, which can be costly when using additional reward models or the GPT-4 API. Current self-rewarding approaches rely heavily on the discriminator's judgment capabilities, which are effective for large-scale models but challenging to transfer to smaller ones. To address these limitations, we propose a novel, only-prompting self-rewarding online algorithm that generates preference datasets without relying on judgment capabilities. Additionally, we employ fine-grained arithmetic control over the optimality gap between positive and negative examples, generating more hard negatives in the later stages of training to help the model better capture subtle human preferences. Finally, we conduct extensive experiments on two base models, Mistral-7B and Mistral-Instruct-7B, which significantly bootstrap the performance of the reference model, achieving 34.5% in the Length-controlled Win Rates of AlpacaEval 2.0.",
    "citation_count": 1,
    "summary": "This paper introduces a novel online reinforcement learning from human feedback (RLHF) method that uses only prompts to generate self-rewards, avoiding costly external reward models. The method improves model performance by dynamically controlling the difficulty of negative examples, achieving significant gains on benchmark tasks."
  },
  {
    "url": "https://arxiv.org/abs/2410.05328",
    "title": "Reward Learning From Preference With Ties",
    "published_date": "2024-10-05",
    "abstract": "Reward learning plays a pivotal role in Reinforcement Learning from Human Feedback (RLHF), ensuring the alignment of language models. The Bradley-Terry (BT) model stands as the prevalent choice for capturing human preferences from datasets containing pairs of chosen and rejected responses. In preference modeling, the focus is not on absolute values but rather on the reward difference between chosen and rejected responses, referred to as preference strength. Thus, precise evaluation of preference strength holds paramount importance in preference modeling. However, an easily overlooked factor significantly affecting preference strength measurement is that human attitudes towards two responses may not solely indicate a preference for one over the other and ties are also a common occurrence. To address this, we propose the adoption of the generalized Bradley-Terry model -- the Bradley-Terry model with ties (BTT) -- to accommodate tied preferences, thus leveraging additional information. We prove that even with the access to the true distributions of prompt and response, disregarding ties can lead to a notable bias in preference strength measurement. Comprehensive experiments further validate the advantages of incorporating ties in preference modeling. Notably, fine-tuning with BTT significantly outperforms fine-tuning with BT on synthetic preference datasets with ties, labeled by state-of-the-art open-source LLMs.",
    "citation_count": 3,
    "summary": "This paper introduces a Bradley-Terry model with ties (BTT) for reward learning from human preferences, addressing the common issue of tied preferences which the standard Bradley-Terry model ignores. The authors demonstrate that incorporating ties via BTT leads to improved preference strength measurement and consequently, superior reinforcement learning from human feedback."
  },
  {
    "url": "https://arxiv.org/abs/2410.04166",
    "title": "Preference Optimization as Probabilistic Inference",
    "published_date": "2024-10-05",
    "abstract": "Existing preference optimization methods are mainly designed for directly learning from human feedback with the assumption that paired examples (preferred vs. dis-preferred) are available. In contrast, we propose a method that can leverage unpaired preferred or dis-preferred examples, and works even when only one type of feedback (positive or negative) is available. This flexibility allows us to apply it in scenarios with varying forms of feedback and models, including training generative language models based on human feedback as well as training policies for sequential decision-making problems, where learned (value) functions are available. Our approach builds upon the probabilistic framework introduced in (Dayan and Hinton, 1997), which proposes to use expectation-maximization (EM) to directly optimize the probability of preferred outcomes (as opposed to classic expected reward maximization). To obtain a practical algorithm, we identify and address a key limitation in current EM-based methods: when applied to preference optimization, they solely maximize the likelihood of preferred examples, while neglecting dis-preferred samples. We show how one can extend EM algorithms to explicitly incorporate dis-preferred outcomes, leading to a novel, theoretically grounded, preference optimization algorithm that offers an intuitive and versatile way to learn from both positive and negative feedback.",
    "summary": "This paper introduces a novel preference optimization method using a probabilistic framework and expectation-maximization (EM) to learn from both paired and unpaired, positive and negative feedback, overcoming limitations of existing methods that only utilize paired examples. This flexible approach is applicable to diverse scenarios, including generative language models and reinforcement learning."
  }
]