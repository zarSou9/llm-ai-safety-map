[
  {
    "url": "https://arxiv.org/abs/2409.04897",
    "title": "Centralized Selection with Preferences in the Presence of Biases",
    "published_date": "2024-09-07",
    "abstract": "This paper considers the scenario in which there are multiple institutions, each with a limited capacity for candidates, and candidates, each with preferences over the institutions. A central entity evaluates the utility of each candidate to the institutions, and the goal is to select candidates for each institution in a way that maximizes utility while also considering the candidates' preferences. The paper focuses on the setting in which candidates are divided into multiple groups and the observed utilities of candidates in some groups are biased--systematically lower than their true utilities. The first result is that, in these biased settings, prior algorithms can lead to selections with sub-optimal true utility and significant discrepancies in the fraction of candidates from each group that get their preferred choices. Subsequently, an algorithm is presented along with proof that it produces selections that achieve near-optimal group fairness with respect to preferences while also nearly maximizing the true utility under distributional assumptions. Further, extensive empirical validation of these results in real-world and synthetic settings, in which the distributional assumptions may not hold, are presented."
  },
  {
    "url": "https://arxiv.org/abs/2403.06003",
    "title": "A Generalized Acquisition Function for Preference-based Reward Learning",
    "published_date": "2024-03-09",
    "abstract": "Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic environment, an assistive robotics environment with domain transfer, and a natural language processing problem with real datasets demonstrate the superior performance of our querying method over the state-of-the-art information gain method.",
    "citation_count": 1
  },
  {
    "url": "https://www.lesswrong.com/posts/Z9P2m462wQ4qmH6uo/aspiration-based-q-learning",
    "author": "Cl√©ment Dumas, Jobst Heitzig",
    "title": "Aspiration-based Q-Learning",
    "published_date": "2023-10-27"
  },
  {
    "url": "https://arxiv.org/abs/2210.13839",
    "title": "Preference-Learning Emitters for Mixed-Initiative Quality-Diversity Algorithms",
    "published_date": "2022-10-25",
    "abstract": "In mixed-initiative cocreation tasks, wherein a human and a machine jointly create items, it is important to provide multiple relevant suggestions to the designer. Quality-diversity algorithms are commonly used for this purpose, as they can provide diverse suggestions that represent salient areas of the solution space, showcasing designs with high fitness and wide variety. Because generated suggestions drive the search process, it is important that they provide inspiration, but also stay aligned with the designer's intentions. Additionally, often many interactions with the system are required before the designer is content with a solution. In this article, we tackle these challenges with an interactive constrained multidimensional archive of phenotypic-elites system that leverages emitters to learn the preferences of the designer, and then, use them in automated steps. By learning preferences, the generated designs remain aligned with the designer's intent, and by applying automatic steps, we generate more solutions per user interaction, giving a larger number of choices to the designer, and thereby, speeding up the search. We propose a general framework for preference-learning emitters and apply it to a procedural content generation task in the video game Space Engineers. We built an interactive application for our algorithm and performed a user study with players.",
    "citation_count": 4
  },
  {
    "url": "https://arxiv.org/pdf/2103.13192v2.pdf",
    "title": "On Preference Learning Based on Sequential Bayesian Optimization with Pairwise Comparison",
    "published_date": "2021-03-24",
    "abstract": "User preference learning is generally a hard problem. Individual preferences are typically unknown even to users themselves, while the space of choices is infinite. Here we study user preference learning from information-theoretic perspective. We model preference learning as a system with two interacting sub-systems, one representing a user with his/her preferences and another one representing an agent that has to learn these preferences. The user with his/her behaviour is modeled by a parametric preference function. To efficiently learn the preferences and reduce search space quickly, we propose the agent that interacts with the user to collect the most informative data for learning. The agent presents two proposals to the user for evaluation, and the user rates them based on his/her preference function. We show that the optimum agent strategy for data collection and preference learning is a result of maximin optimization of the normalized weighted Kullback-Leibler (KL) divergence between true and agent-assigned predictive user response distributions. The resulting value of KL-divergence, which we also call remaining system uncertainty (RSU), provides an efficient performance metric in the absence of the ground truth. This metric characterises how well the agent can predict user and, thus, the quality of the underlying learned user (preference) model. Our proposed agent comprises sequential mechanisms for user model inference and proposal generation. To infer the user model (preference function), Bayesian approximate inference is used in the agent. The data collection strategy is to generate proposals, responses to which help resolving uncertainty associated with prediction of the user responses the most. The efficiency of our approach is validated by numerical simulations. Also a real-life example of preference learning application is provided.",
    "citation_count": 4
  },
  {
    "url": "https://arxiv.org/pdf/2106.12621v1.pdf",
    "title": "Leveraging semantically similar queries for ranking via combining representations",
    "published_date": "2021-06-23",
    "abstract": "In modern ranking problems, different and disparate representations of the items to be ranked are often available. It is sensible, then, to try to combine these representations to improve ranking. Indeed, learning to rank via combining representations is both principled and practical for learning a ranking function for a particular query. In extremely data-scarce settings, however, the amount of labeled data available for a particular query can lead to a highly variable and ineffective ranking function. One way to mitigate the effect of the small amount of data is to leverage information from semantically similar queries. Indeed, as we demonstrate in simulation settings and real data examples, when semantically similar queries are available it is possible to gainfully use them when ranking with respect to a particular query. We describe and explore this phenomenon in the context of the bias-variance trade off and apply it to the data-scarce settings of a Bing navigational graph and the Drosophila larva connectome.",
    "citation_count": 1
  }
]