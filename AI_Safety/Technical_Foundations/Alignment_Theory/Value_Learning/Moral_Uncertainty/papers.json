[
  {
    "url": "https://arxiv.org/pdf/2006.04734.pdf",
    "title": "Reinforcement Learning Under Moral Uncertainty",
    "published_date": "2020-06-08",
    "abstract": "An ambitious goal for artificial intelligence is to create agents that behave ethically: The capacity to abide by human moral norms would greatly expand the context in which autonomous agents could be practically and safely deployed. While ethical agents could be trained through reinforcement, by rewarding correct behavior under a specific moral theory (e.g. utilitarianism), there remains widespread disagreement (both societally and among moral philosophers) about the nature of morality and what ethical theory (if any) is objectively correct. Acknowledging such disagreement, recent work in moral philosophy proposes that ethical behavior requires acting under moral uncertainty, i.e. to take into account when acting that one's credence is split across several plausible ethical theories. Inspired by such work, this paper proposes a formalism that translates such insights to the field of reinforcement learning. Demonstrating the formalism's potential, we then train agents in simple environments to act under moral uncertainty, highlighting how such uncertainty can help curb extreme behavior from commitment to single theories. The overall aim is to draw productive connections from the fields of moral philosophy and machine ethics to that of machine learning, to inspire further research by highlighting a spectrum of machine learning research questions relevant to training ethically capable reinforcement learning agents.",
    "citation_count": 27,
    "summary": "This paper introduces a reinforcement learning formalism for training ethical agents under moral uncertainty, addressing societal and philosophical disagreements about morality by incorporating multiple ethical theories into the agent's decision-making process. This approach aims to mitigate extreme behaviors that might arise from adherence to a single ethical framework."
  },
  {
    "url": "https://www.lesswrong.com/posts/dX7vNKg4vex5vxWCW/making-decisions-under-moral-uncertainty-1",
    "author": "MichaelA",
    "title": "Making decisions under moral uncertainty",
    "published_date": "2019-12-30",
    "summary": "This article series explores decision-making under moral uncertainty, addressing the lack of readily available resources on this topic. It aims to synthesize existing philosophical ideas and potentially introduce novel approaches to handling both moral and empirical uncertainties in decision-making."
  },
  {
    "url": "https://www.lesswrong.com/tag/moral-uncertainty",
    "author": "abramdemski",
    "title": "Moral Uncertainty - LessWrong",
    "published_date": "2020-11-18",
    "summary": "Moral uncertainty arises from disagreements about which moral theory is correct, making decision-making difficult even with complete information. While calculating expected moral value is one approach, resolving inter-theory value comparisons and applying this to non-consequentialist theories remains a challenge."
  },
  {
    "url": "https://www.alignmentforum.org/tag/moral-uncertainty",
    "author": "Abram Demski",
    "title": "Moral Uncertainty - AI Alignment Forum",
    "published_date": "2020-11-18",
    "summary": "Moral uncertainty arises from the lack of consensus on which moral theory is correct, complicating decision-making even with complete information. Addressing this uncertainty involves strategies like maximizing expected moral value or employing a parliamentary model that weighs different theories' probabilities, though challenges remain in comparing values across diverse ethical frameworks."
  },
  {
    "url": "https://www.lesswrong.com/posts/eYiDjCNJrR3w3WcMM/making-decisions-when-both-morally-and-empirically-uncertain",
    "author": "MichaelA",
    "title": "Making decisions when both morally and empirically uncertain",
    "published_date": "2020-01-02",
    "summary": "The article proposes a method for making decisions under both moral and empirical uncertainty, extending existing approaches like Maximising Expected Choice-worthiness (MEC) by applying them to outcomes, weighted by the likelihood of each action leading to those outcomes. This integrates considerations of both moral theory probabilities and the probabilities of different action consequences."
  },
  {
    "url": "https://arxiv.org/abs/2409.15014",
    "title": "Acting for the Right Reasons: Creating Reason-Sensitive Artificial Moral Agents",
    "published_date": "2024-09-23",
    "abstract": "We propose an extension of the reinforcement learning architecture that enables moral decision-making of reinforcement learning agents based on normative reasons. Central to this approach is a reason-based shield generator yielding a moral shield that binds the agent to actions that conform with recognized normative reasons so that our overall architecture restricts the agent to actions that are (internally) morally justified. In addition, we describe an algorithm that allows to iteratively improve the reason-based shield generator through case-based feedback from a moral judge.",
    "summary": "This paper presents a reinforcement learning architecture that incorporates normative reasons into moral decision-making, creating agents whose actions are justified by internal moral reasoning. The architecture includes a \"reason-based shield\" refined iteratively through feedback from a moral judge."
  },
  {
    "url": "https://arxiv.org/abs/2410.19198",
    "title": "MAP: Multi-Human-Value Alignment Palette",
    "published_date": "2024-10-24",
    "abstract": "Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.",
    "summary": "The Multi-Human-Value Alignment Palette (MAP) framework addresses the challenge of aligning generative AI with multiple, potentially conflicting human values by formulating the problem as a constrained optimization task. MAP uses a primal-dual approach to determine achievability and optimize alignment across various values, offering both theoretical analysis and strong empirical performance."
  },
  {
    "url": "https://arxiv.org/abs/2301.08491v1",
    "title": "Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning",
    "published_date": "2023-01-20",
    "abstract": "Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.\n\n\n\nIn this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm-based agents, between morality based on societal norms or internal virtues, and between single- and mixed-virtue (e.g., multi-objective) methodologies. Then, we evaluate our approach by modeling repeated dyadic interactions between learning moral agents in three iterated social dilemma games (Prisoner's Dilemma, Volunteer's Dilemma and Stag Hunt). We analyze the impact of different types of morality on the emergence of cooperation, defection or exploitation, and the corresponding social outcomes. Finally, we discuss the implications of these findings for the development of moral agents in artificial and mixed human-AI societies.",
    "citation_count": 15,
    "summary": "This paper investigates the emergence of moral behavior in multi-agent reinforcement learning by modeling agents with reward functions based on different ethical theories (consequence-based, norm-based, single/mixed-virtue) within classic social dilemma games. The authors analyze how these varying moral frameworks influence cooperation, defection, and overall social outcomes."
  }
]