### Mini Description

Approaches for handling uncertainty about moral principles and value judgments, including frameworks for moral learning and reasoning under ethical uncertainty.

### Description

Moral uncertainty in AI value learning addresses the fundamental challenge of how AI systems should reason and act when faced with uncertainty about moral principles, ethical frameworks, and value judgments. This uncertainty stems from both philosophical disagreements about ethics and practical challenges in translating ethical principles into computational frameworks. The field explores methods for representing and reasoning about different moral theories, developing decision procedures under moral uncertainty, and creating AI systems that can appropriately handle ethical ambiguity.

A key focus is the development of meta-ethical frameworks that can integrate multiple moral theories while acknowledging their varying degrees of credence. This includes approaches like moral parliament models, expected moral value maximization, and various forms of moral hedging. Researchers investigate how to weight different ethical considerations, handle incommensurable values, and make decisions when different moral frameworks suggest conflicting actions.

Current research emphasizes the importance of maintaining appropriate uncertainty and humility in moral reasoning. This includes work on identifying moral blind spots, detecting when moral uncertainty should trigger deferrals to human judgment, and developing frameworks for updating moral beliefs based on new information or reasoning. Key challenges include handling deep uncertainty about fundamental moral questions, managing the interaction between moral uncertainty and empirical uncertainty, and ensuring that uncertainty handling mechanisms remain robust as AI systems become more capable.

### Order

1. Meta-ethical_Frameworks
2. Decision_Procedures
3. Uncertainty_Quantification
4. Learning_and_Updating
5. Deferrals_and_Safeguards
