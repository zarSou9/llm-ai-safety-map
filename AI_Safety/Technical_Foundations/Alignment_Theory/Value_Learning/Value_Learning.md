### Mini Description

Frameworks and methods for learning and representing human values and preferences, including approaches for handling uncertainty, conflicting values, and edge cases.

### Description

Value Learning focuses on developing computational methods to learn, represent, and operationalize human values and preferences in AI systems. This encompasses both the technical challenge of extracting coherent value representations from human feedback and behavior, and the theoretical challenge of handling the complexity, uncertainty, and context-dependency inherent in human values. Core approaches include inverse reinforcement learning from demonstrations, preference learning from comparisons, and various forms of human feedback mechanisms.

A central challenge is the multi-agent nature of value learning, as human values often emerge from and depend on social interactions and cultural contexts. This requires frameworks that can integrate multiple perspectives, handle conflicting preferences, and account for the dynamic, evolving nature of both individual and collective values. Researchers explore methods for aggregating preferences across different stakeholders, learning meta-preferences about how to handle value conflicts, and developing robust approaches to value uncertainty.

Current research emphasizes the importance of learning not just explicit preferences, but also implicit values, moral principles, and higher-order preferences about how preferences should be formed and updated. This includes work on learning from sparse or weak supervision, inferring values from complex human behavior, and developing frameworks that can capture nuanced aspects of human judgment such as moral uncertainty, value reflection, and preference change over time. Key open questions include how to handle edge cases and novel situations, how to maintain value alignment under distribution shift, and how to develop scalable approaches that remain reliable as AI systems become more capable.

### Order

1. Preference_Learning_Mechanisms
2. Value_Representation
3. Multi-stakeholder_Integration
4. Moral_Uncertainty
5. Value_Dynamics
