[
  {
    "url": "http://arxiv.org/abs/2312.14106",
    "title": "Learning Human-like Representations to Enable Learning Human Values",
    "published_date": "2023-12-21",
    "abstract": "How can we build AI systems that can learn any set of individual human values both quickly and safely, avoiding causing harm or violating societal standards for acceptable behavior during the learning process? We explore the effects of representational alignment between humans and AI agents on learning human values. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We demonstrate that this kind of representational alignment can also support safely learning and exploring human values in the context of personalization. We begin with a theoretical prediction, show that it applies to learning human morality judgments, then show that our results generalize to ten different aspects of human values -- including ethics, honesty, and fairness -- training AI agents on each set of values in a multi-armed bandit setting, where rewards reflect human value judgments over the chosen action. Using a set of textual action descriptions, we collect value judgments from humans, as well as similarity judgments from both humans and multiple language models, and demonstrate that representational alignment enables both safe exploration and improved generalization when learning human values.",
    "citation_count": 3,
    "summary": "This paper investigates using human-like representations in AI to safely and efficiently learn diverse human values, demonstrating that representational alignment improves both the safety of the learning process and the generalization of learned values across various contexts. This is achieved by training AI agents on human value judgments in a multi-armed bandit setting, showing improved performance compared to agents without representational alignment."
  },
  {
    "url": "https://arxiv.org/abs/2410.19198",
    "title": "MAP: Multi-Human-Value Alignment Palette",
    "published_date": "2024-10-24",
    "abstract": "Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.",
    "summary": "The Multi-Human-Value Alignment Palette (MAP) framework addresses the challenge of aligning generative AI with multiple, potentially conflicting human values by formulating alignment as a constrained optimization problem. MAP uses a primal-dual approach to determine achievable alignment targets and efficiently achieve them, demonstrating strong empirical performance across various tasks."
  },
  {
    "url": "https://arxiv.org/abs/2410.04503",
    "title": "LRHP: Learning Representations for Human Preferences via Preference Pairs",
    "published_date": "2024-10-06",
    "abstract": "To improve human-preference alignment training, current research has developed numerous preference datasets consisting of preference pairs labeled as\"preferred\"or\"dispreferred\". These preference pairs are typically used to encode human preferences into a single numerical value through reward modeling, which acts as a reward signal during reinforcement learning from human feedback (RLHF). However, representing these human preferences as a numerical value complicates the analysis of these preferences and restricts their broader applications other than RLHF. In contrast, in this work, we introduce a preference representation learning task that aims to construct a richer and more structured representation of human preferences. We further develop a more generalizable framework, Learning Representations for Human Preferences via preference pairs (namely LRHP), which extends beyond traditional reward modeling to tackle this task. We verify the utility of preference representations in two downstream tasks: preference data selection and preference margin prediction. Building upon the human preferences in representations, we achieve strong performance in both tasks, significantly outperforming baselines.",
    "summary": "LRHP is a novel framework that learns richer, structured representations of human preferences from preference pairs, moving beyond simple numerical reward modeling and improving performance on downstream tasks like preference data selection and margin prediction. This approach allows for broader application of human preference data than traditional reinforcement learning from human feedback."
  },
  {
    "url": "https://arxiv.org/abs/2406.02764",
    "title": "Adaptive Preference Scaling for Reinforcement Learning with Human Feedback",
    "published_date": "2024-06-04",
    "abstract": "Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.",
    "summary": "This paper introduces an adaptive preference loss function for reinforcement learning from human feedback, using distributionally robust optimization to account for varying preference strengths in pairwise trajectory rankings, thereby improving policy performance and simplifying hyperparameter tuning. The method achieves this by assigning adaptive scaling parameters to each preference pair, weighting stronger preferences more heavily."
  },
  {
    "url": "https://arxiv.org/pdf/2302.08759.pdf",
    "title": "Value Engineering for Autonomous Agents",
    "published_date": "2023-02-17",
    "abstract": "Machine Ethics (ME) is concerned with the design of Artificial Moral Agents (AMAs), i.e. autonomous agents capable of reasoning and behaving according to moral values. Previous approaches have treated values as labels associated with some actions or states of the world, rather than as integral components of agent reasoning. It is also common to disregard that a value-guided agent operates alongside other value-guided agents in an environment governed by norms, thus omitting the social dimension of AMAs. In this blue sky paper, we propose a new AMA paradigm grounded in moral and social psychology, where values are instilled into agents as context-dependent goals. These goals intricately connect values at individual levels to norms at a collective level by evaluating the outcomes most incentivized by the norms in place. We argue that this type of normative reasoning, where agents are endowed with an understanding of norms' moral implications, leads to value-awareness in autonomous agents. Additionally, this capability paves the way for agents to align the norms enforced in their societies with respect to the human values instilled in them, by complementing the value-based reasoning on norms with agreement mechanisms to help agents collectively agree on the best set of norms that suit their human values. Overall, our agent model goes beyond the treatment of values as inert labels by connecting them to normative reasoning and to the social functionalities needed to integrate value-aware agents into our modern hybrid human-computer societies.",
    "citation_count": 3,
    "summary": "This paper proposes a novel framework for designing Artificial Moral Agents (AMAs) that integrates values as context-dependent goals, connecting individual values to collective norms through normative reasoning and agreement mechanisms to achieve value-alignment within human-computer societies. This approach moves beyond treating values as simple labels, fostering value-awareness and enabling AMAs to adapt norms to better reflect human values."
  },
  {
    "url": "https://arxiv.org/pdf/2303.00894.pdf",
    "title": "Active Reward Learning from Multiple Teachers",
    "published_date": "2023-03-02",
    "abstract": "Reward learning algorithms utilize human feedback to infer a reward function, which is then used to train an AI system. This human feedback is often a preference comparison, in which the human teacher compares several samples of AI behavior and chooses which they believe best accomplishes the objective. While reward learning typically assumes that all feedback comes from a single teacher, in practice these systems often query multiple teachers to gather sufficient training data. In this paper, we investigate this disparity, and find that algorithmic evaluation of these different sources of feedback facilitates more accurate and efficient reward learning. We formally analyze the value of information (VOI) when reward learning from teachers with varying levels of rationality, and define and evaluate an algorithm that utilizes this VOI to actively select teachers to query for feedback. Surprisingly, we find that it is often more informative to query comparatively irrational teachers. By formalizing this problem and deriving an analytical solution, we hope to facilitate improvement in reward learning approaches to aligning AI behavior with human values.",
    "citation_count": 11,
    "summary": "This paper analyzes reward learning from multiple human teachers with varying levels of rationality, showing that actively selecting which teachers to query, based on their value of information, improves both accuracy and efficiency of reward learning. Surprisingly, querying less rational teachers can be more informative than querying highly rational ones."
  },
  {
    "url": "https://arxiv.org/abs/2305.02748",
    "title": "A computational framework of human values for ethical AI",
    "published_date": "2023-05-04",
    "abstract": "In the diverse array of work investigating the nature of human values from psychology, philosophy and social sciences, there is a clear consensus that values guide behaviour. More recently, a recognition that values provide a means to engineer ethical AI has emerged. Indeed, Stuart Russell proposed shifting AI's focus away from simply ``intelligence'' towards intelligence ``provably aligned with human values''. This challenge -- the value alignment problem -- with others including an AI's learning of human values, aggregating individual values to groups, and designing computational mechanisms to reason over values, has energised a sustained research effort. Despite this, no formal, computational definition of values has yet been proposed. We address this through a formal conceptual framework rooted in the social sciences, that provides a foundation for the systematic, integrated and interdisciplinary investigation into how human values can support designing ethical AI.",
    "citation_count": 4,
    "summary": "This paper proposes a novel computational framework for defining human values, addressing the lack of formal definition in current ethical AI research and providing a foundation for aligning AI with human values. This framework integrates insights from psychology, philosophy, and social sciences to facilitate systematic and interdisciplinary investigation."
  },
  {
    "url": "https://arxiv.org/abs/2312.07401v3",
    "title": "On Diversified Preferences of Large Language Model Alignment",
    "published_date": "2023-12-12",
    "abstract": "Aligning large language models (LLMs) with human preferences has been recognized as the key to improving LLMs' interaction quality. However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods. This paper presents the first quantitative analysis of the experimental scaling law for reward models with varying sizes, from 1.3 billion to 7 billion parameters, trained with human feedback exhibiting diverse preferences. Our analysis reveals that the impact of diversified human preferences depends on both model size and data size. Larger models with sufficient capacity mitigate the negative effects of diverse preferences, while smaller models struggle to accommodate them. To mitigate the impact of diverse preferences, we introduce a new metric, Expected Calibration Error (ECE), to evaluate RMs and show their obvious positive correlation with the alignment performance of LLMs. Furthermore, we propose a Multi-Objective Reward learning method (MORE) to enhance the calibration performance of RMs on shared preferences. Through experiments on four models and five human preference datasets, we find the calibration error can be adopted as a key metric for evaluating RMs and MORE can obtain superior alignment performance.",
    "citation_count": 11,
    "summary": "This paper investigates the impact of diverse human preferences on large language model (LLM) alignment, finding that larger reward models with sufficient data mitigate negative effects better than smaller ones. A new metric, Expected Calibration Error (ECE), and a Multi-Objective Reward learning method (MORE) are proposed to improve alignment performance by focusing on shared preferences."
  },
  {
    "url": "https://www.alignmentforum.org/tag/human-values",
    "author": "Quintin Pope, Alex Turner",
    "title": "Human Values - AI Alignment Forum",
    "published_date": "2022-09-29",
    "summary": "Human values, which are complex and multifaceted, represent what we cherish and desire a benevolent superintelligence to protect and promote. Extrapolating these values into a comprehensive framework for AI alignment presents a significant challenge."
  },
  {
    "url": "https://www.lesswrong.com/tag/human-values",
    "author": "Quintin Pope, TurnTrout",
    "title": "Human Values - LessWrong",
    "published_date": "2022-09-29",
    "summary": "Human values, encompassing what we cherish and desire a superintelligence to uphold, are intricate and multifaceted. Their complexity presents a challenge in defining and ensuring an AI aligns with them."
  }
]