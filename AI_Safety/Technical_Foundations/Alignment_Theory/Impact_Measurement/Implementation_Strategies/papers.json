[
  {
    "url": "https://arxiv.org/pdf/2306.13686.pdf",
    "title": "Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems",
    "published_date": "2023-06-22",
    "abstract": "The increased use of AI systems is associated with multi-faceted societal, environmental, and economic consequences. These include non-transparent decision-making processes, discrimination, increasing inequalities, rising energy consumption and greenhouse gas emissions in AI model development and application, and an increasing concentration of economic power. By considering the multi-dimensionality of sustainability, this paper takes steps towards substantiating the call for an overarching perspective on\"sustainable AI\". It presents the SCAIS Framework (Sustainability Criteria and Indicators for Artificial Intelligence Systems) which contains a set 19 sustainability criteria for sustainable AI and 67 indicators that is based on the results of a critical review and expert workshops. This interdisciplinary approach contributes a unique holistic perspective to facilitate and structure the discourse on sustainable AI. Further, it provides a concrete framework that lays the foundation for developing standards and tools to support the conscious development and application of AI systems.",
    "citation_count": 1,
    "summary": "The paper introduces the SCAIS framework, a comprehensive set of 19 sustainability criteria and 67 indicators for assessing the societal, environmental, and economic impacts of AI systems, aiming to promote the development of truly \"sustainable AI\". This framework provides a holistic perspective and foundational structure for developing standards and tools to support responsible AI development and deployment."
  },
  {
    "url": "https://arxiv.org/abs/2304.07163v2",
    "title": "Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning",
    "published_date": "2023-04-14",
    "abstract": "A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason about the long-term consequences of following the expert policy or the default RL algorithm. Our experiments in four different settings show that these proposed algorithms achieve the above-mentioned goals whereas the other algorithms fail to do so.",
    "summary": "This paper proposes three novel bandit-based algorithms (UPIES, RPIES, and LPIES) for incorporating external advice into reinforcement learning, addressing the challenge of maintaining policy invariance while accelerating learning from arbitrary advice by modeling the problem as a multi-armed bandit and accounting for the non-stationary nature of returns. Empirical results demonstrate their superior performance compared to existing methods."
  },
  {
    "url": "https://arxiv.org/abs/2303.15471v2",
    "title": "Embedding Contextual Information through Reward Shaping in Multi-Agent Learning: A Case Study from Google Football",
    "published_date": "2023-03-25",
    "abstract": "Artificial Intelligence has been used to help human complete difficult tasks in complicated environments by providing optimized strategies for decision-making or replacing the manual labour. In environments including multiple agents, such as football, the most common methods to train agents are Imitation Learning and Multi-Agent Reinforcement Learning (MARL). However, the agents trained by Imitation Learning cannot outperform the expert demonstrator, which makes humans hardly get new insights from the learnt policy. Besides, MARL is prone to the credit assignment problem. In environments with sparse reward signal, this method can be inefficient. The objective of our research is to create a novel reward shaping method by embedding contextual information in reward function to solve the aforementioned challenges. We demonstrate this in the Google Research Football (GRF) environment. We quantify the contextual information extracted from game state observation and use this quantification together with original sparse reward to create the shaped reward. The experiment results in the GRF environment prove that our reward shaping method is a useful addition to state-of-the-art MARL algorithms for training agents in environments with sparse reward signal.",
    "citation_count": 1,
    "summary": "This paper proposes a novel reward shaping method for multi-agent reinforcement learning (MARL) that incorporates contextual information from game state observations to address the credit assignment problem in sparse reward environments. Experiments in Google Research Football demonstrate improved agent performance compared to standard MARL algorithms."
  },
  {
    "url": "https://www.lesswrong.com/posts/2eaLH7zp6pxdQwYSH",
    "author": "Austin Witte",
    "title": "A Brief Overview of AI Safety/Alignment Orgs, Fields, Researchers, and Resources for ML Researchers",
    "published_date": "2023-02-02",
    "summary": "Two overview documents, a short and a long version, have been created to help machine learning researchers quickly assess the AI safety field and identify potential research areas aligning with their existing skills. The resources list organizations, researchers, key papers, and keywords to facilitate efficient exploration of the field."
  },
  {
    "url": "https://arxiv.org/pdf/2209.07076.pdf",
    "title": "Responsible AI Implementation: A Human-centered Framework for Accelerating the Innovation Process",
    "published_date": "2022-09-15",
    "abstract": "There is still a significant gap between expectations and the successful adoption of AI to innovate and improve businesses. Due to the emergence of deep learning, AI adoption is more complex as it often incorporates big data and the internet of things (IoT), affecting data privacy. Existing frameworks have identified the need to focus on human-centered design, combining technical and business/organizational perspectives. However, trust remains a critical issue that needs to be designed from the beginning. The proposed framework is the first to expand from the human-centered design approach, emphasizing and maintaining the trust that underpins the whole process. This paper proposes a new theoretical framework for responsible artificial intelligence (AI) implementation. The proposed framework emphasizes a synergistic business-technology approach for the agile co-creation process. The aim is to streamline the adoption process of AI to innovate and improve business by involving all stakeholders throughout the project so that the AI technology is designed, developed, and deployed in conjunction with people and not in isolation. The framework presents a fresh viewpoint on responsible AI implementation based on analytical literature review, conceptual framework design, and practitioners' mediating expertise. The framework emphasizes establishing and maintaining trust throughout the human-centered design and agile development of AI. This human-centered approach is aligned with and enabled by the \"privacy-by-design‚Äù principle. The creators of the technology and the end-users are working together to tailor the AI solution specifically for the business requirements and human characteristics. An illustrative case study on adopting AI for assisting planning in a hospital will demonstrate that the proposed framework applies to real-life applications.",
    "citation_count": 4,
    "summary": "This paper proposes a new framework for responsible AI implementation that prioritizes trust and human-centered design, integrating business and technological perspectives through agile co-creation to streamline AI adoption and innovation. The framework emphasizes \"privacy-by-design\" and stakeholder collaboration throughout the entire AI lifecycle."
  },
  {
    "url": "https://www.alignmentforum.org/tag/practical",
    "author": "Luke H Miles",
    "title": "Practical - AI Alignment Forum",
    "published_date": "2022-06-18",
    "summary": "This website offers practical advice on achieving goals and improving well-being, grounding its recommendations in evidence-based research such as survey data, literature reviews, and self-blinded trials. It covers various topics including productivity, skills, interpersonal relationships, and domains of well-being."
  },
  {
    "url": "https://arxiv.org/pdf/2111.09478v1.pdf",
    "title": "Software engineering for Responsible AI: An empirical study and operationalised patterns",
    "published_date": "2021-11-18",
    "abstract": "AI ethics principles and guidelines are typically high-level and do not provide concrete guidance on how to develop responsible AI systems. To address this shortcoming, we perform an empirical study involving interviews with 21 scientists and engineers to understand the practitioners' views on AI ethics principles and their implementation. Our major findings are: (1) the current practice is often a done-once-and-forget type of ethical risk assessment at a particular development step, which is not sufficient for highly uncertain and continual learning AI systems; (2) ethical requirements are either omitted or mostly stated as high-level objectives, and not specified explicitly in verifiable way as system outputs or outcomes; (3) although ethical requirements have the characteristics of cross-cutting quality and non-functional requirements amenable to architecture and design analysis, system-level architecture and design are under-explored; (4) there is a strong desire for continuously monitoring and validating AI systems post deployment for ethical requirements but current operation practices provide limited guidance. To address these findings, we suggest a preliminary list of patterns to provide operationalised guidance for developing responsible AI systems.",
    "citation_count": 29,
    "summary": "This study reveals that current software engineering practices for responsible AI are inadequate, lacking continuous ethical assessment and verifiable specifications for ethical requirements throughout the system lifecycle. The authors propose operational patterns to address these shortcomings and improve the development of ethically responsible AI systems."
  },
  {
    "url": "https://arxiv.org/abs/2102.03479v8",
    "title": "Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning",
    "published_date": "2021-02-06",
    "abstract": "Many complex multi-agent systems such as robot swarms control and autonomous vehicle coordination can be modeled as Multi-Agent Reinforcement Learning (MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a baseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge (SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX target relaxing the monotonicity constraint of QMIX, allowing for performance improvement in SMAC. In this paper, we investigate the code-level optimizations of these variants and the monotonicity constraint. (1) We find that such improvements of the variants are significantly affected by various code-level optimizations. (2) The experiment results show that QMIX with normalized optimizations outperforms other works in SMAC; (3) beyond the common wisdom from these works, the monotonicity constraint can improve sample efficiency in SMAC and DEPP. We also discuss why monotonicity constraints work well in purely cooperative tasks with a theoretical analysis. We open-source the code at \\url{https://github.com/hijkzzz/pymarl2}.",
    "citation_count": 76,
    "summary": "This paper investigates code optimizations and the monotonicity constraint in QMIX, a multi-agent reinforcement learning algorithm, finding that optimized QMIX with the monotonicity constraint achieves superior performance and sample efficiency in benchmark environments like SMAC and DEPP. The authors provide a theoretical explanation for the constraint's effectiveness and open-source their code."
  }
]