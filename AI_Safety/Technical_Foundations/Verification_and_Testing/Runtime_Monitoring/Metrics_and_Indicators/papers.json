[
  {
    "url": "https://arxiv.org/abs/2208.14660",
    "title": "Unifying Evaluation of Machine Learning Safety Monitors",
    "published_date": "2022-08-31",
    "abstract": "With the increasing use of Machine Learning (ML) in critical autonomous systems, runtime monitors have been developed to detect prediction errors and keep the system in a safe state during operations. Monitors have been proposed for different applications involving diverse perception tasks and ML models, and specific evaluation procedures and metrics are used for different contexts. This paper introduces three unified safety-oriented metrics, representing the safety benefits of the monitor (Safety Gain), the remaining safety gaps after using it (Residual Hazard), and its negative impact on the system's performance (Availability Cost). To compute these metrics, one requires to define two return functions, representing how a given ML prediction will impact expected future rewards and hazards. Three use-cases (classification, drone landing, and autonomous driving) are used to demonstrate how metrics from the literature can be expressed in terms of the proposed metrics. Experimental results on these examples show how different evaluation choices impact the perceived performance of a monitor. As our formalism requires us to formulate explicit safety assumptions, it allows us to ensure that the evaluation conducted matches the high-level system requirements.",
    "citation_count": 9,
    "summary": "This paper proposes a unified framework for evaluating machine learning safety monitors using three metrics: Safety Gain, Residual Hazard, and Availability Cost, enabling consistent comparison across diverse applications by explicitly defining safety assumptions and reward/hazard functions. The framework's applicability is demonstrated through case studies in classification, drone landing, and autonomous driving."
  },
  {
    "url": "https://arxiv.org/abs/2405.13254",
    "title": "System Safety Monitoring of Learned Components Using Temporal Metric Forecasting",
    "published_date": "2024-05-21",
    "abstract": "In learning-enabled autonomous systems, safety monitoring of learned components is crucial to ensure their outputs do not lead to system safety violations, given the operational context of the system. However, developing a safety monitor for practical deployment in real-world applications is challenging. This is due to limited access to internal workings and training data of the learned component. Furthermore, safety monitors should predict safety violations with low latency, while consuming a reasonable computation resource amount.\n \n To address the challenges, we propose a safety monitoring method based on probabilistic time series forecasting. Given the learned component outputs and an operational context, we empirically investigate different Deep Learning (DL)-based probabilistic forecasting to predict the objective measure capturing the satisfaction or violation of a safety requirement (\n safety metric\n ). We empirically evaluate safety metric and violation prediction accuracy, and inference latency and resource usage of four state-of-the-art models, with varying horizons, using autonomous aviation and autonomous driving case studies. Our results suggest that probabilistic forecasting of safety metrics, given learned component outputs and scenarios, is effective for safety monitoring. Furthermore, for both case studies, the Temporal Fusion Transformer (TFT) was the most accurate model for predicting imminent safety violations, with acceptable latency and resource consumption.\n",
    "summary": "This paper proposes a safety monitoring method for learned components in autonomous systems using probabilistic time series forecasting, specifically evaluating several deep learning models' ability to predict safety metric violations with low latency and resource consumption, finding the Temporal Fusion Transformer to be most effective."
  },
  {
    "url": "https://arxiv.org/abs/2308.09937",
    "title": "Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services",
    "published_date": "2023-08-19",
    "abstract": "As modern software systems continue to grow in terms of complexity and volume, anomaly detection on multivariate monitoring metrics, which profile systems' health status, becomes more and more critical and challenging. In particular, the dependency between different metrics and their historical patterns plays a critical role in pursuing prompt and accurate anomaly detection. Existing approaches fall short of industrial needs for being unable to capture such information efficiently. To fill this significant gap, in this paper, we propose CMAnomaly, an anomaly detection framework on multivariate monitoring metrics based on collaborative machine. The proposed collaborative machine is a mechanism to capture the pairwise interactions along with feature and temporal dimensions with linear time complexity. Cost-effective models can then be employed to leverage both the dependency between monitoring metrics and their historical patterns for anomaly detection. The proposed framework is extensively evaluated with both public data and industrial data collected from a large-scale online service system of Huawei Cloud. The experimental results demonstrate that compared with state-of-the-art baseline models, CMAnomaly achieves an average F1 score of 0.9494, outperforming baselines by 6.77% ~ 10.68%, and runs 10× ~ 20× faster. Furthermore, we also share our experience of deploying CMAnomaly in Huawei Cloud.",
    "citation_count": 5,
    "summary": "CMAnomaly, a novel anomaly detection framework for multivariate monitoring metrics, uses a collaborative machine to efficiently capture inter-metric dependencies and historical patterns, achieving significantly higher F1 scores and faster processing speeds than existing methods. Its effectiveness is validated through extensive experiments on public and industrial datasets from Huawei Cloud."
  },
  {
    "url": "https://arxiv.org/pdf/2305.17754.pdf",
    "title": "Online Causation Monitoring of Signal Temporal Logic",
    "published_date": "2023-05-28",
    "abstract": "Online monitoring is an effective validation approach for hybrid systems, that, at runtime, checks whether the (partial) signals of a system satisfy a specification in, e.g., Signal Temporal Logic (STL). The classic STL monitoring is performed by computing a robustness interval that specifies, at each instant, how far the monitored signals are from violating and satisfying the specification. However, since a robustness interval monotonically shrinks during monitoring, classic online monitors may fail in reporting new violations or in precisely describing the system evolution at the current instant. In this paper, we tackle these issues by considering the causation of violation or satisfaction, instead of directly using the robustness. We first introduce a Boolean causation monitor that decides whether each instant is relevant to the violation or satisfaction of the specification. We then extend this monitor to a quantitative causation monitor that tells how far an instant is from being relevant to the violation or satisfaction. We further show that classic monitors can be derived from our proposed ones. Experimental results show that the two proposed monitors are able to provide more detailed information about system evolution, without requiring a significantly higher monitoring cost.",
    "citation_count": 1,
    "summary": "This paper proposes novel online monitors for Signal Temporal Logic (STL) specifications that track the causation of violations and satisfactions, providing more detailed information about system evolution than traditional robustness-based monitors. These causation-based monitors, both Boolean and quantitative, offer improved insights without significant computational overhead."
  },
  {
    "url": "https://arxiv.org/abs/2307.10869v1",
    "title": "Identifying Performance Issues in Cloud Service Systems Based on Relational-Temporal Features",
    "published_date": "2023-07-20",
    "abstract": "\n Cloud systems, typically comprised of various components (\n e.g.\n , microservices), are susceptible to performance issues, which may cause service-level agreement violations and financial losses. Identifying performance issues is thus of paramount importance for cloud vendors. In current practice, crucial metrics,\n i.e.\n , key performance indicators (KPIs), are monitored periodically to provide insight into the operational status of components. Identifying performance issues is often formulated as an anomaly detection problem, which is tackled by analyzing each metric independently. However, this approach overlooks the complex dependencies existing among cloud components. Some graph neural network-based methods take both temporal and relational information into account, however, the correlation violations in the metrics that serve as indicators of underlying performance issues are difficult for them to identify. Furthermore, a large volume of components in a cloud system results in a vast array of noisy metrics. This complexity renders it impractical for engineers to fully comprehend the correlations, making it challenging to identify performance issues accurately. To address these limitations, we propose Identifying Performance Issues based on Relational-Temporal Features (ISOLATE ), a learning-based approach that leverages both the relational and temporal features of metrics to identify performance issues. In particular, it adopts a graph neural network with attention to characterizing the relations among metrics and extracts long-term and multi-scale temporal patterns using a GRU and a convolution network, respectively. The learned graph attention weights can be further used to localize the correlation-violated metrics. Moreover, to relieve the impact of noisy data, ISOLATE utilizes a positive unlabeled learning strategy that tags pseudo labels based on a small portion of confirmed negative examples. Extensive evaluation on both public and industrial datasets shows that ISOLATE outperforms all baseline models with 0.945 F1-score and 0.920 Hit rate@3. The ablation study also proves the effectiveness of the relational-temporal features and the PU-learning strategy. Furthermore, we share the success stories of leveraging ISOLATE to identify performance issues in Huawei Cloud, which demonstrates its superiority in practice.\n",
    "citation_count": 2,
    "summary": "ISOLATE, a novel approach for identifying cloud service performance issues, leverages graph neural networks and a positive unlabeled learning strategy to analyze both relational and temporal features of system metrics, significantly improving accuracy over existing methods. Its effectiveness is demonstrated through extensive evaluation on public and industrial datasets, achieving high F1-score and Hit rate, and successful application within Huawei Cloud."
  },
  {
    "title": "Monitoring Runtime Metrics of Fog Manufacturing via a Qualitative and Quantitative (QQ) Control Chart",
    "abstract": "Fog manufacturing combines Fog and Cloud computing in a manufacturing network to provide efficient data analytics and support real-time decision-making. Detecting anomalies, including imbalanced computational workloads and cyber-attacks, is critical to ensure reliable and responsive computation services. However, such anomalies often concur with dynamic offloading events where computation tasks are migrated from well-occupied Fog nodes to less-occupied ones to reduce the overall computation time latency and improve the throughput. Such concurrences jointly affect the system behaviors, which makes anomaly detection inaccurate. We propose a qualitative and quantitative (QQ) control chart to monitor system anomalies through identifying the changes of monitored runtime metric relationship (quantitative variables) under the presence of dynamic offloading (qualitative variable) using a risk-adjusted monitoring framework. Both the simulation and Fog manufacturing case studies show the advantage of the proposed method compared with the existing literature under the dynamic offloading influence.",
    "published_date": "2022-03-17",
    "url": "https://dl.acm.org/doi/10.1145/3501262",
    "summary": "This paper presents a novel qualitative and quantitative (QQ) control chart for monitoring runtime metrics in fog manufacturing, addressing the challenge of accurately detecting anomalies amidst dynamic task offloading events that confound traditional methods. The proposed approach improves anomaly detection accuracy by considering both quantitative metric changes and the qualitative impact of offloading."
  },
  {
    "url": "https://arxiv.org/pdf/2204.08999.pdf",
    "title": "STPA-driven Multilevel Runtime Monitoring for In-time Hazard Detection",
    "published_date": "2022-04-19",
    "abstract": ". Runtime veriﬁcation or runtime monitoring equips safety-critical cyber-physical systems to augment design assurance measures and ensure operational safety and security. Cyber-physical systems have interaction failures, attack surfaces, and attack vectors resulting in unan-ticipated hazards and loss scenarios. These interaction failures pose challenges to runtime veriﬁcation regarding monitoring speciﬁcations and monitoring placements for in-time detection of hazards. We develop a well-formed workﬂow model that connects system theoretic process analysis, commonly referred to as STPA, hazard causation information to lower-level runtime monitoring to detect hazards at the operational phase. Speciﬁcally, our model follows the DepDevOps paradigm to provide evidence and insights to runtime monitoring on what to monitor, where to monitor, and the monitoring context. We demonstrate and evaluate the value of multilevel monitors by injecting hazards on an autonomous emergency braking system model.",
    "citation_count": 4,
    "summary": "This paper presents a workflow model integrating System-Theoretic Process Analysis (STPA) with multilevel runtime monitoring to improve hazard detection in cyber-physical systems. The model uses STPA to inform the placement and specifications of monitors, enabling timely detection of hazards during operation, as demonstrated through an autonomous emergency braking system example."
  },
  {
    "url": "https://www.lesswrong.com/posts/cCcCJnvMEHqrgiCnx/practical-use-of-the-beta-distribution-for-data-analysis",
    "author": "Maxwell Peterson",
    "title": "Practical use of the Beta distribution for data analysis",
    "published_date": "2022-04-03",
    "summary": "For calculating probabilities from binary count data, the Beta distribution is more accurate than the commonly used Gaussian (normal) distribution, especially with small datasets or probabilities near 0 or 1 where the Gaussian approximation breaks down, yielding nonsensical results like negative probabilities. The Beta distribution is readily available in common statistical software and provides a superior solution."
  }
]