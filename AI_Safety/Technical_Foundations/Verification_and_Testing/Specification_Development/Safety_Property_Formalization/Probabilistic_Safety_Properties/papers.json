[
  {
    "url": "https://www.alignmentforum.org/posts/LkECxpbjvSifPfjnb/towards-guaranteed-safe-ai-a-framework-for-ensuring-robust-1",
    "author": "Joar Skalse",
    "title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems",
    "published_date": "2024-05-17"
  },
  {
    "url": "https://www.alignmentforum.org/posts/yi4pqB6G73dcTnatq/aspiration-based-designs-3-performance-and-safety-criteria",
    "author": "Jobst Heitzig",
    "title": "[Aspiration-based designs] 3. Performance and safety criteria, and aspiration intervals",
    "published_date": "2024-04-28"
  },
  {
    "url": "https://www.lesswrong.com/posts/3P8WBwLyfPBEkbG3c/proveably-safe-self-driving-cars",
    "author": "Davidmanheim",
    "title": "Proveably Safe Self Driving Cars",
    "published_date": "2024-09-15"
  },
  {
    "url": "https://www.lesswrong.com/posts/ahDYwfKaPnNJSjtDL/safe-agi-complexity-guessing-a-higher-order-algebraic-number",
    "author": "Sven Nilsen",
    "title": "Safe AGI Complexity: Guessing a Higher-Order Algebraic Number",
    "published_date": "2023-04-10"
  },
  {
    "url": "https://www.lesswrong.com/posts/Zd5Bsra7ar2pa3bwS/probability-theory-and-logical-induction-as-lenses",
    "author": "Alex Flint",
    "title": "Probability theory and logical induction as lenses",
    "published_date": "2021-04-23"
  },
  {
    "url": "https://www.alignmentforum.org/s/57bsaXbJXbzKqNkrf",
    "author": "Mark Xu",
    "title": "Intermittent Distllations - AI Alignment Forum",
    "published_date": "2021-04-14"
  }
]