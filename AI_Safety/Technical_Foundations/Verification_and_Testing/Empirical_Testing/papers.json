[
  {
    "url": "https://arxiv.org/auth/show-endorsers/1910.13645",
    "title": "Automatic Testing With Reusable Adversarial Agents",
    "published_date": "2019-10-30",
    "abstract": "Autonomous systems such as self-driving cars and general-purpose robots are safety-critical systems that operate in highly uncertain and dynamic environments. We propose an interactive multi-agent framework where the system-underdesign is modeled as an ego agent and its environment is modeled by a number of adversarial (ado) agents. For example, a selfdriving car is an ego agent whose behavior is influenced by ado agents such as pedestrians, bicyclists, traffic lights, road geometry etc. Given a logical specification of the correct behavior of the ego agent, and a set of constraints that encode reasonable adversarial behavior, our framework reduces the adversarial testing problem to the problem of synthesizing controllers for (constrained) ado agents that cause the ego agent to violate its specifications. Specifically, we explore the use of tabular and deep reinforcement learning approaches for synthesizing adversarial agents. We show that ado agents trained in this fashion are better than traditional falsification or testing techniques because they can generalize to ego agents and environments that differ from the original ego agent. We demonstrate the efficacy of our technique on two realworld case studies from the domain of self-driving cars.",
    "citation_count": 4,
    "summary": "This paper presents a multi-agent framework for automatically testing autonomous systems by training adversarial agents via reinforcement learning to violate the system's specifications; these reusable adversarial agents generalize better than traditional testing methods, as demonstrated in self-driving car case studies."
  },
  {
    "url": "https://arxiv.org/abs/2411.19567",
    "title": "AdvFuzz: Finding More Violations Caused by the EGO Vehicle in Simulation Testing by Adversarial NPC Vehicles",
    "published_date": "2024-11-29",
    "abstract": "Recently, there has been a significant escalation in both academic and industrial commitment towards the development of autonomous driving systems (ADSs). A number of simulation testing approaches have been proposed to generate diverse driving scenarios for ADS testing. However, scenarios generated by these previous approaches are static and lack interactions between the EGO vehicle and the NPC vehicles, resulting in a large amount of time on average to find violation scenarios. Besides, a large number of the violations they found are caused by aggressive behaviors of NPC vehicles, revealing none bugs of ADS. In this work, we propose the concept of adversarial NPC vehicles and introduce AdvFuzz, a novel simulation testing approach, to generate adversarial scenarios on main lanes (e.g., urban roads and highways). AdvFuzz allows NPC vehicles to dynamically interact with the EGO vehicle and regulates the behaviors of NPC vehicles, finding more violation scenarios caused by the EGO vehicle more quickly. We compare AdvFuzz with a random approach and three state-of-the-art scenario-based testing approaches. Our experiments demonstrate that AdvFuzz can generate 198.34% more violation scenarios compared to the other four approaches in 12 hours and increase the proportion of violations caused by the EGO vehicle to 87.04%, which is more than 7 times that of other approaches. Additionally, AdvFuzz is at least 92.21% faster in finding one violation caused by the EGO vehicle than that of the other approaches.",
    "summary": "AdvFuzz is a novel simulation testing approach for autonomous driving systems that uses adversarial non-player characters (NPCs) to efficiently generate scenarios revealing EGO vehicle violations, significantly outperforming existing methods in both speed and the proportion of EGO-caused violations. This results in faster identification of bugs within the autonomous driving system itself."
  },
  {
    "url": "https://www.alignmentforum.org/posts/XSqntCNMafhcy9irf/third-party-testing-as-a-key-ingredient-of-ai-policy",
    "author": "Zac Hatfield-Dodds",
    "title": "Third-party testing as a key ingredient of AI policy",
    "published_date": "2024-03-25",
    "summary": "The article advocates for third-party testing of large-scale AI systems to mitigate societal harm from misuse or accidents. This approach, involving industry, government, and academia, aims to build trust, avoid overly burdensome regulation, and facilitate international cooperation."
  },
  {
    "url": "https://www.lesswrong.com/posts/324pQjqoHEHeF2vPs/ai-clarity-an-initial-research-agenda",
    "author": "Justin Bullock, Corin Katzke, Zershaaneh Qureshi, David_Kristoffersson",
    "title": "AI Clarity: An Initial Research Agenda",
    "published_date": "2024-05-03",
    "summary": "The AI Clarity research program uses scenario planning to explore potential pathways to existential risks from transformative AI (TAI), particularly focusing on short timelines (within a decade). The program aims to evaluate strategies for AI safety and governance across these scenarios to mitigate potential existential threats."
  },
  {
    "url": "https://www.alignmentforum.org/posts/m6poxWegJkp8LPpjw/can-generalized-adversarial-testing-enable-more-rigorous-llm",
    "author": "Stephen Casper",
    "title": "Can Generalized Adversarial Testing Enable More Rigorous LLM Safety Evals?",
    "published_date": "2024-07-30",
    "summary": "Current LLM safety evaluations often fall short because they focus on standard inputs, neglecting the risk of malicious modifications. \"Generalized\" adversarial testing, which allows manipulation of model weights and activations, offers a more robust approach to identifying and mitigating latent harmful capabilities in deployed LLMs, even those accessed via black-box APIs."
  },
  {
    "url": "https://www.lesswrong.com/posts/2eaLH7zp6pxdQwYSH",
    "author": "Austin Witte",
    "title": "A Brief Overview of AI Safety/Alignment Orgs, Fields, Researchers, and Resources for ML Researchers",
    "published_date": "2023-02-02",
    "summary": "Two overview documents, a short and a long version, have been created to help machine learning researchers quickly assess the AI safety field and identify potential research areas based on their existing skills. These resources list key organizations, researchers, papers, and keywords to facilitate efficient exploration of the field."
  },
  {
    "url": "https://arxiv.org/abs/2205.08589v1",
    "title": "Hierarchical Distribution-aware Testing of Deep Learning",
    "published_date": "2022-05-17",
    "abstract": "With its growing use in safety/security-critical applications, Deep Learning (DL) has raised increasing concerns regarding its dependability. In particular, DL has a notorious problem of lacking robustness. Input added with adversarial perturbations, i.e., Adversarial Examples (AEs), are easily mispredicted by the DL model. Despite recent efforts made in detecting AEs via state-of-the-art attack and testing methods, they are normally input distribution–agnostic and/or disregard the perceptual quality of adversarial perturbations. Consequently, the detected AEs are irrelevant inputs in the application context or noticeably unrealistic to humans. This may lead to a limited effect on improving the DL model's dependability, as the testing budget is likely to be wasted on detecting AEs that are encountered very rarely in its real-life operations. In this article, we propose a new robustness testing approach for detecting AEs that considers both the feature-level distribution and the pixel-level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. First, we select test seeds based on the density of feature-level distribution and the vulnerability of adversarial robustness. The vulnerability of test seeds is indicated by the auxiliary information, which are highly correlated with local robustness. Given a test seed, we then develop a novel genetic algorithm–based local test case generation method, in which two fitness functions work alternatively to control the perceptual quality of detected AEs. Finally, extensive experiments confirm that our holistic approach considering hierarchical distributions is superior to the state-of-the-arts that either disregard any input distribution or only consider a single (non-hierarchical) distribution, in terms of not only detecting imperceptible AEs but also improving the overall robustness of the DL model under testing.",
    "citation_count": 7,
    "summary": "This paper introduces a novel hierarchical distribution-aware testing approach for deep learning models, improving robustness testing by generating perceptually realistic adversarial examples through a genetic algorithm that considers both feature-level and pixel-level input distributions, leading to more effective detection of vulnerabilities and model improvements."
  },
  {
    "url": "https://arxiv.org/pdf/2201.05647v1.pdf",
    "title": "Tools and Practices for Responsible AI Engineering",
    "published_date": "2022-01-14",
    "abstract": "Responsible Artificial Intelligence (AI) - the practice of developing, evaluating, and maintaining accurate AI systems that also exhibit essential properties such as robustness and explainability - represents a multifaceted challenge that often stretches standard machine learning tooling, frameworks, and testing methods beyond their limits. In this paper, we present two new software libraries - hydra-zen and the rAI-toolbox - that address critical needs for responsible AI engineering. hydra-zen dramatically simplifies the process of making complex AI applications configurable, and their behaviors reproducible. The rAI-toolbox is designed to enable methods for evaluating and enhancing the robustness of AI-models in a way that is scalable and that composes naturally with other popular ML frameworks. We describe the design principles and methodologies that make these tools effective, including the use of property-based testing to bolster the reliability of the tools themselves. Finally, we demonstrate the composability and flexibility of the tools by showing how various use cases from adversarial robustness and explainable AI can be concisely implemented with familiar APIs.",
    "citation_count": 14,
    "summary": "This paper introduces hydra-zen and the rAI-toolbox, two software libraries designed to improve responsible AI engineering by simplifying configuration, promoting reproducibility, and enabling scalable robustness evaluation of AI models. The libraries utilize property-based testing and integrate with existing ML frameworks for enhanced flexibility and composability."
  },
  {
    "url": "https://www.lesswrong.com/posts/7gkXuHEm6CqEGT2mg/ai-safety-seems-hard-to-measure",
    "author": "HoldenKarnofsky",
    "title": "AI Safety Seems Hard to Measure",
    "published_date": "2022-12-08",
    "summary": "The article argues that ensuring AI safety is exceptionally difficult due to the inherent challenges in reliably testing for genuinely safe AI behavior, as opposed to merely apparent safety. Four key problems are highlighted: the difficulty of distinguishing genuine compliance from sophisticated deception, the uncertainty of how AI will behave with increased autonomy, the limitations of testing on less capable AI systems, and the complete unpredictability of vastly superhuman AI."
  },
  {
    "url": "https://arxiv.org/pdf/2101.10430.pdf",
    "title": "Test and Evaluation Framework for Multi-Agent Systems of Autonomous Intelligent Agents",
    "published_date": "2021-01-25",
    "abstract": "Test and evaluation is a necessary process for ensuring that engineered systems perform as intended under a variety of conditions, both expected and unexpected. In this work, we consider the unique challenges of developing a unifying test and evaluation framework for complex ensembles of cyber-physical systems with embedded artificial intelligence. We propose a framework that incorporates test and evaluation throughout not only the development life cycle, but continues into operation as the system learns and adapts in a noisy, changing, and contended environment. The framework accounts for the challenges of testing the integration of diverse systems at various hierarchical scales of composition while respecting that testing time and resources are limited. A generic use case is provided for illustrative purposes. Research directions emerging as a result of exploring the use case via the framework are suggested.",
    "citation_count": 6,
    "summary": "This paper proposes a unifying test and evaluation framework for complex multi-agent systems, addressing the challenges of testing integrated cyber-physical systems with AI throughout their lifecycle, from development to operation in dynamic environments. The framework considers resource limitations and diverse system integration at multiple hierarchical scales."
  }
]