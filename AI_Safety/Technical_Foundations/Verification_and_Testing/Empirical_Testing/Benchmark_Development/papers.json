[
  {
    "url": "https://arxiv.org/abs/2411.12990",
    "title": "BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices",
    "published_date": "2024-11-20",
    "abstract": "AI models are increasingly prevalent in high-stakes environments, necessitating thorough assessment of their capabilities and risks. Benchmarks are popular for measuring these attributes and for comparing model performance, tracking progress, and identifying weaknesses in foundation and non-foundation models. They can inform model selection for downstream tasks and influence policy initiatives. However, not all benchmarks are the same: their quality depends on their design and usability. In this paper, we develop an assessment framework considering 46 best practices across an AI benchmark's lifecycle and evaluate 24 AI benchmarks against it. We find that there exist large quality differences and that commonly used benchmarks suffer from significant issues. We further find that most benchmarks do not report statistical significance of their results nor allow for their results to be easily replicated. To support benchmark developers in aligning with best practices, we provide a checklist for minimum quality assurance based on our assessment. We also develop a living repository of benchmark assessments to support benchmark comparability, accessible at betterbench.stanford.edu.",
    "citation_count": 2,
    "summary": "BetterBench assesses 24 AI benchmarks against 46 best practices, revealing significant quality discrepancies and common issues like lack of statistical significance and reproducibility; the study provides a checklist and repository to improve benchmark development and comparability."
  },
  {
    "url": "https://arxiv.org/abs/2407.21792",
    "title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?",
    "published_date": "2024-07-31",
    "abstract": "As artificial intelligence systems grow more powerful, there has been increasing interest in\"AI safety\"research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, we conduct a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. Our findings reveal that many safety benchmarks highly correlate with both upstream model capabilities and training compute, potentially enabling\"safetywashing\"--where capability improvements are misrepresented as safety advancements. Based on these findings, we propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, we aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress.",
    "citation_count": 7,
    "summary": "The paper argues that current AI safety benchmarks often correlate strongly with general model capabilities and compute, potentially allowing researchers to misrepresent capability gains as safety improvements (\"safetywashing\"). The authors propose a more rigorous, empirically grounded definition of AI safety to facilitate measurable progress."
  },
  {
    "url": "https://arxiv.org/abs/2407.17436",
    "title": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies",
    "published_date": "2024-07-11",
    "abstract": "Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-Bench 2024, uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-Bench 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.",
    "citation_count": 6,
    "summary": "AIR-Bench 2024 is a new AI safety benchmark aligned with existing regulations and company policies, containing 5,694 prompts categorized by a four-tiered taxonomy derived from an analysis of 8 government regulations and 16 company policies, enabling more comprehensive evaluation of foundation model safety."
  },
  {
    "url": "https://arxiv.org/abs/2404.12241",
    "title": "Introducing v0.5 of the AI Safety Benchmark from MLCommons",
    "published_date": "2024-04-18",
    "abstract": "This paper introduces v0.5 of the AI Safety Benchmark, which has been created by the MLCommons AI Safety Working Group. The AI Safety Benchmark has been designed to assess the safety risks of AI systems that use chat-tuned language models. We introduce a principled approach to specifying and constructing the benchmark, which for v0.5 covers only a single use case (an adult chatting to a general-purpose assistant in English), and a limited set of personas (i.e., typical users, malicious users, and vulnerable users). We created a new taxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark. We plan to release version 1.0 of the AI Safety Benchmark by the end of 2024. The v1.0 benchmark will provide meaningful insights into the safety of AI systems. However, the v0.5 benchmark should not be used to assess the safety of AI systems. We have sought to fully document the limitations, flaws, and challenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes (1) a principled approach to specifying and constructing the benchmark, which comprises use cases, types of systems under test (SUTs), language and context, personas, tests, and test items; (2) a taxonomy of 13 hazard categories with definitions and subcategories; (3) tests for seven of the hazard categories, each comprising a unique set of test items, i.e., prompts. There are 43,090 test items in total, which we created with templates; (4) a grading system for AI systems against the benchmark; (5) an openly available platform, and downloadable tool, called ModelBench that can be used to evaluate the safety of AI systems on the benchmark; (6) an example evaluation report which benchmarks the performance of over a dozen openly available chat-tuned language models; (7) a test specification for the benchmark.",
    "citation_count": 21,
    "summary": "MLCommons' AI Safety Benchmark v0.5, focusing on chat-tuned language models interacting with various user personas, introduces a framework for evaluating AI safety risks across seven hazard categories using 43,090 test prompts; however, it's a preliminary version unsuitable for definitive safety assessments."
  },
  {
    "url": "https://arxiv.org/abs/2412.14470",
    "title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents",
    "published_date": "2024-12-19",
    "abstract": "As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through quantitative analysis, we identify critical failure modes and summarize two fundamental safety detects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone is insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. We release Agent-SafetyBench at \\url{https://github.com/thu-coai/Agent-SafetyBench} to facilitate further research and innovation in agent safety evaluation and improvement.",
    "summary": "Agent-SafetyBench is a new benchmark evaluating the safety of large language model (LLM) agents across 349 environments and 2,000 test cases, revealing that none of 16 popular agents achieved a safety score above 60% and highlighting critical flaws in robustness and risk awareness."
  },
  {
    "url": "https://www.lesswrong.com/posts/BCz7viTXMhjxdkFRs/paper-identifying-the-risks-of-lm-agents-with-an-lm-emulated",
    "author": "Singularian2501",
    "title": "Paper: Identifying the Risks of LM Agents with an LM-Emulated Sandbox - University of Toronto 2023 - Benchmark consisting of 36 high-stakes tools and 144 test cases!",
    "published_date": "2023-10-09",
    "summary": "ToolEmu is a framework using a language model to emulate tool execution and automatically evaluate the safety of language model agents interacting with tools, significantly reducing the cost and effort of identifying high-stakes risks; initial testing reveals substantial safety failures even in leading agents, highlighting the need for improved safety measures."
  },
  {
    "url": "https://www.lesswrong.com/posts/7DhwRLoKm4nMrFFsH/measuring-and-forecasting-risks",
    "author": "abergal, Nick_Beckstead, jsteinhardt",
    "title": "Measuring and forecasting risks",
    "published_date": "2021-10-29",
    "summary": "This RFP seeks proposals for measuring safety-related properties of machine learning systems, focusing on quantifiable risks like hacking, misgeneralization, and misalignment that worsen with increased capabilities, as well as emergent capabilities that pose unforeseen threats. The goal is to operationalize these risks through measurable quantities, incentivizing developers to prioritize AI safety."
  },
  {
    "url": "https://arxiv.org/abs/2403.04893",
    "title": "A Safe Harbor for AI Evaluation and Red Teaming",
    "published_date": "2024-03-07",
    "abstract": "Independent evaluation and red teaming are critical for identifying the risks posed by generative AI systems. However, the terms of service and enforcement strategies used by prominent AI companies to deter model misuse have disincentives on good faith safety evaluations. This causes some researchers to fear that conducting such research or releasing their findings will result in account suspensions or legal reprisal. Although some companies offer researcher access programs, they are an inadequate substitute for independent research access, as they have limited community representation, receive inadequate funding, and lack independence from corporate incentives. We propose that major AI developers commit to providing a legal and technical safe harbor, indemnifying public interest safety research and protecting it from the threat of account suspensions or legal reprisal. These proposals emerged from our collective experience conducting safety, privacy, and trustworthiness research on generative AI systems, where norms and incentives could be better aligned with public interests, without exacerbating model misuse. We believe these commitments are a necessary step towards more inclusive and unimpeded community efforts to tackle the risks of generative AI.",
    "citation_count": 20,
    "summary": "The paper argues that current AI company policies hinder independent safety research by creating disincentives through fear of legal repercussions; it proposes a \"safe harbor\" providing legal protection for researchers evaluating and red-teaming generative AI models."
  }
]