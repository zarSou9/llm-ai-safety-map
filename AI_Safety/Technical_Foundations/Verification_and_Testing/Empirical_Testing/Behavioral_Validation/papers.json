[
  {
    "url": "https://arxiv.org/pdf/2012.01557.pdf",
    "title": "Value Alignment Verification",
    "published_date": "2020-12-02",
    "abstract": "As humans interact with autonomous agents to perform increasingly complicated, potentially risky tasks, it is important that humans can verify these agents' trustworthiness and efficiently evaluate their performance and correctness. In this paper we formalize the problem of value alignment verification: how to efficiently test whether the goals and behavior of another agent are aligned with a human's values? We explore several different value alignment verification settings and provide foundational theory regarding value alignment verification. We study alignment verification problems with an idealized human that has an explicit reward function as well as value alignment verification problems where the human has implicit values. Our theoretical and empirical results in both a discrete grid navigation domain and a continuous autonomous driving domain demonstrate that it is possible to synthesize highly efficient and accurate value alignment verification tests for certifying the alignment of autonomous agents.",
    "citation_count": 28,
    "summary": "This paper formalizes the problem of value alignment verification for autonomous agents, exploring theoretical and empirical methods to efficiently test if an agent's goals and behavior align with human values, both explicit and implicit. The authors demonstrate the feasibility of creating effective verification tests in simulated navigation and autonomous driving scenarios."
  },
  {
    "url": "https://arxiv.org/abs/2410.00081",
    "title": "From homeostasis to resource sharing: Biologically and economically compatible multi-objective multi-agent AI safety benchmarks",
    "published_date": "2024-09-30",
    "abstract": "Developing safe agentic AI systems benefits from automated empirical testing that conforms with human values, a subfield that is largely underdeveloped at the moment. To contribute towards this topic, present work focuses on introducing biologically and economically motivated themes that have been neglected in the safety aspects of modern reinforcement learning literature, namely homeostasis, balancing multiple objectives, bounded objectives, diminishing returns, sustainability, and multi-agent resource sharing. We implemented eight main benchmark environments on the above themes, for illustrating the potential shortcomings of current mainstream discussions on AI safety.",
    "summary": "This paper introduces eight new reinforcement learning benchmark environments designed to evaluate AI safety, focusing on biologically and economically inspired challenges like homeostasis, multi-objective optimization, and resource sharing, thereby highlighting shortcomings of current AI safety research. These benchmarks aim to improve the alignment of AI systems with human values through more realistic and nuanced testing."
  },
  {
    "url": "https://arxiv.org/abs/2409.09586",
    "title": "ValueCompass: A Framework of Fundamental Values for Human-AI Alignment",
    "published_date": "2024-09-15",
    "abstract": "As AI systems become more advanced, ensuring their alignment with a diverse range of individuals and societal values becomes increasingly critical. But how can we capture fundamental human values and assess the degree to which AI systems align with them? We introduce ValueCompass, a framework of fundamental values, grounded in psychological theory and a systematic review, to identify and evaluate human-AI alignment. We apply ValueCompass to measure the value alignment of humans and language models (LMs) across four real-world vignettes: collaborative writing, education, public sectors, and healthcare. Our findings uncover risky misalignment between humans and LMs, such as LMs agreeing with values like\"Choose Own Goals\", which are largely disagreed by humans. We also observe values vary across vignettes, underscoring the necessity for context-aware AI alignment strategies. This work provides insights into the design space of human-AI alignment, offering foundations for developing AI that responsibly reflects societal values and ethics.",
    "summary": "ValueCompass is a framework using psychological theory to identify and evaluate human-AI value alignment, revealing significant misalignments between humans and language models across various real-world scenarios, highlighting the need for context-aware alignment strategies."
  },
  {
    "url": "https://arxiv.org/abs/2410.19198",
    "title": "MAP: Multi-Human-Value Alignment Palette",
    "published_date": "2024-10-24",
    "abstract": "Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.",
    "summary": "The Multi-Human-Value Alignment Palette (MAP) framework addresses the challenge of aligning generative AI with multiple, potentially conflicting human values by formulating the problem as a constrained optimization task. MAP uses a primal-dual approach to determine the achievability and method for achieving user-defined value alignment targets, demonstrating strong empirical performance across various tasks."
  },
  {
    "url": "https://www.alignmentforum.org/posts/aoEnDEmoKCK9S99hL/cognitive-biases-contributing-to-ai-x-risk-a-deleted-excerpt",
    "author": "Andrew_Critch",
    "title": "Cognitive Biases Contributing to AI X-risk — a deleted excerpt from my 2018 ARCHES draft",
    "published_date": "2024-12-03",
    "summary": "The author discusses cognitive biases, specifically the \"illusion of control\" and \"scope insensitivity,\" that hinder accurate assessment of artificial intelligence existential risks. These biases, along with a lack of established scientific methods for evaluating AI prepotence, increase the likelihood of unrecognized and potentially catastrophic AI misalignment."
  },
  {
    "url": "https://www.alignmentforum.org/posts/xj5nzResmDZDqLuLo/estimating-tail-risk-in-neural-networks",
    "author": "Jacob Hilton; Mark Xu",
    "title": "Estimating Tail Risk in Neural Networks",
    "published_date": "2024-09-13",
    "summary": "Current methods for assessing the risk of catastrophic AI behavior are insufficient because they rely on finding specific harmful inputs, which is computationally prohibitive. The article proposes developing methods to estimate the probability of catastrophic events without relying on identifying harmful inputs, thereby improving safety in AI systems."
  },
  {
    "url": "https://arxiv.org/abs/2305.19452",
    "title": "Bigger, Better, Faster: Human-level Atari with human-level efficiency",
    "published_date": "2023-05-30",
    "abstract": "We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.",
    "citation_count": 68,
    "summary": "The BBF agent achieves superhuman performance on the Atari 100K benchmark through scaled neural networks and sample-efficient design choices, prompting a reevaluation of sample efficiency benchmarks in reinforcement learning."
  },
  {
    "url": "https://www.lesswrong.com/posts/Z9P2m462wQ4qmH6uo/aspiration-based-q-learning",
    "author": "Clément Dumas, Jobst Heitzig",
    "title": "Aspiration-based Q-Learning",
    "published_date": "2023-10-27",
    "summary": "This internship project introduced ℵ-aspiring agents, a novel reinforcement learning approach inspired by satisficing, aiming for a specific expected reward (ℵ) rather than maximization. Preliminary results showed promise in simple environments but require further research for complex scenarios."
  }
]