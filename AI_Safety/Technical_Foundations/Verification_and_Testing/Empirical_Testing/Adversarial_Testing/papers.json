[
  {
    "url": "https://arxiv.org/pdf/2002.12078v1.pdf",
    "title": "Training Adversarial Agents to Exploit Weaknesses in Deep Control Policies",
    "published_date": "2020-02-27",
    "abstract": "Deep learning has become an increasingly common technique for various control problems, such as robotic arm manipulation, robot navigation, and autonomous vehicles. However, the downside of using deep neural networks to learn control policies is their opaque nature and the difficulties of validating their safety. As the networks used to obtain state-of-the-art results become increasingly deep and complex, the rules they have learned and how they operate become more challenging to understand. This presents an issue, since in safety-critical applications the safety of the control policy must be ensured to a high confidence level. In this paper, we propose an automated black box testing framework based on adversarial reinforcement learning. The technique uses an adversarial agent, whose goal is to degrade the performance of the target model under test. We test the approach on an autonomous vehicle problem, by training an adversarial reinforcement learning agent, which aims to cause a deep neural network-driven autonomous vehicle to collide. Two neural networks trained for autonomous driving are compared, and the results from the testing are used to compare the robustness of their learned control policies. We show that the proposed framework is able to find weaknesses in both control policies that were not evident during online testing and therefore, demonstrate a significant benefit over manual testing methods.",
    "citation_count": 42,
    "summary": "This paper presents an adversarial reinforcement learning framework for automated black-box testing of deep control policies, demonstrating its effectiveness in uncovering vulnerabilities in autonomous driving models that traditional testing methods miss. The framework trains an adversarial agent to exploit weaknesses and degrade the performance of the target control policy, revealing crucial safety concerns."
  },
  {
    "url": "https://www.alignmentforum.org/posts/m6poxWegJkp8LPpjw/can-generalized-adversarial-testing-enable-more-rigorous-llm",
    "author": "Stephen Casper",
    "title": "Can Generalized Adversarial Testing Enable More Rigorous LLM Safety Evals?",
    "published_date": "2024-07-30",
    "summary": "Current LLM safety evaluations often fall short because they primarily focus on input-space attacks, neglecting the potential for attackers to manipulate internal model weights or activations. \"Generalized\" adversarial testing, which considers these broader attack vectors, is proposed as a more robust method for assessing and mitigating LLM risks, even in black-box deployment scenarios."
  },
  {
    "title": "Towards Characterizing Adversarial Defects of Deep Learning Software from the Lens of Uncertainty",
    "abstract": "Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty. In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by ourmethod is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.",
    "published_date": "2020-04-24",
    "citation_count": 73,
    "url": "https://dl.acm.org/doi/10.1145/3377811.3380368",
    "summary": "This paper investigates the relationship between deep learning model uncertainty and adversarial examples, finding that existing methods miss many types of adversarial examples and benign inputs. The authors propose a novel automated testing technique to generate these uncommon examples, demonstrating its effectiveness in reducing the success rate of existing defenses."
  },
  {
    "url": "https://www.lesswrong.com/posts/zZbM5JdMs5uCtMkgs/robustness-of-contrast-consistent-search-to-adversarial",
    "author": "Nandi, i, Jamie Wright, Seamus_F, hugofry",
    "title": "Robustness of Contrast-Consistent Search to Adversarial Prompting",
    "published_date": "2023-11-01",
    "summary": "This research investigates how adversarial prompts affect large language model (LLM) accuracy, comparing their impact on LLMs and Contrast-Consistent Search (CCS). The study finds that while adversarial prompts significantly reduce LLM accuracy, CCS remains relatively robust, suggesting its potential use in detecting LLM falsehoods."
  },
  {
    "url": "https://arxiv.org/abs/1907.07174v1",
    "title": "Natural Adversarial Examples",
    "published_date": "2019-07-16",
    "abstract": "We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets' real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called IMAGENET-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called IMAGENET-O, which is the first out-of-distribution detection dataset created for ImageNet models. On IMAGENET-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on IMAGENET-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.",
    "citation_count": 1264,
    "summary": "The paper introduces IMAGENET-A and IMAGENET-O, two challenging datasets revealing widespread vulnerabilities in computer vision models, causing accuracy drops of up to 90% and near-random out-of-distribution detection performance; these datasets highlight the need for improved model architectures rather than solely relying on data augmentation or additional training data."
  },
  {
    "url": "https://arxiv.org/abs/2411.19567",
    "title": "AdvFuzz: Finding More Violations Caused by the EGO Vehicle in Simulation Testing by Adversarial NPC Vehicles",
    "published_date": "2024-11-29",
    "abstract": "Recently, there has been a significant escalation in both academic and industrial commitment towards the development of autonomous driving systems (ADSs). A number of simulation testing approaches have been proposed to generate diverse driving scenarios for ADS testing. However, scenarios generated by these previous approaches are static and lack interactions between the EGO vehicle and the NPC vehicles, resulting in a large amount of time on average to find violation scenarios. Besides, a large number of the violations they found are caused by aggressive behaviors of NPC vehicles, revealing none bugs of ADS. In this work, we propose the concept of adversarial NPC vehicles and introduce AdvFuzz, a novel simulation testing approach, to generate adversarial scenarios on main lanes (e.g., urban roads and highways). AdvFuzz allows NPC vehicles to dynamically interact with the EGO vehicle and regulates the behaviors of NPC vehicles, finding more violation scenarios caused by the EGO vehicle more quickly. We compare AdvFuzz with a random approach and three state-of-the-art scenario-based testing approaches. Our experiments demonstrate that AdvFuzz can generate 198.34% more violation scenarios compared to the other four approaches in 12 hours and increase the proportion of violations caused by the EGO vehicle to 87.04%, which is more than 7 times that of other approaches. Additionally, AdvFuzz is at least 92.21% faster in finding one violation caused by the EGO vehicle than that of the other approaches.",
    "summary": "AdvFuzz, a novel simulation testing approach for autonomous driving systems, uses adversarial non-player characters (NPCs) to dynamically interact with the ego vehicle, significantly increasing the speed and efficiency of finding ego-vehicle-caused violations compared to existing methods. This results in a substantially higher proportion of identified violations attributable to the ego vehicle's behavior."
  },
  {
    "url": "https://arxiv.org/abs/2410.05334",
    "title": "TA3: Testing Against Adversarial Attacks on Machine Learning Models",
    "published_date": "2024-10-06",
    "abstract": "Adversarial attacks are major threats to the deployment of machine learning (ML) models in many applications. Testing ML models against such attacks is becoming an essential step for evaluating and improving ML models. In this paper, we report the design and development of an interactive system for aiding the workflow of Testing Against Adversarial Attacks (TA3). In particular, with TA3, human-in-the-loop (HITL) enables human-steered attack simulation and visualization-assisted attack impact evaluation. While the current version of TA3 focuses on testing decision tree models against adversarial attacks based on the One Pixel Attack Method, it demonstrates the importance of HITL in ML testing and the potential application of HITL to the ML testing workflows for other types of ML models and other types of adversarial attacks.",
    "summary": "TA3 is an interactive system designed to test machine learning models, specifically decision trees, against adversarial attacks using a human-in-the-loop approach. It facilitates human-steered attack simulations and visualization-aided impact evaluation to improve model robustness."
  },
  {
    "url": "https://www.alignmentforum.org/posts/xj5nzResmDZDqLuLo/estimating-tail-risk-in-neural-networks",
    "author": "Jacob Hilton; Mark Xu",
    "title": "Estimating Tail Risk in Neural Networks",
    "published_date": "2024-09-13",
    "summary": "Current methods for assessing the risk of catastrophic AI behavior rely on identifying harmful inputs, which is computationally expensive and incomplete. The article proposes researching alternative methods for estimating the probability of tail events (catastrophic failures) that don't depend on finding specific harmful inputs."
  }
]