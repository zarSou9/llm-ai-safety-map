[
  {
    "url": "https://arxiv.org/abs/2402.16982",
    "title": "Synthesizing Tight Privacy and Accuracy Bounds via Weighted Model Counting",
    "published_date": "2024-02-26",
    "abstract": "Programmatically generating tight differential privacy (DP) bounds is a hard problem. Two core challenges are (1) finding expressive, compact, and efficient encodings of the distributions of DP algorithms, and (2) state space explosion stemming from the multiple quantifiers and relational properties of the DP definition. We address the first challenge by developing a method for tight privacy and accuracy bound synthesis using weighted model counting on binary decision diagrams, a state of the art technique from the artificial intelligence and automated reasoning communities for exactly computing probability distributions. We address the second challenge by developing a framework for leveraging inherent symmetries in DP algorithms. Our solution benefits from ongoing research in probabilistic programming languages, allowing us to succinctly and expressively represent different DP algorithms with approachable language syntax that can be used by non-experts. We provide a detailed case study of our solution on the binary randomized response algorithm. We also evaluate an implementation of our solution using the Dice probabilistic programming language for the randomized response and truncated geometric above threshold algorithms. We compare to prior work on exact DP verification using Markov chain probabilistic model checking and the decision procedure DiPC. Very few existing works consider mechanized analysis of accuracy guarantees for DP algorithms. We additionally provide a detailed analysis using our technique for finding tight accuracy bounds for DP algorithms.",
    "summary": "This paper presents a novel method for synthesizing tight differential privacy (DP) bounds using weighted model counting on binary decision diagrams, addressing challenges in encoding DP algorithm distributions and managing state space explosion through symmetry exploitation. The approach leverages probabilistic programming languages for succinct algorithm representation and offers both privacy and accuracy bound analysis, demonstrated on several DP algorithms."
  },
  {
    "url": "https://www.alignmentforum.org/posts/SyeQjjBoEC48MvnQC/formal-verification-heuristic-explanations-and-surprise",
    "author": "Jacob Hilton",
    "title": "Formal verification, heuristic explanations and surprise accounting",
    "published_date": "2024-06-25",
    "summary": "The article explores the limitations of using formal verification to guarantee the safety of neural networks, arguing that the strictness of proof methods makes it impractical for large models. Instead, it proposes \"heuristic explanations,\" a less rigorous but more scalable approach to understanding and quantifying a model's behavior, exemplified by \"surprise accounting.\""
  },
  {
    "url": "https://www.alignmentforum.org/posts/B2bg677TaS4cmDPzL/limitations-on-formal-verification-for-ai-safety",
    "author": "Andrew Dickson",
    "title": "Limitations on Formal Verification for AI Safety",
    "published_date": "2024-08-19",
    "summary": "The article argues that applying formal verification to guarantee AI safety is unrealistic in the near term. The complexity of the real world, particularly in areas like biology and physics, makes obtaining the necessary complete models and data for rigorous formal verification practically impossible, contrasting sharply with the over-optimistic claims of some AI safety researchers."
  },
  {
    "url": "https://www.alignmentforum.org/posts/LkECxpbjvSifPfjnb/towards-guaranteed-safe-ai-a-framework-for-ensuring-robust-1",
    "author": "Joar Skalse",
    "title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems",
    "published_date": "2024-05-17",
    "summary": "There is no article provided to summarize."
  },
  {
    "url": "https://www.lesswrong.com/posts/3P8WBwLyfPBEkbG3c/proveably-safe-self-driving-cars",
    "author": "Davidmanheim",
    "title": "Proveably Safe Self Driving Cars",
    "published_date": "2024-09-15",
    "summary": "The article argues that while fully \"provably safe AI\" is a long-term goal, near-term applications are achievable, using the example of autonomous vehicles. By building upon existing methods of formal verification for components like microkernels and integrating proven reliability estimates for sensors and mechanical systems, a path towards provably safer autonomous vehicles is proposed."
  },
  {
    "url": "https://www.lesswrong.com/posts/uSSPuttae5GHfsNQL/ai-compute-governance-verifying-ai-chip-location",
    "author": "Farhan",
    "title": "AI Compute governance: Verifying AI chip location",
    "published_date": "2024-10-12",
    "summary": "This article proposes a delay-based location verification mechanism for on-chip compute governance, using the speed of light as a constraint to verify AI chip locations. However, the mechanism's reliance on network latency introduces a significant false positive problem due to network congestion, necessitating further solutions."
  },
  {
    "url": "https://arxiv.org/pdf/2302.06082.pdf",
    "title": "Lower Bounds for Possibly Divergent Probabilistic Programs",
    "published_date": "2023-02-13",
    "abstract": "We present a new proof rule for verifying lower bounds on quantities of probabilistic programs. Our proof rule is not confined to almost-surely terminating programs -- as is the case for existing rules -- and can be used to establish non-trivial lower bounds on, e.g., termination probabilities and expected values, for possibly divergent probabilistic loops, e.g., the well-known three-dimensional random walk on a lattice.",
    "citation_count": 7,
    "summary": "This paper introduces a novel proof rule for verifying lower bounds on probabilistic program quantities, unlike existing rules, it accommodates possibly divergent programs. This allows establishing non-trivial lower bounds for properties like termination probability and expected values in programs with potentially infinite loops."
  },
  {
    "url": "https://arxiv.org/abs/2301.06136v1",
    "title": "Quantitative Verification With Neural Networks For Probabilistic Programs and Stochastic Systems",
    "published_date": "2023-01-15",
    "abstract": "We present a data-driven approach to the quantitative verification of probabilistic programs and stochastic dynamical models. Our approach leverages neural networks to compute tight and sound bounds for the probability that a stochastic process hits a target condition within finite time. This problem subsumes a variety of quantitative verification questions, from the reachability and safety analysis of discrete-time stochastic dynamical models, to the study of assertion-violation and termination analysis of probabilistic programs. We rely on neural networks to represent supermartingale certificates that yield such probability bounds, which we compute using a counterexample-guided inductive synthesis loop: we train the neural certificate while tightening the probability bound over samples of the state space using stochastic optimisation, and then we formally check the certificate's validity over every possible state using satisfiability modulo theories; if we receive a counterexample, we add it to our set of samples and repeat the loop until validity is confirmed. We demonstrate on a diverse set of benchmarks that, thanks to the expressive power of neural networks, our method yields smaller or comparable probability bounds than existing symbolic methods in all cases, and that our approach succeeds on models that are entirely beyond the reach of such alternative techniques.",
    "citation_count": 2,
    "summary": "This paper introduces a novel data-driven approach to quantitatively verify probabilistic programs and stochastic systems using neural networks to compute sound probability bounds for hitting target conditions. The method employs a counterexample-guided inductive synthesis loop, combining neural network training with formal verification to achieve tighter bounds than existing symbolic methods, particularly for complex models."
  }
]