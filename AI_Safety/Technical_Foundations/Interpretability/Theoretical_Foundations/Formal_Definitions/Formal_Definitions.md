### Mini Description

Mathematical frameworks for precisely defining interpretability and related concepts, including axiomatization of desirable properties and formal specification of explanation types.

### Description

Formal Definitions in AI interpretability focuses on establishing precise mathematical frameworks for what constitutes an explanation or interpretation of an AI system. This includes developing rigorous definitions for different types of interpretability (such as post-hoc vs. inherent interpretability), formalizing the requirements for valid explanations, and creating mathematical specifications for desired properties like completeness, consistency, and human-comprehensibility.

A key challenge in this area is bridging the gap between intuitive notions of interpretability and formal mathematical definitions. Researchers work to develop axioms that capture essential properties of interpretability while remaining mathematically tractable. This includes formal treatments of concepts like feature attribution, counterfactual explanations, and model simplification, as well as theoretical frameworks for comparing different approaches to interpretation.

Current research emphasizes the need for definitions that scale with model complexity and remain meaningful for advanced AI systems. This includes work on compositional definitions that can handle hierarchical explanations, formal specifications of interpretation validity across different levels of abstraction, and mathematical frameworks for reasoning about the completeness and faithfulness of interpretations. A particular focus is developing definitions that can accommodate both local explanations of specific decisions and global interpretations of model behavior.

### Order

1. Axiomatization
2. Explanation_Types
3. Abstraction_Levels
4. Validity_Criteria
5. Semantic_Foundations
