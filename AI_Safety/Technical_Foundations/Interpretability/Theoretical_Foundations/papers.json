[
  {
    "url": "https://arxiv.org/abs/2406.10529",
    "title": "A Theory of Interpretable Approximations",
    "published_date": "2024-06-15",
    "abstract": "Can a deep neural network be approximated by a small decision tree based on simple features? This question and its variants are behind the growing demand for machine learning models that are *interpretable* by humans. In this work we study such questions by introducing *interpretable approximations*, a notion that captures the idea of approximating a target concept $c$ by a small aggregation of concepts from some base class $\\mathcal{H}$. In particular, we consider the approximation of a binary concept $c$ by decision trees based on a simple class $\\mathcal{H}$ (e.g., of bounded VC dimension), and use the tree depth as a measure of complexity. Our primary contribution is the following remarkable trichotomy. For any given pair of $\\mathcal{H}$ and $c$, exactly one of these cases holds: (i) $c$ cannot be approximated by $\\mathcal{H}$ with arbitrary accuracy; (ii) $c$ can be approximated by $\\mathcal{H}$ with arbitrary accuracy, but there exists no universal rate that bounds the complexity of the approximations as a function of the accuracy; or (iii) there exists a constant $\\kappa$ that depends only on $\\mathcal{H}$ and $c$ such that, for *any* data distribution and *any* desired accuracy level, $c$ can be approximated by $\\mathcal{H}$ with a complexity not exceeding $\\kappa$. This taxonomy stands in stark contrast to the landscape of supervised classification, which offers a complex array of distribution-free and universally learnable scenarios. We show that, in the case of interpretable approximations, even a slightly nontrivial a-priori guarantee on the complexity of approximations implies approximations with constant (distribution-free and accuracy-free) complexity. We extend our trichotomy to classes $\\mathcal{H}$ of unbounded VC dimension and give characterizations of interpretability based on the algebra generated by $\\mathcal{H}$.",
    "summary": "This paper introduces \"interpretable approximations,\" analyzing the approximation of a target concept by a small decision tree using simple features, and proves a trichotomy theorem classifying the achievable approximation complexity into three distinct cases based on the relationship between the target concept and the base feature class."
  },
  {
    "url": "https://arxiv.org/pdf/1707.03886v1.pdf",
    "title": "A Formal Framework to Characterize Interpretability of Procedures",
    "published_date": "2017-06-11",
    "abstract": "We provide a novel notion of what it means to be interpretable, looking past the usual association with human understanding. Our key insight is that interpretability is not an absolute concept and so we define it relative to a target model, which may or may not be a human. We define a framework that allows for comparing interpretable procedures by linking it to important practical aspects such as accuracy and robustness. We characterize many of the current state-of-the-art interpretable methods in our framework portraying its general applicability.",
    "citation_count": 19,
    "summary": "This paper introduces a formal framework for characterizing interpretability, defining it relative to a target model rather than solely human understanding, and enabling comparisons of interpretable procedures based on accuracy and robustness. The framework's generality is demonstrated by characterizing several existing interpretable methods."
  },
  {
    "url": "https://arxiv.org/abs/2010.13764v1",
    "title": "Enforcing Interpretability and its Statistical Impacts: Trade-offs between Accuracy and Interpretability",
    "published_date": "2020-10-26",
    "abstract": "To date, there has been no formal study of the statistical cost of interpretability in machine learning. As such, the discourse around potential trade-offs is often informal and misconceptions abound. In this work, we aim to initiate a formal study of these trade-offs. A seemingly insurmountable roadblock is the lack of any agreed upon definition of interpretability. Instead, we propose a shift in perspective. Rather than attempt to define interpretability, we propose to model the \\emph{act} of \\emph{enforcing} interpretability. As a starting point, we focus on the setting of empirical risk minimization for binary classification, and view interpretability as a constraint placed on learning. That is, we assume we are given a subset of hypothesis that are deemed to be interpretable, possibly depending on the data distribution and other aspects of the context. We then model the act of enforcing interpretability as that of performing empirical risk minimization over the set of interpretable hypotheses. This model allows us to reason about the statistical implications of enforcing interpretability, using known results in statistical learning theory. Focusing on accuracy, we perform a case analysis, explaining why one may or may not observe a trade-off between accuracy and interpretability when the restriction to interpretable classifiers does or does not come at the cost of some excess statistical risk. We close with some worked examples and some open problems, which we hope will spur further theoretical development around the tradeoffs involved in interpretability.",
    "citation_count": 34,
    "summary": "This paper formally investigates the statistical trade-off between accuracy and interpretability in machine learning by modeling interpretability enforcement as a constraint on empirical risk minimization. The authors analyze this model to determine when restricting to interpretable classifiers leads to excess statistical risk, thus impacting accuracy."
  },
  {
    "url": "https://arxiv.org/pdf/1702.08608v2.pdf",
    "title": "Towards A Rigorous Science of Interpretable Machine Learning",
    "published_date": "2017-02-28",
    "abstract": "As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",
    "citation_count": 3390,
    "summary": "This paper argues for a more rigorous scientific approach to interpretable machine learning, advocating for clear definitions of interpretability, identifying when it's necessary, and proposing a taxonomy for evaluating its effectiveness. It highlights the current lack of consensus and outlines key open questions in the field."
  },
  {
    "url": "https://www.alignmentforum.org/posts/7tSthxSgnNxbt4Hk6/what-s-in-the-box-towards-interpretability-by-distinguishing-1",
    "author": "Joshua Clancy",
    "title": "What's in the box?! â€“ Towards interpretability by distinguishing niches of value within neural networks.",
    "published_date": "2024-02-29",
    "summary": "This paper proposes a novel theoretical model of neural network internal representations using economic and information theory principles. The model, derived from both top-down and bottom-up analyses, predicts neuron roles based on context and internal structure, although practical validation requires further software development."
  },
  {
    "url": "https://arxiv.org/abs/2311.11491",
    "title": "On the Relationship Between Interpretability and Explainability in Machine Learning",
    "published_date": "2023-11-20",
    "abstract": "Interpretability and explainability have gained more and more attention in the field of machine learning as they are crucial when it comes to high-stakes decisions and troubleshooting. Since both provide information about predictors and their decision process, they are often seen as two independent means for one single end. This view has led to a dichotomous literature: explainability techniques designed for complex black-box models, or interpretable approaches ignoring the many explainability tools. In this position paper, we challenge the common idea that interpretability and explainability are substitutes for one another by listing their principal shortcomings and discussing how both of them mitigate the drawbacks of the other. In doing so, we call for a new perspective on interpretability and explainability, and works targeting both topics simultaneously, leveraging each of their respective assets.",
    "summary": "This paper argues against viewing interpretability and explainability in machine learning as interchangeable concepts, instead highlighting their complementary strengths and advocating for research that integrates both to overcome individual limitations. It challenges the existing dichotomous research focusing on either interpretable models or explainability techniques for black boxes."
  },
  {
    "url": "https://www.alignmentforum.org/tag/interpretability-ml-and-ai",
    "author": "Lee Sharkey, Dan Braun, Beren Millidge",
    "title": "Interpretability (ML & AI) - AI Alignment Forum",
    "published_date": "2023-04-30",
    "summary": "Machine learning models often lack transparency, hindering bias detection and understanding of their decision-making processes. Mechanistic interpretability, a key subfield, aims to understand how these models function internally, contrasting with methods focused on attributing outputs to specific input features."
  },
  {
    "url": "https://www.lesswrong.com/posts/rEMpTapcAzjTiSckf/on-developing-a-mathematical-theory-of-interpretability",
    "author": "Spencer Becker-Kahn",
    "title": "On Developing a Mathematical Theory of Interpretability",
    "published_date": "2023-02-09",
    "summary": "Developing robust mathematical frameworks for interpreting deep learning models is crucial for ensuring their safe application, but this process is slow and challenging due to the rapid evolution of deep learning practices. The author argues that a more abstract, mathematically driven approach, similar to algebraic topology, may ultimately yield greater insights and reliability than a purely reactive, application-driven approach."
  },
  {
    "url": "https://www.lesswrong.com/posts/sTe78dNJDGywu9Dz6/solving-the-mechanistic-interpretability-challenges-eis-vii",
    "author": "StefanHex, Marius Hobbhahn",
    "title": "Solving the Mechanistic Interpretability challenges: EIS VII Challenge 1",
    "published_date": "2023-05-09",
    "summary": "Researchers solved a machine learning interpretability challenge by reverse-engineering a CNN trained on a binary MNIST classification task. They determined the model classified images based on their similarity to a template \"1\" and its inverse, using a simple dot-product similarity measure."
  },
  {
    "url": "https://www.lesswrong.com/posts/AaABQpuoNC8gpHf2n/a-barebones-guide-to-mechanistic-interpretability",
    "author": "Neel Nanda",
    "title": "A Barebones Guide to Mechanistic Interpretability Prerequisites",
    "published_date": "2022-10-24",
    "summary": "This guide outlines the essential math, coding, and machine learning skills needed to begin research in mechanistic interpretability, emphasizing that the required knowledge is less extensive than commonly perceived and providing specific learning resources. It focuses on linear algebra, probability, calculus, Python/NumPy/PyTorch programming, and a deep understanding of transformer architectures."
  }
]