[
  {
    "url": "https://arxiv.org/abs/2409.06594",
    "title": "How to Verify Any (Reasonable) Distribution Property: Computationally Sound Argument Systems for Distributions",
    "published_date": "2024-09-10",
    "abstract": "As statistical analyses become more central to science, industry and society, there is a growing need to ensure correctness of their results. Approximate correctness can be verified by replicating the entire analysis, but can we verify without replication? Building on a recent line of work, we study proof-systems that allow a probabilistic verifier to ascertain that the results of an analysis are approximately correct, while drawing fewer samples and using less computational resources than would be needed to replicate the analysis. We focus on distribution testing problems: verifying that an unknown distribution is close to having a claimed property. Our main contribution is a interactive protocol between a verifier and an untrusted prover, which can be used to verify any distribution property that can be decided in polynomial time given a full and explicit description of the distribution. If the distribution is at statistical distance $\\varepsilon$ from having the property, then the verifier rejects with high probability. This soundness property holds against any polynomial-time strategy that a cheating prover might follow, assuming the existence of collision-resistant hash functions (a standard assumption in cryptography). For distributions over a domain of size $N$, the protocol consists of $4$ messages and the communication complexity and verifier runtime are roughly $\\widetilde{O}\\left(\\sqrt{N} / \\varepsilon^2 \\right)$. The verifier's sample complexity is $\\widetilde{O}\\left(\\sqrt{N} / \\varepsilon^2 \\right)$, and this is optimal up to $\\polylog(N)$ factors (for any protocol, regardless of its communication complexity). Even for simple properties, approximately deciding whether an unknown distribution has the property can require quasi-linear sample complexity and running time. For any such property, our protocol provides a quadratic speedup over replicating the analysis.",
    "summary": "This paper presents an interactive proof system enabling efficient verification of approximate distribution properties, achieving a quadratic speedup over replication by leveraging a computationally sound argument system. The protocol verifies any polynomially decidable property with optimal sample complexity, assuming collision-resistant hash functions."
  },
  {
    "url": "https://www.alignmentforum.org/posts/SyeQjjBoEC48MvnQC/formal-verification-heuristic-explanations-and-surprise",
    "author": "Jacob Hilton",
    "title": "Formal verification, heuristic explanations and surprise accounting",
    "published_date": "2024-06-25",
    "summary": "The article discusses the challenges of formally verifying neural networks, arguing that rigorous proofs are impractical for large models due to the complexity of accounting for all possible interactions. Instead, the authors propose \"heuristic explanations,\" a less formal approach that quantifies the quality of an explanation using \"surprise accounting\" to better understand model behavior."
  },
  {
    "url": "https://www.alignmentforum.org/posts/LkECxpbjvSifPfjnb/towards-guaranteed-safe-ai-a-framework-for-ensuring-robust-1",
    "author": "Joar Skalse",
    "title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems",
    "published_date": "2024-05-17",
    "summary": "There is no article provided to summarize."
  },
  {
    "url": "https://www.alignmentforum.org/posts/zy2AECRAi8Nuu5XMk/time-complexity-for-deterministic-string-machines",
    "author": "alcatal",
    "title": "Time complexity for deterministic string machines",
    "published_date": "2024-04-21",
    "summary": "This paper introduces \"filtered transducers,\" operating on categories enriched over filtered sets, to address the lack of representation-independent complexity bounds in the existing String Machines framework. By restricting to finite-state filtered transducers, the authors prove constraints on time complexity growth and expressivity."
  },
  {
    "url": "https://www.alignmentforum.org/posts/B2bg677TaS4cmDPzL/limitations-on-formal-verification-for-ai-safety",
    "author": "Andrew Dickson",
    "title": "Limitations on Formal Verification for AI Safety",
    "published_date": "2024-08-19",
    "summary": "The article argues that applying formal verification to ensure AI safety is currently impractical due to the inherent complexity of the real world and the limitations of modeling physical systems. The author expresses skepticism towards claims that formal verification can provide strong, near-term guarantees against major AI threats."
  },
  {
    "url": "https://arxiv.org/pdf/2301.04709.pdf",
    "title": "Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability",
    "published_date": "2023-01-11",
    "abstract": "Causal abstraction provides a theoretical foundation for mechanistic interpretability, the field concerned with providing intelligible algorithms that are faithful simplifications of the known, but opaque low-level details of black box AI models. Our contributions are (1) generalizing the theory of causal abstraction from mechanism replacement (i.e., hard and soft interventions) to arbitrary mechanism transformation (i.e., functionals from old mechanisms to new mechanisms), (2) providing a flexible, yet precise formalization for the core concepts of modular features, polysemantic neurons, and graded faithfulness, and (3) unifying a variety of mechanistic interpretability methodologies in the common language of causal abstraction, namely activation and path patching, causal mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept erasure, sparse autoencoders, differential binary masking, distributed alignment search, and activation steering.",
    "citation_count": 39,
    "summary": "This paper introduces a generalized theory of causal abstraction, formally defining key concepts in mechanistic interpretability and unifying diverse methodologies under a common framework of mechanism transformation. This framework allows for intelligible, faithful simplifications of opaque AI models."
  },
  {
    "url": "https://www.alignmentforum.org/posts/rEMpTapcAzjTiSckf/on-developing-a-mathematical-theory-of-interpretability",
    "author": "Spencer Becker-Kahn",
    "title": "On Developing a Mathematical Theory of Interpretability",
    "published_date": "2023-02-09",
    "summary": "Developing a robust mathematical framework for interpreting deep learning models is crucial for ensuring their safe and reliable application, but this is a slow and challenging process hampered by the rapid pace of technological advancement. The author argues that while initially reactive, a more proactive, mathematically driven approach, inspired by fields like algebraic topology, could ultimately yield significant insights and predictive power."
  },
  {
    "url": "https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research",
    "author": "Roman Leventov",
    "title": "A multi-disciplinary view on AI safety research",
    "published_date": "2023-02-08",
    "summary": "The article advocates for a multidisciplinary approach to AI safety research, arguing that achieving safe artificial general intelligence (AGI) requires integrating diverse fields like cognitive science, social sciences, and engineering, and prioritizing the design of \"civilisational intelligence\" rather than solely focusing on technical alignment solutions."
  }
]