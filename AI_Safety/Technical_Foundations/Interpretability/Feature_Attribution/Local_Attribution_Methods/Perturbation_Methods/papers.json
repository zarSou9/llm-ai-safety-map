[
  {
    "url": "https://www.alignmentforum.org/posts/93nKtsDL6YY5fRbQv/case-studies-in-reverse-engineering-sparse-autoencoder",
    "author": "Jacob Dunefsky, Philippe Chlenski, Senthooran Rajamanoharan, Neel Nanda",
    "title": "Case Studies in Reverse-Engineering Sparse Autoencoder Features by Using MLP Linearization",
    "published_date": "2024-01-14"
  },
  {
    "url": "https://www.alignmentforum.org/posts/DtdzGwFh9dCfsekZZ/sparse-autoencoders-work-on-attention-layer-outputs",
    "author": "Connor Kissane, robertzk, Arthur Conmy, Neel Nanda",
    "title": "Sparse Autoencoders Work on Attention Layer Outputs",
    "published_date": "2024-01-16"
  },
  {
    "url": "https://www.alignmentforum.org/posts/NMLq8yoTecAF44KX9/sae-probing-what-is-it-good-for-absolutely-something",
    "author": "Subhash Kantamneni; JoshEngels; Senthooran Rajamanoharan; Neel Nanda",
    "title": "SAE Probing: What is it good for? Absolutely something!",
    "published_date": "2024-11-01"
  },
  {
    "url": "https://www.alignmentforum.org/posts/cLfsabkCPtieJ5LoK/investigating-bias-representations-in-llms-via-activation",
    "author": "DawnLu",
    "title": "Investigating Bias Representations in LLMs via Activation Steering",
    "published_date": "2024-01-15"
  },
  {
    "url": "https://www.alignmentforum.org/posts/8ev6coxChSWcxCDy8/self-explaining-sae-features",
    "author": "Dmitrii Kharlapenko; Neverix; Neel Nanda; Arthur Conmy",
    "title": "Self-explaining SAE features",
    "published_date": "2024-08-05"
  },
  {
    "url": "https://arxiv.org/abs/2306.07462",
    "title": "On the Robustness of Removal-Based Feature Attributions",
    "published_date": "2023-06-12",
    "abstract": "To explain predictions made by complex machine learning models, many feature attribution methods have been developed that assign importance scores to input features. Some recent work challenges the robustness of these methods by showing that they are sensitive to input and model perturbations, while other work addresses this issue by proposing robust attribution methods. However, previous work on attribution robustness has focused primarily on gradient-based feature attributions, whereas the robustness of removal-based attribution methods is not currently well understood. To bridge this gap, we theoretically characterize the robustness properties of removal-based feature attributions. Specifically, we provide a unified analysis of such methods and derive upper bounds for the difference between intact and perturbed attributions, under settings of both input and model perturbations. Our empirical results on synthetic and real-world data validate our theoretical results and demonstrate their practical implications, including the ability to increase attribution robustness by improving the model's Lipschitz regularity.",
    "citation_count": 10
  }
]