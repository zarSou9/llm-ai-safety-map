[
  {
    "url": "https://arxiv.org/pdf/2104.12437.pdf",
    "title": "Towards Rigorous Interpretations: a Formalisation of Feature Attribution",
    "published_date": "2021-04-26",
    "abstract": "Feature attribution is often loosely presented as the process of selecting a subset of relevant features as a rationale of a prediction. Task-dependent by nature, precise definitions of\"relevance\"encountered in the literature are however not always consistent. This lack of clarity stems from the fact that we usually do not have access to any notion of ground-truth attribution and from a more general debate on what good interpretations are. In this paper we propose to formalise feature selection/attribution based on the concept of relaxed functional dependence. In particular, we extend our notions to the instance-wise setting and derive necessary properties for candidate selection solutions, while leaving room for task-dependence. By computing ground-truth attributions on synthetic datasets, we evaluate many state-of-the-art attribution methods and show that, even when optimised, some fail to verify the proposed properties and provide wrong solutions.",
    "citation_count": 15,
    "summary": "This paper formalizes feature attribution using relaxed functional dependence, addressing the inconsistent definitions of \"relevance\" in existing literature by establishing necessary properties for selecting features and evaluating the performance of state-of-the-art methods against these properties. The authors demonstrate that even optimized methods can fail to satisfy these properties, highlighting the need for rigorous formalization."
  },
  {
    "url": "https://arxiv.org/abs/2202.00449?context=cs.CV",
    "title": "A Consistent and Efficient Evaluation Strategy for Attribution Methods",
    "published_date": "2022-02-01",
    "abstract": "With a variety of local feature attribution methods being proposed in recent years, follow-up work suggested several evaluation strategies. To assess the attribution quality across different attribution techniques, the most popular among these evaluation strategies in the image domain use pixel perturbations. However, recent advances discovered that different evaluation strategies produce conflicting rankings of attribution methods and can be prohibitively expensive to compute. In this work, we present an information-theoretic analysis of evaluation strategies based on pixel perturbations. Our findings reveal that the results are strongly affected by information leakage through the shape of the removed pixels as opposed to their actual values. Using our theoretical insights, we propose a novel evaluation framework termed Remove and Debias (ROAD) which offers two contributions: First, it mitigates the impact of the confounders, which entails higher consistency among evaluation strategies. Second, ROAD does not require the computationally expensive retraining step and saves up to 99% in computational costs compared to the state-of-the-art. We release our source code at https://github.com/tleemann/road_evaluation.",
    "citation_count": 78,
    "summary": "The paper introduces ROAD, a novel evaluation framework for image attribution methods that addresses inconsistencies and high computational costs of existing pixel-perturbation based strategies by mitigating information leakage from pixel shape and eliminating retraining. This leads to more consistent rankings of attribution methods and significant computational savings."
  },
  {
    "url": "https://arxiv.org/abs/2310.06514",
    "title": "AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments",
    "published_date": "2023-10-10",
    "abstract": "Feature attribution explains neural network outputs by identifying relevant input features. The attribution has to be faithful, meaning that the attributed features must mirror the input features that influence the output. One recent trend to test faithfulness is to fit a model on designed data with known relevant features and then compare attributions with ground truth input features.This idea assumes that the model learns to use all and only these designed features, for which there is no guarantee. In this paper, we solve this issue by designing the network and manually setting its weights, along with designing data. The setup, AttributionLab, serves as a sanity check for faithfulness: If an attribution method is not faithful in a controlled environment, it can be unreliable in the wild. The environment is also a laboratory for controlled experiments by which we can analyze attribution methods and suggest improvements.",
    "citation_count": 1,
    "summary": "AttributionLab is a controlled environment for evaluating the faithfulness of feature attribution methods in neural networks by manually designing both the network weights and input data, providing a rigorous test of attribution reliability. Its controlled nature allows for analysis and improvement of attribution methods."
  },
  {
    "url": "https://arxiv.org/abs/2404.11330",
    "title": "Toward Understanding the Disagreement Problem in Neural Network Feature Attribution",
    "published_date": "2024-04-17",
    "abstract": "In recent years, neural networks have demonstrated their remarkable ability to discern intricate patterns and relationships from raw data. However, understanding the inner workings of these black box models remains challenging, yet crucial for high-stake decisions. Among the prominent approaches for explaining these black boxes are feature attribution methods, which assign relevance or contribution scores to each input variable for a model prediction. Despite the plethora of proposed techniques, ranging from gradient-based to backpropagation-based methods, a significant debate persists about which method to use. Various evaluation metrics have been proposed to assess the trustworthiness or robustness of their results. However, current research highlights disagreement among state-of-the-art methods in their explanations. Our work addresses this confusion by investigating the explanations' fundamental and distributional behavior. Additionally, through a comprehensive simulation study, we illustrate the impact of common scaling and encoding techniques on the explanation quality, assess their efficacy across different effect sizes, and demonstrate the origin of inconsistency in rank-based evaluation metrics.",
    "citation_count": 2,
    "summary": "The paper investigates the disagreement among neural network feature attribution methods, analyzing the underlying causes of this inconsistency through a simulation study that examines the influence of data scaling, encoding, and effect size on explanation quality and rank-based evaluation metrics. This work aims to clarify the selection of appropriate feature attribution methods by understanding their fundamental behavior and limitations."
  },
  {
    "url": "https://arxiv.org/pdf/2105.13841v1.pdf",
    "title": "A General Taylor Framework for Unifying and Revisiting Attribution Methods",
    "published_date": "2021-05-28",
    "abstract": "Attribution methods provide an insight into the decision-making process of machine learning models, especially deep neural networks, by assigning contribution scores to each individual feature. However, the attribution problem has not been well-defined, which lacks a unified guideline to the contribution assignment process. Furthermore, existing attribution methods often built upon various empirical intuitions and heuristics. There still lacks a general theoretical framework that not only can offer a good description of the attribution problem, but also can be applied to unifying and revisiting existing attribution methods. To bridge the gap, in this paper, we propose a Taylor attribution framework, which models the attribution problem as how to decide individual payoffs in a coalition. Then, we reformulate fourteen mainstream attribution methods into the Taylor framework and analyze these attribution methods in terms of rationale, fidelity, and limitation in the framework. Moreover, we establish three principles for a good attribution in the Taylor attribution framework, i.e., low approximation error, correct Taylor contribution assignment, and unbiased baseline selection. Finally, we empirically validate the Taylor reformulations and reveal a positive correlation between the attribution performance and the number of principles followed by the attribution method via benchmarking on real-world datasets.",
    "citation_count": 2,
    "summary": "This paper introduces a Taylor attribution framework that unifies and analyzes fourteen existing machine learning attribution methods by modeling the attribution problem as a coalition payoff game. The framework establishes three principles for effective attribution and empirically validates its reformulations, revealing a correlation between performance and adherence to these principles."
  },
  {
    "url": "https://arxiv.org/auth/show-endorsers/2104.14403",
    "title": "Do Feature Attribution Methods Correctly Attribute Features?",
    "published_date": "2021-04-27",
    "abstract": "Feature attribution methods are popular in interpretable machine learning. These methods compute the attribution of each input feature to represent its importance, but there is no consensus on the definition of \"attribution\", leading to many competing methods with little systematic evaluation, complicated in particular by the lack of ground truth attribution. To address this, we propose a dataset modification procedure to induce such ground truth. Using this procedure, we evaluate three common methods: saliency maps, rationales, and attentions. We identify several deficiencies and add new perspectives to the growing body of evidence questioning the correctness and reliability of these methods applied on datasets in the wild. We further discuss possible avenues for remedy and recommend new attribution methods to be tested against ground truth before deployment. The code and appendix are available at https://yilunzhou.github.io/feature-attribution-evaluation/.",
    "citation_count": 126,
    "summary": "This paper evaluates the accuracy of three common feature attribution methods (saliency maps, rationales, and attentions) using a novel ground truth generation procedure, revealing significant deficiencies and questioning their reliability in real-world applications. The authors propose future research directions focused on developing and evaluating more accurate attribution methods."
  },
  {
    "url": "https://www.lesswrong.com/posts/zbebxYCqsryPALh8C/matryoshka-sparse-autoencoders",
    "author": "Noa Nabeshima",
    "title": "Matryoshka Sparse Autoencoders",
    "published_date": "2024-12-14",
    "summary": "This paper introduces Matryoshka Sparse Autoencoders (SAEs), a training method addressing feature absorption and fragmentation issues in larger SAEs. Matryoshka SAEs, trained using a sum of losses on random prefixes of latent layers, preserve abstract features better than traditional SAEs while maintaining the benefits of larger models."
  },
  {
    "url": "https://arxiv.org/abs/2307.03380v2",
    "title": "On Formal Feature Attribution and Its Approximation",
    "published_date": "2023-07-07",
    "abstract": "Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution problem. Additionally, a formal explanation despite being formally sound is typically quite large, which hampers its applicability in practical settings. Motivated by the above, this paper proposes a way to apply the apparatus of formal XAI to the case of feature attribution based on formal explanation enumeration. Formal feature attribution (FFA) is argued to be advantageous over the existing methods, both formal and non-formal. Given the practical complexity of the problem, the paper then proposes an efficient technique for approximating exact FFA. Finally, it offers experimental evidence of the effectiveness of the proposed approximate FFA in comparison to the existing feature attribution algorithms not only in terms of feature importance and but also in terms of their relative order.",
    "citation_count": 6,
    "summary": "This paper introduces formal feature attribution (FFA), a novel approach to explainable AI that addresses limitations of existing methods by leveraging formal explanations, and proposes an efficient approximation technique to overcome its scalability challenges. The authors demonstrate its effectiveness compared to existing feature attribution algorithms."
  }
]