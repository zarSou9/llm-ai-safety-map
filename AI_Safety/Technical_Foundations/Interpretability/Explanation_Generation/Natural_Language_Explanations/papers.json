[
  {
    "url": "https://arxiv.org/pdf/2011.05268v3.pdf",
    "title": "Towards Interpretable Natural Language Understanding with Explanations as Latent Variables",
    "published_date": "2020-10-24",
    "abstract": "Recently generating natural language explanations has shown very promising results in not only offering interpretable explanations but also providing additional information and supervision for prediction. However, existing approaches usually require a large set of human annotated explanations for training while collecting a large set of explanations is not only time consuming but also expensive. In this paper, we develop a general framework for interpretable natural language understanding that requires only a small set of human annotated explanations for training. Our framework treats natural language explanations as latent variables that model the underlying reasoning process of a neural model. We develop a variational EM framework for optimization where an explanation generation module and an explanation-augmented prediction module are alternatively optimized and mutually enhance each other. Moreover, we further propose an explanation-based self-training method under this framework for semi-supervised learning. It alternates between assigning pseudo-labels to unlabeled data and generating new explanations to iteratively improve each other. Experiments on two natural language understanding tasks demonstrate that our framework can not only make effective predictions in both supervised and semi-supervised settings, but also generate good natural language explanation.",
    "citation_count": 38,
    "summary": "This paper introduces a framework for interpretable natural language understanding that uses natural language explanations as latent variables, requiring only a small set of annotated explanations through a variational EM framework and an explanation-based self-training method for semi-supervised learning. The framework improves both prediction accuracy and explanation quality in supervised and semi-supervised settings."
  },
  {
    "url": "https://arxiv.org/abs/1910.03065v1",
    "title": "Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations",
    "published_date": "2019-10-07",
    "abstract": "To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ”Because there is a dog in the image.” and ”Because there is no dog in the [same] image.”, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.",
    "citation_count": 90,
    "summary": "This paper demonstrates that neural models generating natural language explanations for AI predictions are vulnerable to producing contradictory explanations, revealing weaknesses in either the model's decision-making or explanation generation. The authors introduce an adversarial framework to identify and expose this inconsistency, addressing limitations in existing sequence-to-sequence attack methods."
  },
  {
    "url": "https://arxiv.org/abs/2311.18062",
    "title": "Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation",
    "published_date": "2023-11-29",
    "abstract": "Intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. It is vital that these agents are able to explain the reasoning behind their decisions to human counterparts; however, their behavior is often produced by uninterpretable models such as deep neural networks. We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, thus making our method independent from the underlying model's representation. For such models, we first learn a behavior representation and subsequently use it to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. We evaluate our method in a multi-agent search-and-rescue environment and demonstrate the effectiveness of our explanations for agents executing various behaviors. Through user studies and empirical experiments, we show that our approach generates explanations as helpful as those produced by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.",
    "summary": "This paper introduces a method for generating natural language explanations of agent behavior using only observed states and actions, leveraging large language models to produce helpful and accurate explanations independent of the agent's internal model. User studies show these explanations are comparable to those from human experts and facilitate beneficial interaction."
  },
  {
    "url": "https://arxiv.org/pdf/2305.12995.pdf",
    "title": "MaNtLE: Model-agnostic Natural Language Explainer",
    "published_date": "2023-05-22",
    "abstract": "Understanding the internal reasoning behind the predictions of machine learning systems is increasingly vital, given their rising adoption and acceptance. While previous approaches, such as LIME, generate algorithmic explanations by attributing importance to input features for individual examples, recent research indicates that practitioners prefer examining language explanations that explain sub-groups of examples. In this paper, we introduce MaNtLE, a model-agnostic natural language explainer that analyzes multiple classifier predictions and generates faithful natural language explanations of classifier rationale for structured classification tasks. MaNtLE uses multi-task training on thousands of synthetic classification tasks to generate faithful explanations. Simulated user studies indicate that, on average, MaNtLE-generated explanations are at least 11% more faithful compared to LIME and Anchors explanations across three tasks. Human evaluations demonstrate that users can better predict model behavior using explanations from MaNtLE compared to other techniques",
    "citation_count": 2,
    "summary": "MaNtLE is a model-agnostic natural language explainer that generates faithful explanations for structured classification tasks by training on synthetic data, outperforming existing methods like LIME and Anchors in both faithfulness and user understanding of model behavior."
  },
  {
    "url": "https://arxiv.org/abs/2212.04231",
    "title": "Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations",
    "published_date": "2022-12-08",
    "abstract": "Natural language explanations promise to offer intuitively understandable explanations of a neural network's decision process in complex vision-language tasks, as pursued in recent VL-NLE models. While current models offer impressive performance on task accuracy and explanation plausibility, they suffer from a range of issues: Some models feature a modular design where the explanation generation module is poorly integrated with a separate module for task-answer prediction, employ backbone models trained on limited sets of tasks, or incorporate ad hoc solutions to increase performance on single datasets. We propose to evade these limitations by applying recent advances in large-scale multi-task pretraining of generative Transformer models to the problem of VL-NLE tasks. Our approach outperforms recent models by a large margin, with human annotators preferring the generated explanations over the ground truth in two out of three evaluated datasets. As a novel challenge in VL-NLE research, we propose the problem of multi-task VL-NLE and show that jointly training on multiple tasks can increase the explanation quality. We discuss the ethical implications of high-quality NLE generation and other issues in recent VL-NLE research.",
    "citation_count": 5,
    "summary": "This paper introduces a novel approach to vision-language natural language explanation (VL-NLE) using multi-task pretrained generative Transformer models, significantly improving explanation quality and surpassing existing methods; the authors also introduce multi-task VL-NLE as a new research challenge and discuss ethical implications."
  },
  {
    "url": "https://arxiv.org/pdf/2012.09157v1.pdf",
    "title": "LIREx: Augmenting Language Inference with Relevant Explanation",
    "published_date": "2020-12-16",
    "abstract": "Natural language explanations (NLEs) are a special form of data annotation in which annotators identify rationales (most significant text tokens) when assigning labels to data instances, and write out explanations for the labels in natural language based on the rationales. NLEs have been shown to capture human reasoning better, but not as beneficial for natural language inference (NLI). In this paper, we analyze two primary flaws in the way NLEs are currently used to train explanation generators for language inference tasks. We find that the explanation generators do not take into account the variability inherent in human explanation of labels, and that the current explanation generation models generate spurious explanations. To overcome these limitations, we propose a novel framework, LIREx, that incorporates both a rationale-enabled explanation generator and an instance selector to select only relevant, plausible NLEs to augment NLI models. When evaluated on the standardized SNLI data set, LIREx achieved an accuracy of 91.87%, an improvement of 0.32 over the baseline and matching the best-reported performance on the data set. It also achieves significantly better performance than previous studies when transferred to the out-of-domain MultiNLI data set. Qualitative analysis shows that LIREx generates flexible, faithful, and relevant NLEs that allow the model to be more robust to spurious explanations. The code is available at https://github.com/zhaoxy92/LIREx.",
    "citation_count": 35,
    "summary": "LIREx improves natural language inference (NLI) by addressing flaws in existing explanation generation methods, specifically by incorporating a rationale-enabled generator and an instance selector to filter out spurious explanations, resulting in a model that achieves state-of-the-art accuracy on SNLI and strong performance on MultiNLI."
  },
  {
    "url": "https://arxiv.org/abs/2410.04148",
    "title": "Introductory Tutorial: Reasoning with Natural Language Explanations",
    "published_date": "2024-10-05",
    "abstract": "TODO",
    "summary": "The paper is an introductory tutorial lacking an abstract, thus no summary of its content can be provided."
  },
  {
    "url": "https://arxiv.org/abs/2407.03545",
    "title": "On Evaluating Explanation Utility for Human-AI Decision Making in NLP",
    "published_date": "2024-07-03",
    "abstract": "Is explainability a false promise? This debate has emerged from the insufficient evidence that explanations help people in situations they are introduced for. More human-centered, application-grounded evaluations of explanations are needed to settle this. Yet, with no established guidelines for such studies in NLP, researchers accustomed to standardized proxy evaluations must discover appropriate measurements, tasks, datasets, and sensible models for human-AI teams in their studies. To aid with this, we first review existing metrics suitable for application-grounded evaluation. We then establish criteria to select appropriate datasets, and using them, we find that only 4 out of over 50 datasets available for explainability research in NLP meet them. We then demonstrate the importance of reassessing the state of the art to form and study human-AI teams: teaming people with models for certain tasks might only now start to make sense, and for others, it remains unsound. Finally, we present the exemplar studies of human-AI decision-making for one of the identified tasks -- verifying the correctness of a legal claim given a contract. Our results show that providing AI predictions, with or without explanations, does not cause decision makers to speed up their work without compromising performance. We argue for revisiting the setup of human-AI teams and improving automatic deferral of instances to AI, where explanations could play a useful role.",
    "citation_count": 2,
    "summary": "This paper argues that evaluating explanation utility in NLP requires more human-centered, application-grounded methods, critically examining existing metrics and datasets while demonstrating that integrating AI explanations into human decision-making doesn't always improve efficiency or accuracy."
  }
]