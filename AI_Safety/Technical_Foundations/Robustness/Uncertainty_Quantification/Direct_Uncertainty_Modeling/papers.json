[
  {
    "url": "https://arxiv.org/abs/2102.08501v2",
    "title": "DEUP: Direct Epistemic Uncertainty Prediction",
    "published_date": "2021-02-16",
    "abstract": "Epistemic Uncertainty is a measure of the lack of knowledge of a learner which diminishes with more evidence. While existing work focuses on using the variance of the Bayesian posterior due to parameter uncertainty as a measure of epistemic uncertainty, we argue that this does not capture the part of lack of knowledge induced by model misspecification. We discuss how the excess risk, which is the gap between the generalization error of a predictor and the Bayes predictor, is a sound measure of epistemic uncertainty which captures the effect of model misspecification. We thus propose a principled framework for directly estimating the excess risk by learning a secondary predictor for the generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. We discuss the merits of this novel measure of epistemic uncertainty, and highlight how it differs from variance-based measures of epistemic uncertainty and addresses its major pitfall. Our framework, Direct Epistemic Uncertainty Prediction (DEUP) is particularly interesting in interactive learning environments, where the learner is allowed to acquire novel examples in each round. Through a wide set of experiments, we illustrate how existing methods in sequential model optimization can be improved with epistemic uncertainty estimates from DEUP, and how DEUP can be used to drive exploration in reinforcement learning. We also evaluate the quality of uncertainty estimates from DEUP for probabilistic image classification and predicting synergies of drug combinations.",
    "citation_count": 69,
    "summary": "DEUP is a novel framework for directly estimating epistemic uncertainty by predicting excess risk, addressing limitations of variance-based methods that fail to account for model misspecification. This allows for improved performance in interactive learning settings, such as sequential model optimization and reinforcement learning, as demonstrated across diverse applications."
  },
  {
    "url": "https://arxiv.org/abs/2402.13768",
    "title": "Democratizing Uncertainty Quantification",
    "published_date": "2024-02-21",
    "abstract": "Uncertainty Quantification (UQ) is vital to safety-critical model-based analyses, but the widespread adoption of sophisticated UQ methods is limited by technical complexity. In this paper, we introduce UM-Bridge (the UQ and Modeling Bridge), a high-level abstraction and software protocol that facilitates universal interoperability of UQ software with simulation codes. It breaks down the technical complexity of advanced UQ applications and enables separation of concerns between experts. UM-Bridge democratizes UQ by allowing effective interdisciplinary collaboration, accelerating the development of advanced UQ methods, and making it easy to perform UQ analyses from prototype to High Performance Computing (HPC) scale. In addition, we present a library of ready-to-run UQ benchmark problems, all easily accessible through UM-Bridge. These benchmarks support UQ methodology research, enabling reproducible performance comparisons. We demonstrate UM-Bridge with several scientific applications, harnessing HPC resources even using UQ codes not designed with HPC support.",
    "summary": "UM-Bridge is a software protocol that simplifies uncertainty quantification (UQ) by enabling interoperability between UQ software and simulation codes, thus democratizing access to advanced UQ methods and fostering collaboration. It achieves this through a high-level abstraction and a library of benchmark problems, facilitating scalable UQ analyses from prototype to HPC environments."
  },
  {
    "url": "https://arxiv.org/abs/2405.20550",
    "title": "Uncertainty Quantification for Deep Learning",
    "published_date": "2024-05-31",
    "abstract": "A complete and statistically consistent uncertainty quantification for deep learning is provided, including the sources of uncertainty arising from (1) the new input data, (2) the training and testing data (3) the weight vectors of the neural network, and (4) the neural network because it is not a perfect predictor. Using Bayes Theorem and conditional probability densities, we demonstrate how each uncertainty source can be systematically quantified. We also introduce a fast and practical way to incorporate and combine all sources of errors for the first time. For illustration, the new method is applied to quantify errors in cloud autoconversion rates, predicted from an artificial neural network that was trained by aircraft cloud probe measurements in the Azores and the stochastic collection equation formulated as a two-moment bin model. For this specific example, the output uncertainty arising from uncertainty in the training and testing data is dominant, followed by uncertainty in the input data, in the trained neural network, and uncertainty in the weights. We discuss the usefulness of the methodology for machine learning practice, and how, through inclusion of uncertainty in the training data, the new methodology is less sensitive to input data that falls outside of the training data set.",
    "summary": "This paper presents a novel Bayesian method for comprehensive uncertainty quantification in deep learning, encompassing input data, training data, network weights, and model imperfections, and demonstrates its application to predicting cloud autoconversion rates. The method efficiently combines these uncertainty sources, revealing that training data uncertainty often dominates."
  },
  {
    "url": "https://arxiv.org/abs/2312.08083",
    "title": "Training of Neural Networks with Uncertain Data, A Mixture of Experts Approach",
    "published_date": "2023-12-13",
    "abstract": "This paper introduces the\"Uncertainty-aware Mixture of Experts\"(uMoE), a novel solution aimed at addressing aleatoric uncertainty within Neural Network (NN) based predictive models. While existing methodologies primarily concentrate on managing uncertainty during inference, uMoE uniquely embeds uncertainty into the training phase. Employing a\"Divide and Conquer\"strategy, uMoE strategically partitions the uncertain input space into more manageable subspaces. It comprises Expert components, individually trained on their respective subspace uncertainties. Overarching the Experts, a Gating Unit, leveraging additional information regarding the distribution of uncertain in-puts across these subspaces, dynamically adjusts the weighting to minimize deviations from ground truth. Our findings demonstrate the superior performance of uMoE over baseline methods in effectively managing data uncertainty. Furthermore, through a comprehensive robustness analysis, we showcase its adaptability to varying uncertainty levels and propose optimal threshold parameters. This innovative approach boasts broad applicability across diverse da-ta-driven domains, including but not limited to biomedical signal processing, autonomous driving, and production quality control.",
    "summary": "The Uncertainty-aware Mixture of Experts (uMoE) model addresses aleatoric uncertainty in neural networks by training expert components on partitioned subspaces of uncertain data, with a gating unit weighting their outputs based on input uncertainty distributions. This approach outperforms baselines by incorporating uncertainty directly into the training process, demonstrating improved performance and robustness across various uncertainty levels."
  },
  {
    "url": "https://arxiv.org/pdf/2212.08376.pdf",
    "title": "Easy Uncertainty Quantification (EasyUQ): Generating Predictive Distributions from Single-Valued Model Output",
    "published_date": "2022-12-16",
    "abstract": "How can we quantify uncertainty if our favorite computational tool - be it a numerical, a statistical, or a machine learning approach, or just any computer model - provides single-valued output only? In this article, we introduce the Easy Uncertainty Quantification (EasyUQ) technique, which transforms real-valued model output into calibrated statistical distributions, based solely on training data of model output-outcome pairs, without any need to access model input. In its basic form, EasyUQ is a special case of the recently introduced Isotonic Distributional Regression (IDR) technique that leverages the pool-adjacent-violators algorithm for nonparametric isotonic regression. EasyUQ yields discrete predictive distributions that are calibrated and optimal in finite samples, subject to stochastic monotonicity. The workflow is fully automated, without any need for tuning. The Smooth EasyUQ approach supplements IDR with kernel smoothing, to yield continuous predictive distributions that preserve key properties of the basic form, including both, stochastic monotonicity with respect to the original model output, and asymptotic consistency. For the selection of kernel parameters, we introduce multiple one-fit grid search, a computationally much less demanding approximation to leave-one-out cross-validation. We use simulation examples and forecast data from weather prediction to illustrate the techniques. In a study of benchmark problems from machine learning, we show how EasyUQ and Smooth EasyUQ can be integrated into the workflow of neural network learning and hyperparameter tuning, and find EasyUQ to be competitive with conformal prediction, as well as more elaborate input-based approaches.",
    "citation_count": 7,
    "summary": "EasyUQ is a novel uncertainty quantification technique that converts single-valued model outputs into calibrated predictive distributions using only model output-outcome pairs, requiring no model input. It employs isotonic regression, optionally smoothed with kernel methods and automated parameter selection, to produce either discrete or continuous distributions while maintaining stochastic monotonicity and achieving competitive accuracy."
  },
  {
    "url": "https://arxiv.org/pdf/2201.05890.pdf",
    "title": "Robust uncertainty estimates with out-of-distribution pseudo-inputs training",
    "published_date": "2022-01-15",
    "abstract": "Probabilistic models often use neural networks to control their predictive uncertainty. However, when making out-of-distribution (OOD)} predictions, the often-uncontrollable extrapolation properties of neural networks yield poor uncertainty predictions. Such models then don't know what they don't know, which directly limits their robustness w.r.t unexpected inputs. To counter this, we propose to explicitly train the uncertainty predictor where we are not given data to make it reliable. As one cannot train without data, we provide mechanisms for generating pseudo-inputs in informative low-density regions of the input space, and show how to leverage these in a practical Bayesian framework that casts a prior distribution over the model uncertainty. With a holistic evaluation, we demonstrate that this yields robust and interpretable predictions of uncertainty while retaining state-of-the-art performance on diverse tasks such as regression and generative modelling",
    "citation_count": 1,
    "summary": "This paper addresses the issue of poor uncertainty estimates in probabilistic models for out-of-distribution (OOD) inputs by training the uncertainty predictor using synthetic pseudo-inputs generated in low-density regions of the input space. This approach improves the robustness and interpretability of uncertainty predictions while maintaining strong performance across various tasks."
  },
  {
    "url": "https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained",
    "author": "Jay Bailey",
    "title": "Deep Q-Networks Explained",
    "published_date": "2022-09-13",
    "summary": "This article explains Deep Q-Networks (DQN), a deep reinforcement learning algorithm, at multiple levels of detail. It provides a high-level overview, a technical explanation with equations, and practical replication advice, allowing readers to choose the depth of understanding they require."
  },
  {
    "url": "https://www.lesswrong.com/posts/MFm3A4ihz9s5j2cCo/variational-bayesian-methods",
    "author": "Ege Erdil",
    "title": "Variational Bayesian methods",
    "published_date": "2022-08-25",
    "summary": "Variational Bayesian methods address the intractability of calculating P(x) in Bayesian inference by approximating the true posterior P(z|x) with a simpler distribution Q(z|x). This approximation minimizes the Kullback-Leibler divergence between Q(z|x) and P(z|x), allowing for a tractable estimation of P(x)."
  }
]