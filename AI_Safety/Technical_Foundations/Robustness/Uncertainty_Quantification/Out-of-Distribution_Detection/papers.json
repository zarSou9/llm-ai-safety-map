[
  {
    "title": "An Advanced Dirichlet Prior Network for Out-of-Distribution Detection in Remote Sensing",
    "abstract": "Remote sensing deals with a plethora of sensors, a large number of classes/categories, and a huge variation in geography. Owing to the difficulty of collecting labeled data uniformly representing all scenarios, data-hungry deep learning models are often trained with labeled data in a source domain that is limited in the above-mentioned aspects. However during test/inference phase, such deep learning models are often subjected to a distributional shift, also called out-of-distribution (OOD) samples, in the form of unseen classes, geographic differences, and multi-sensor differences. Deep learning models can behave in an unexpected manner when subjected to such distributional uncertainties. Vulnerability to OOD data severely reduces the reliability of deep learning models and trusting on such predictions in absence of any reliability indicator may lead to wrong policy decisions or mishaps in time-bound remote sensing applications. Motivated by this, in this work, we propose a Dirichlet Prior Network-based model to quantify distributional uncertainty of deep learning-based remote sensing models. The approach seeks to maximize the representation gap between the in-domain and OOD examples for better segregation of OOD samples at test time. Extensive experiments on several remote sensing image classification data sets demonstrate that the proposed model can quantify distributional uncertainty. To the best of our knowledge this is the first work to elaborately study distributional uncertainty in context of remote sensing. The codes are publicly available at.",
    "published_date": "2022-01-01",
    "citation_count": 39,
    "url": "https://ieeexplore.ieee.org/ielx7/36/9633014/09668955.pdf",
    "summary": "This paper introduces a Dirichlet Prior Network to improve out-of-distribution (OOD) detection in remote sensing by quantifying the distributional uncertainty of deep learning models, addressing the challenges posed by limited labeled data and distributional shifts in real-world applications. The method maximizes the representation gap between in-domain and OOD samples for better OOD detection."
  },
  {
    "title": "Understanding Failures in Out-of-Distribution Detection with Deep Generative Models",
    "abstract": "Deep generative models (dgms) seem a natural fit for detecting out-of-distribution (ood) inputs, but such models have been shown to assign higher probabilities or densities to ood images than images from the training distribution. In this work, we explain why this behavior should be attributed to model misestimation. We first prove that no method can guarantee performance beyond random chance without assumptions on which out-distributions are relevant. We then interrogate the typical set hypothesis, the claim that relevant out-distributions can lie in high likelihood regions of the data distribution, and that ood detection should be defined based on the data distribution's typical set. We highlight the consequences implied by assuming support overlap between in- and out-distributions, as well as the arbitrariness of the typical set for ood detection. Our results suggest that estimation error is a more plausible explanation than the misalignment between likelihood-based ood detection and out-distributions of interest, and we illustrate how even minimal estimation error can lead to ood detection failures, yielding implications for future work in deep generative modeling and ood detection.",
    "published_date": "2021-07-01",
    "citation_count": 91,
    "url": "https://ncbi.nlm.nih.gov/pmc/articles/PMC9295254/",
    "summary": "Deep generative models often fail at out-of-distribution (OOD) detection because of model mis-estimation, not inherent misalignment between likelihood-based detection and relevant OOD distributions; the paper demonstrates that even small estimation errors can cause significant OOD detection failures."
  },
  {
    "url": "https://www.alignmentforum.org/posts/xj5nzResmDZDqLuLo/estimating-tail-risk-in-neural-networks",
    "author": "Jacob Hilton; Mark Xu",
    "title": "Estimating Tail Risk in Neural Networks",
    "published_date": "2024-09-13",
    "summary": "Current methods for assessing the risk of catastrophic AI behavior rely on finding problematic inputs, which is insufficient for guaranteeing safety. The article proposes novel techniques for estimating the probability of tail events (catastrophic failures) without relying on identifying specific harmful inputs, aiming to improve AI safety."
  },
  {
    "url": "https://www.alignmentforum.org/posts/Tzbvdp5rhM4L8aSKL/low-probability-estimation-in-language-models",
    "author": "Gabriel Wu",
    "title": "Low Probability Estimation in Language Models",
    "published_date": "2024-10-18",
    "summary": "This paper empirically compares methods for estimating low probabilities of language model outputs, focusing on single-token predictions. The study uses importance sampling and activation extrapolation techniques to estimate probabilities of rare events, aiming to improve AI safety by mitigating harmful worst-case behaviors."
  },
  {
    "url": "https://www.lesswrong.com/posts/79eegMp3EBs8ptFqa/neural-uncertainty-estimation-for-alignment",
    "author": "Charlie Steiner",
    "title": "Neural uncertainty estimation review article (for alignment)",
    "published_date": "2023-12-05",
    "summary": "The article surveys methods for estimating uncertainty in neural network outputs, focusing on Bayesian neural networks and ensemble methods. While Bayesian approaches offer a theoretically sound framework, practical application often requires computationally expensive approximations; ensembles, though less theoretically grounded, provide a more computationally efficient alternative."
  },
  {
    "url": "https://www.lesswrong.com/posts/dhbLE8BqRvhBtsXhS/mlsn-3-neurips-safety-paper-roundup",
    "author": "Dan H",
    "title": "[MLSN #3]: NeurIPS Safety Paper Roundup",
    "published_date": "2022-03-08",
    "summary": "The ML Safety Newsletter's third issue summarizes recent research on improving machine learning model robustness, including findings that Vision Transformers offer greater distribution shift robustness but not inherently superior adversarial robustness to ConvNets, and a novel fractal-based data augmentation technique (PixMix) enhancing various reliability metrics. The issue also highlights advancements in adversarial attack detection and out-of-distribution anomaly detection."
  },
  {
    "url": "https://www.alignmentforum.org/posts/AwMb7C72etphiRvah/unsolved-ml-safety-problems",
    "author": "jsteinhardt",
    "title": "Unsolved ML Safety Problems",
    "published_date": "2021-09-29",
    "summary": "The paper \"Unsolved Problems in ML Safety\" outlines crucial challenges in making machine learning systems safer, focusing on robustness (withstanding hazards like long-tail events and adversarial attacks), monitoring (detecting anomalies and backdoors), and alignment (ensuring safe objectives and their pursuit). The authors propose a roadmap addressing these issues to mitigate risks posed by increasingly powerful models."
  },
  {
    "url": "https://www.lesswrong.com/posts/8Gv5zSCnGeLxK5FAF/mlsn-1-iclr-safety-paper-roundup",
    "author": "Dan H",
    "title": "[MLSN #1]: ICLR Safety Paper Roundup",
    "published_date": "2021-10-18",
    "summary": "The first issue of the ML Safety Newsletter covers recent research on improving model robustness, including the use of discrete representations in vision transformers. It also highlights studies showing that larger language models are more prone to repeating misinformation and gaming reward proxies, emphasizing the need for improved alignment and monitoring techniques."
  }
]