[
  {
    "url": "https://arxiv.org/pdf/1811.07209.pdf",
    "title": "A Statistical Approach to Assessing Neural Network Robustness",
    "published_date": "2018-09-27",
    "abstract": "We present a new approach to assessing the robustness of neural networks based on estimating the proportion of inputs for which a property is violated. Specifically, we estimate the probability of the event that the property is violated under an input model. Our approach critically varies from the formal verification framework in that when the property can be violated, it provides an informative notion of how robust the network is, rather than just the conventional assertion that the network is not verifiable. Furthermore, it provides an ability to scale to larger networks than formal verification approaches. Though the framework still provides a formal guarantee of satisfiability whenever it successfully finds one or more violations, these advantages do come at the cost of only providing a statistical estimate of unsatisfiability whenever no violation is found. Key to the practical success of our approach is an adaptation of multi-level splitting, a Monte Carlo approach for estimating the probability of rare events, to our statistical robustness framework. We demonstrate that our approach is able to emulate formal verification procedures on benchmark problems, while scaling to larger networks and providing reliable additional information in the form of accurate estimates of the violation probability.",
    "citation_count": 75,
    "summary": "This paper introduces a statistical method for evaluating neural network robustness by estimating the probability of property violations under an input model, offering a scalable alternative to formal verification that provides informative robustness measures even when violations exist. This approach leverages multi-level splitting to efficiently estimate the probability of rare events, enabling analysis of larger networks than previously possible with formal verification methods."
  },
  {
    "url": "https://www.alignmentforum.org/posts/JDrxA3vwZAKZfmShz/degeneracies-are-sticky-for-sgd",
    "author": "Guillaume Corlouer; Nicolas Macé",
    "title": "Degeneracies are sticky for SGD",
    "published_date": "2024-06-16",
    "summary": "This study investigates how the degeneracy of loss landscapes affects Stochastic Gradient Descent (SGD) in deep learning. The authors find that degeneracy slows convergence and influences the direction SGD escapes degenerate manifolds, sometimes resulting in discrepancies between SGD's final parameter distribution and the Bayesian posterior prediction."
  },
  {
    "url": "https://www.lesswrong.com/posts/d9MkMeLWvoDEsqpQP/a-compilation-of-misuses-of-statistics",
    "author": "Younes Kamel",
    "title": "A compilation of misuses of statistics",
    "published_date": "2022-02-14",
    "summary": "The article highlights common statistical errors, primarily focusing on the misuse of Gaussian assumptions in modeling fat-tailed distributions and misinterpretations of p-values, base rates, and statistical power. These errors lead to flawed conclusions in various fields, including finance and scientific research."
  },
  {
    "url": "https://www.lesswrong.com/posts/cCcCJnvMEHqrgiCnx/practical-use-of-the-beta-distribution-for-data-analysis",
    "author": "Maxwell Peterson",
    "title": "Practical use of the Beta distribution for data analysis",
    "published_date": "2022-04-03",
    "summary": "While the Gaussian distribution is often used to approximate probabilities from binary count data, the Beta distribution is more accurate, especially for small datasets or probabilities near 0 or 1. The Beta distribution is easily implemented and provides reliable uncertainty intervals, unlike the Gaussian approximation which can produce nonsensical results in these scenarios."
  },
  {
    "url": "https://www.lesswrong.com/posts/MFm3A4ihz9s5j2cCo/variational-bayesian-methods",
    "author": "Ege Erdil",
    "title": "Variational Bayesian methods",
    "published_date": "2022-08-25",
    "summary": "Variational Bayesian methods address the intractability of calculating P(x) in Bayesian inference by approximating the true posterior P(z|x) with a simpler distribution Q(z|x). This approximation minimizes the Kullback-Leibler divergence between Q(z|x) and P(z|x), allowing for a tractable estimation of P(x)."
  },
  {
    "url": "https://arxiv.org/pdf/2102.05368.pdf",
    "title": "RoBIC: A Benchmark Suite For Assessing Classifiers Robustness",
    "published_date": "2021-02-10",
    "abstract": "Many defenses have emerged with the development of adversarial attacks. Models must be objectively evaluated accordingly. This paper systematically tackles this concern by proposing a new parameter-free benchmark we coin ROBIC. ROBIC fairly evaluates the robustness of image classifiers using a new half-distortion measure. It gauges the robustness of the network against white and black box attacks, independently of its accuracy. ROBIC is faster than the other available benchmarks. We present the significant differences in the robustness of 16 recent models as assessed by ROBIC.We make this benchmark publicly available for use and contribution at https://gitlab.inria.fr/t;maho/robustness_benchmark.",
    "citation_count": 4,
    "summary": "RoBIC is a novel, parameter-free benchmark suite for evaluating the robustness of image classifiers against adversarial attacks, offering a faster and more objective assessment independent of model accuracy using a half-distortion measure. It reveals significant differences in robustness across various models and is publicly available."
  },
  {
    "url": "https://www.lesswrong.com/posts/B6WefmeyaST7Puddz/there-is-no-control-system-for-covid",
    "author": "Mike Harris",
    "title": "There Is No Control System For COVID",
    "published_date": "2021-04-06",
    "summary": "The standard epidemiological model poorly predicts COVID-19 infection rates across US states, failing to explain the surprisingly similar infection levels despite varying policy responses. A proposed \"vulnerability model,\" incorporating fluctuating individual susceptibility to infection, better accounts for the observed data and resolves inconsistencies in transmission rate estimations."
  },
  {
    "url": "https://arxiv.org/pdf/2006.05095v2.pdf",
    "title": "Towards an Intrinsic Definition of Robustness for a Classifier",
    "published_date": "2020-06-09",
    "abstract": "Finding good measures of robustness – i.e. the ability to correctly classify corrupted input signals – of a trained classifier is an important question for sensitive practical applications. In this paper, we point out that averaging the radius of robustness of samples in a validation set is a statistically weak measure. We propose instead to weight the importance of samples depending on their difficulty. We motivate the proposed score by a theoretical case study using logistic regression. We also empirically demonstrate the ability of the proposed score to measure robustness of classifiers with little dependence on the choice of samples in more complex settings, including deep convolutional neural networks and real datasets.",
    "citation_count": 2,
    "summary": "This paper argues that averaging sample-wise robustness radii is an inadequate measure of classifier robustness, proposing instead a weighted score that prioritizes difficult samples and demonstrating its improved performance and stability across various classifier types and datasets."
  }
]