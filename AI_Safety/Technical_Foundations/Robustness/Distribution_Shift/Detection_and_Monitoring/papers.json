[
  {
    "url": "https://www.lesswrong.com/posts/rmwAuWXYTo24E5nnX/a-pin-and-a-balloon-anthropic-fragility-increases-chances-of",
    "author": "avturchin",
    "title": "A Pin and a Balloon: Anthropic Fragility Increases Chances of Runaway Global Warming",
    "published_date": "2022-09-11",
    "summary": "Due to survival bias, we underestimate the likelihood and proximity of climate tipping points, making Earth more vulnerable to human-caused catastrophes. This \"anthropic fragility\" increases the probability of runaway global warming and potential human extinction, highlighting the urgent need for geoengineering research."
  },
  {
    "url": "https://www.lesswrong.com/posts/MFm3A4ihz9s5j2cCo/variational-bayesian-methods",
    "author": "Ege Erdil",
    "title": "Variational Bayesian methods",
    "published_date": "2022-08-25",
    "summary": "Variational Bayesian methods address the intractability of calculating P(x) in Bayesian inference by approximating the true posterior P(z|x) with a simpler distribution Q(z|x). This approximation minimizes the Kullback-Leibler divergence between Q(z|x) and P(z|x), allowing for a tractable calculation of P(x)."
  },
  {
    "url": "https://www.lesswrong.com/posts/B6WefmeyaST7Puddz/there-is-no-control-system-for-covid",
    "author": "Mike Harris",
    "title": "There Is No Control System For COVID",
    "published_date": "2021-04-06",
    "summary": "The standard epidemiological model poorly predicts COVID-19 infection rates across US states, failing to explain the surprisingly similar infection levels despite varying policies and behaviors. A proposed \"vulnerability model,\" incorporating fluctuating individual susceptibility to infection over time, better fits the observed data and resolves inconsistencies of the standard model."
  },
  {
    "url": "https://www.lesswrong.com/posts/3RLSqqfP5oQE2WDrr/new-paper-long-term-trajectories-of-human-civilization",
    "author": "Kaj_Sotala",
    "title": "New paper: Long-Term Trajectories of Human Civilization",
    "published_date": "2018-08-12",
    "summary": "This paper establishes the study of long-term human civilization trajectories as a scientific and ethical field, identifying four potential paths: status quo, catastrophe, technological transformation, and astronomical expansion. The authors argue that understanding these trajectories and their potential impact is crucial for informed present-day decision-making."
  },
  {
    "url": "https://www.lesswrong.com/posts/Tx6dGzYLtfzzkuGtF/the-vulnerable-world-hypothesis-by-bostrom",
    "author": "Ben Pace",
    "title": "The Vulnerable World Hypothesis (by Bostrom)",
    "published_date": "2018-11-06",
    "summary": "Nick Bostrom's \"Vulnerable World Hypothesis\" argues that continued technological advancement significantly increases the likelihood of civilizational collapse unless global governance substantially improves. This risk stems from various technological vulnerabilities, ranging from easily accessible destructive technologies to the combined effects of many individually minor harmful actions."
  },
  {
    "url": "https://www.lesswrong.com/posts/Rku2g6QTTfN5pRRYo/the-map-of-natural-global-catastrophic-risks",
    "author": "turchin",
    "title": "The map of natural global catastrophic risks",
    "published_date": "2016-09-25",
    "summary": "While asteroid impacts pose a negligible risk, supervolcano eruptions represent a more significant natural threat. However, the greatest risk may be the underestimation of natural risks due to biases and the possibility that humanity currently inhabits a uniquely fragile and unstable period."
  },
  {
    "url": "https://www.lesswrong.com/posts/9Ep7bNRQZhh5QNKft/the-map-of-global-catastrophic-risks-connected-with",
    "author": "turchin",
    "title": "The map of global catastrophic risks connected with biological weapons and genetic engineering",
    "published_date": "2016-02-22",
    "summary": "The author argues that the risk of a globally catastrophic bioengineered pandemic is severely underestimated and comparable to the threat of uncontrolled artificial intelligence, potentially leading to human extinction. This risk is considered imminent due to readily available technology and the relatively low cost of developing such weapons, demanding immediate attention and research."
  },
  {
    "url": "https://www.lesswrong.com/posts/6wYcnAB9urWmgrNtv/the-map-of-double-scenarios-of-a-global-catastrophe",
    "author": "turchin",
    "title": "The map of double scenarios of a global catastrophe",
    "published_date": "2015-12-05",
    "summary": "The linked PDF, \"Double Scenarios of a Global Catastrophe,\" likely explores hypothetical scenarios involving paired global catastrophes, examining their potential combined impacts and cascading effects. It analyzes the likelihood and consequences of these simultaneous or sequentially linked events."
  }
]