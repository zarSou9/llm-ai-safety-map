[
  {
    "url": "https://arxiv.org/abs/1706.06083v4",
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "published_date": "2017-06-19",
    "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
    "citation_count": 11084,
    "summary": "This paper investigates adversarial robustness in deep neural networks using robust optimization, developing training and attack methods that provide concrete security guarantees against a wide range of adversarial examples and significantly improve network resistance. The approach focuses on achieving robustness against first-order adversaries as a crucial step towards fully resistant models."
  },
  {
    "url": "https://arxiv.org/abs/2112.09279",
    "title": "Robust Upper Bounds for Adversarial Training",
    "published_date": "2021-12-17",
    "abstract": "Many state-of-the-art adversarial training methods for deep learning leverage upper bounds of the adversarial loss to provide security guarantees against adversarial attacks. Yet, these methods rely on convex relaxations to propagate lower and upper bounds for intermediate layers, which affect the tightness of the bound at the output layer. We introduce a new approach to adversarial training by minimizing an upper bound of the adversarial loss that is based on a holistic expansion of the network instead of separate bounds for each layer. This bound is facilitated by state-of-the-art tools from Robust Optimization; it has closed-form and can be effectively trained using backpropagation. We derive two new methods with the proposed approach. The first method (Approximated Robust Upper Bound or aRUB) uses the first order approximation of the network as well as basic tools from Linear Robust Optimization to obtain an empirical upper bound of the adversarial loss that can be easily implemented. The second method (Robust Upper Bound or RUB), computes a provable upper bound of the adversarial loss. Across a variety of tabular and vision data sets we demonstrate the effectiveness of our approach -- RUB is substantially more robust than state-of-the-art methods for larger perturbations, while aRUB matches the performance of state-of-the-art methods for small perturbations.",
    "summary": "This paper introduces a novel adversarial training approach using a holistic, closed-form upper bound of the adversarial loss, avoiding layer-wise relaxations common in existing methods. Two resulting methods, aRUB and RUB, demonstrate improved robustness compared to state-of-the-art techniques, especially for larger perturbations."
  },
  {
    "url": "https://arxiv.org/abs/2204.13779",
    "title": "Formulating Robustness Against Unforeseen Attacks",
    "published_date": "2022-04-28",
    "abstract": "Existing defenses against adversarial examples such as adversarial training typically assume that the adversary will conform to a specific or known threat model, such as $\\ell_p$ perturbations within a fixed budget. In this paper, we focus on the scenario where there is a mismatch in the threat model assumed by the defense during training, and the actual capabilities of the adversary at test time. We ask the question: if the learner trains against a specific\"source\"threat model, when can we expect robustness to generalize to a stronger unknown\"target\"threat model during test-time? Our key contribution is to formally define the problem of learning and generalization with an unforeseen adversary, which helps us reason about the increase in adversarial risk from the conventional perspective of a known adversary. Applying our framework, we derive a generalization bound which relates the generalization gap between source and target threat models to variation of the feature extractor, which measures the expected maximum difference between extracted features across a given threat model. Based on our generalization bound, we propose variation regularization (VR) which reduces variation of the feature extractor across the source threat model during training. We empirically demonstrate that using VR can lead to improved generalization to unforeseen attacks during test-time, and combining VR with perceptual adversarial training (Laidlaw et al., 2021) achieves state-of-the-art robustness on unforeseen attacks. Our code is publicly available at https://github.com/inspire-group/variation-regularization.",
    "citation_count": 7,
    "summary": "This paper addresses the problem of adversarial robustness when the training and test-time attacks differ, formally defining this \"unforeseen adversary\" scenario and deriving a generalization bound. The authors propose variation regularization (VR), a training method improving robustness against unforeseen attacks by minimizing feature extractor variation under the training threat model, demonstrating state-of-the-art results when combined with perceptual adversarial training."
  },
  {
    "url": "https://arxiv.org/abs/2403.05100",
    "title": "Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume",
    "published_date": "2024-03-08",
    "abstract": "The escalating threat of adversarial attacks on deep learning models, particularly in security-critical fields, has underscored the need for robust deep learning systems. Conventional robustness evaluations have relied on adversarial accuracy, which measures a model's performance under a specific perturbation intensity. However, this singular metric does not fully encapsulate the overall resilience of a model against varying degrees of perturbation. To address this gap, we propose a new metric termed adversarial hypervolume, assessing the robustness of deep learning models comprehensively over a range of perturbation intensities from a multi-objective optimization standpoint. This metric allows for an in-depth comparison of defense mechanisms and recognizes the trivial improvements in robustness afforded by less potent defensive strategies. Additionally, we adopt a novel training algorithm that enhances adversarial robustness uniformly across various perturbation intensities, in contrast to methods narrowly focused on optimizing adversarial accuracy. Our extensive empirical studies validate the effectiveness of the adversarial hypervolume metric, demonstrating its ability to reveal subtle differences in robustness that adversarial accuracy overlooks. This research contributes a new measure of robustness and establishes a standard for assessing and benchmarking the resilience of current and future defensive models against adversarial threats.",
    "citation_count": 2,
    "summary": "This paper introduces \"adversarial hypervolume,\" a new metric for evaluating the robustness of deep learning models across a range of attack intensities, providing a more comprehensive assessment than traditional adversarial accuracy. It also presents a novel training algorithm designed to improve robustness uniformly across these intensities."
  },
  {
    "url": "https://arxiv.org/abs/2310.02480",
    "title": "Splitting the Difference on Adversarial Training",
    "published_date": "2023-10-03",
    "abstract": "The existence of adversarial examples points to a basic weakness of deep neural networks. One of the most effective defenses against such examples, adversarial training, entails training models with some degree of robustness, usually at the expense of a degraded natural accuracy. Most adversarial training methods aim to learn a model that finds, for each class, a common decision boundary encompassing both the clean and perturbed examples. In this work, we take a fundamentally different approach by treating the perturbed examples of each class as a separate class to be learned, effectively splitting each class into two classes:\"clean\"and\"adversarial.\"This split doubles the number of classes to be learned, but at the same time considerably simplifies the decision boundaries. We provide a theoretical plausibility argument that sheds some light on the conditions under which our approach can be expected to be beneficial. Likewise, we empirically demonstrate that our method learns robust models while attaining optimal or near-optimal natural accuracy, e.g., on CIFAR-10 we obtain near-optimal natural accuracy of $95.01\\%$ alongside significant robustness across multiple tasks. The ability to achieve such near-optimal natural accuracy, while maintaining a significant level of robustness, makes our method applicable to real-world applications where natural accuracy is at a premium. As a whole, our main contribution is a general method that confers a significant level of robustness upon classifiers with only minor or negligible degradation of their natural accuracy.",
    "citation_count": 2,
    "summary": "This paper proposes a novel adversarial training method that splits each class into \"clean\" and \"adversarial\" sub-classes, simplifying decision boundaries and improving robustness. This approach achieves near-optimal natural accuracy while maintaining significant robustness against adversarial examples, unlike traditional methods which often trade accuracy for robustness."
  },
  {
    "url": "https://www.lesswrong.com/posts/timk6zHDTFdrHYLmu/adversarial-robustness-could-help-prevent-catastrophic",
    "author": "aogara",
    "title": "Adversarial Robustness Could Help Prevent Catastrophic Misuse",
    "published_date": "2023-12-11",
    "summary": "The article argues that adversarial robustness in AI models is crucial to mitigating catastrophic misuse, as current models, despite safety training, remain vulnerable to adversarial prompts that elicit harmful behavior. The author emphasizes the need for increased research and funding in this area to prevent potential AI-enabled catastrophes like bioterrorism."
  },
  {
    "url": "https://arxiv.org/pdf/2110.15767v1.pdf",
    "title": "Adversarial Robustness with Semi-Infinite Constrained Learning",
    "published_date": "2021-10-29",
    "abstract": "Despite strong performance in numerous applications, the fragility of deep learning to input perturbations has raised serious questions about its use in safety-critical domains. While adversarial training can mitigate this issue in practice, state-of-the-art methods are increasingly application-dependent, heuristic in nature, and suffer from fundamental trade-offs between nominal performance and robustness. Moreover, the problem of finding worst-case perturbations is non-convex and underparameterized, both of which engender a non-favorable optimization landscape. Thus, there is a gap between the theory and practice of adversarial training, particularly with respect to when and why adversarial training works. In this paper, we take a constrained learning approach to address these questions and to provide a theoretical foundation for robust learning. In particular, we leverage semi-infinite optimization and non-convex duality theory to show that adversarial training is equivalent to a statistical problem over perturbation distributions, which we characterize completely. Notably, we show that a myriad of previous robust training techniques can be recovered for particular, sub-optimal choices of these distributions. Using these insights, we then propose a hybrid Langevin Monte Carlo approach of which several common algorithms (e.g., PGD) are special cases. Finally, we show that our approach can mitigate the trade-off between nominal and robust performance, yielding state-of-the-art results on MNIST and CIFAR-10. Our code is available at: https://github.com/arobey1/advbench.",
    "citation_count": 39,
    "summary": "This paper frames adversarial training as a semi-infinite constrained learning problem, providing a theoretical foundation by showing its equivalence to a statistical problem over perturbation distributions and proposing a novel hybrid Langevin Monte Carlo approach that improves the trade-off between nominal and robust performance. This approach unifies and extends existing methods, achieving state-of-the-art results."
  },
  {
    "url": "https://arxiv.org/pdf/2103.01276.pdf",
    "title": "A Multiclass Boosting Framework for Achieving Fast and Provable Adversarial Robustness",
    "published_date": "2021-03-01",
    "abstract": "Alongside the well-publicized accomplishments of deep neural networks there has emerged an apparent bug in their success on tasks such as object recognition: with deep models trained using vanilla methods, input images can be slightly corrupted in order to modify output predictions, even when these corruptions are practically invisible. This apparent lack of robustness has led researchers to propose methods that can help to prevent an adversary from having such capabilities. The state-of-the-art approaches have incorporated the robustness requirement into the loss function, and the training process involves taking stochastic gradient descent steps not using original inputs but on adversarially-corrupted ones. In this paper we propose a multiclass boosting framework to ensure adversarial robustness. Boosting algorithms are generally well-suited for adversarial scenarios, as they were classically designed to satisfy a minimax guarantee. We provide a theoretical foundation for this methodology and describe conditions under which robustness can be achieved given a weak training oracle. We show empirically that adversarially-robust multiclass boosting not only outperforms the state-of-the-art methods, it does so at a fraction of the training time.",
    "citation_count": 6,
    "summary": "This paper introduces a multiclass boosting framework for achieving fast and provably robust deep neural networks against adversarial attacks, theoretically justifying its effectiveness and demonstrating superior performance and training speed compared to state-of-the-art methods."
  },
  {
    "url": "https://arxiv.org/abs/2010.09670v1",
    "title": "RobustBench: a standardized adversarial robustness benchmark",
    "published_date": "2020-10-19",
    "abstract": "Evaluation of adversarial robustness is often error-prone leading to overestimation of the true robustness of models. While adaptive attacks designed for a particular defense are a way out of this, there are only approximate guidelines on how to perform them. Moreover, adaptive evaluations are highly customized for particular models, which makes it difficult to compare different defenses. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. This requires to impose some restrictions on the admitted models to rule out defenses that only make gradient-based attacks ineffective without improving actual robustness. We evaluate robustness of models for our benchmark with AutoAttack, an ensemble of white- and black-box attacks which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. Our leaderboard, hosted at this http URL, aims at reflecting the current state of the art on a set of well-defined tasks in $\\ell_\\infty$- and $\\ell_2$-threat models with possible extensions in the future. Additionally, we open-source the library this http URL that provides unified access to state-of-the-art robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze general trends in $\\ell_p$-robustness and its impact on other tasks such as robustness to various distribution shifts and out-of-distribution detection.",
    "citation_count": 609,
    "summary": "RobustBench is a standardized adversarial robustness benchmark using AutoAttack to accurately evaluate model robustness, addressing previous overestimation issues and facilitating fair comparisons between different defense methods. It provides a leaderboard and open-source library for accessing state-of-the-art robust models and analyzing robustness trends across various threat models and tasks."
  },
  {
    "url": "https://www.lesswrong.com/posts/NTwA3J99RPkgmp6jh/an-62-are-adversarial-examples-caused-by-real-but",
    "author": "Rohin Shah",
    "title": "[AN #62] Are adversarial examples caused by real but imperceptible features?",
    "published_date": "2019-08-22",
    "summary": "The Alignment Newsletter discusses a paper arguing that adversarial examples in machine learning aren't bugs but rather exploitable \"non-robust features\" that generalize to test sets. Two experiments demonstrate that models can learn from datasets containing these features, achieving good performance even with seemingly incorrect labels, suggesting model distillation even with flawed data."
  }
]