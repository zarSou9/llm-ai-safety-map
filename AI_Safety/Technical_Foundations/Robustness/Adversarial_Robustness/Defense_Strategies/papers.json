[
  {
    "url": "https://arxiv.org/abs/1901.08573v1",
    "title": "Theoretically Principled Trade-off between Robustness and Accuracy",
    "published_date": "2019-01-24",
    "abstract": "We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by $11.41\\%$ in terms of mean $\\ell_2$ perturbation distance.",
    "citation_count": 2347,
    "summary": "This paper establishes a theoretically principled trade-off between robustness and accuracy in adversarial example defense, decomposing robust error and deriving a tight upper bound using classification-calibrated loss; this analysis informs a new defense method, TRADES, which achieved state-of-the-art performance in the NeurIPS 2018 Adversarial Vision Challenge."
  },
  {
    "url": "https://arxiv.org/abs/2310.02480",
    "title": "Splitting the Difference on Adversarial Training",
    "published_date": "2023-10-03",
    "abstract": "The existence of adversarial examples points to a basic weakness of deep neural networks. One of the most effective defenses against such examples, adversarial training, entails training models with some degree of robustness, usually at the expense of a degraded natural accuracy. Most adversarial training methods aim to learn a model that finds, for each class, a common decision boundary encompassing both the clean and perturbed examples. In this work, we take a fundamentally different approach by treating the perturbed examples of each class as a separate class to be learned, effectively splitting each class into two classes:\"clean\"and\"adversarial.\"This split doubles the number of classes to be learned, but at the same time considerably simplifies the decision boundaries. We provide a theoretical plausibility argument that sheds some light on the conditions under which our approach can be expected to be beneficial. Likewise, we empirically demonstrate that our method learns robust models while attaining optimal or near-optimal natural accuracy, e.g., on CIFAR-10 we obtain near-optimal natural accuracy of $95.01\\%$ alongside significant robustness across multiple tasks. The ability to achieve such near-optimal natural accuracy, while maintaining a significant level of robustness, makes our method applicable to real-world applications where natural accuracy is at a premium. As a whole, our main contribution is a general method that confers a significant level of robustness upon classifiers with only minor or negligible degradation of their natural accuracy.",
    "citation_count": 2,
    "summary": "This paper proposes a novel adversarial training method that splits each class into \"clean\" and \"adversarial\" subclasses, simplifying decision boundaries and improving robustness. This approach achieves near-optimal natural accuracy while maintaining significant robustness against adversarial examples, unlike traditional methods that often trade off accuracy for robustness."
  },
  {
    "url": "https://arxiv.org/abs/1909.04068v1",
    "title": "Adversarial Robustness Against the Union of Multiple Perturbation Models",
    "published_date": "2019-09-09",
    "abstract": "Owing to the susceptibility of deep learning systems to adversarial attacks, there has been a great deal of work in developing (both empirically and certifiably) robust classifiers, but the vast majority has defended against single types of attacks. Recent work has looked at defending against multiple attacks, specifically on the MNIST dataset, yet this approach used a relatively complex architecture, claiming that standard adversarial training can not apply because it \"overfits\" to a particular norm. In this work, we show that it is indeed possible to adversarially train a robust model against a union of norm-bounded attacks, by using a natural generalization of the standard PGD-based procedure for adversarial training to multiple threat models. With this approach, we are able to train standard architectures which are robust against $\\ell_\\infty$, $\\ell_2$, and $\\ell_1$ attacks, outperforming past approaches on the MNIST dataset and providing the first CIFAR10 network trained to be simultaneously robust against $(\\ell_{\\infty}, \\ell_{2},\\ell_{1})$ threat models, which achieves adversarial accuracy rates of $(47.6\\%, 64.8\\%, 53.4\\%)$ for $(\\ell_{\\infty}, \\ell_{2},\\ell_{1})$ perturbations with radius $\\epsilon = (0.03,0.5,12)$.",
    "citation_count": 147,
    "summary": "This paper demonstrates that standard architectures can be adversarially trained to be robust against multiple ($\\ell_\\infty$, $\\ell_2$, $\\ell_1$) norm-bounded attacks simultaneously, using a generalized projected gradient descent method, achieving state-of-the-art results on MNIST and the first such results on CIFAR-10."
  },
  {
    "title": "On the Security of Randomized Defenses Against Adversarial Samples",
    "abstract": "Deep Learning has been shown to be particularly vulnerable to adversarial samples. To combat adversarial strategies, numerous defensive techniques have been proposed. Among these, a promising approach is to use randomness in order to make the classification process unpredictable and presumably harder for the adversary to control. In this paper, we study the effectiveness of randomized defenses against adversarial samples. To this end, we categorize existing state-of-the-art adversarial strategies into three attacker models of increasing strength, namely blackbox, graybox, and whitebox (a.k.a. adaptive) attackers. We also devise a lightweight randomization strategy for image classification based on feature squeezing, that consists of pre-processing the classifier input by embedding randomness within each feature, before applying feature squeezing. We evaluate the proposed defense and compare it to other randomized techniques in the literature via thorough experiments. Our results indeed show that careful integration of randomness can be effective against both graybox and blackbox attacks without significantly degrading the accuracy of the underlying classifier. However, our experimental results offer strong evidence that in the present form such randomization techniques cannot deter a whitebox adversary that has access to all classifier parameters and has full knowledge of the defense. Our work thoroughly and empirically analyzes the impact of randomization techniques against all classes of adversarial strategies.",
    "published_date": "2018-12-11",
    "citation_count": 1,
    "url": "https://dl.acm.org/doi/10.1145/3320269.3384751",
    "summary": "This paper analyzes the effectiveness of randomized defenses against adversarial attacks on deep learning models, categorizing attacks by attacker knowledge (blackbox, graybox, whitebox). While effective against blackbox and graybox attacks, the study demonstrates that such defenses are vulnerable to whitebox attacks with full knowledge of the model and defense mechanism."
  },
  {
    "url": "https://arxiv.org/abs/1706.06083v4",
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "published_date": "2017-06-19",
    "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
    "citation_count": 11084,
    "summary": "This paper investigates adversarial robustness in deep neural networks using robust optimization, developing training and attack methods that provide a concrete security guarantee against a wide range of attacks and significantly improve resistance. The authors propose first-order adversary robustness as a key step towards creating fully resistant deep learning models."
  },
  {
    "url": "https://arxiv.org/abs/2408.13274",
    "title": "Robust Image Classification: Defensive Strategies against FGSM and PGD Adversarial Attacks",
    "published_date": "2024-08-20",
    "abstract": "Adversarial attacks, particularly the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) pose significant threats to the robustness of deep learning models in image classification. This paper explores and refines defense mechanisms against these attacks to enhance the resilience of neural networks. We employ a combination of adversarial training and innovative preprocessing techniques, aiming to mitigate the impact of adversarial perturbations. Our methodology involves modifying input data before classification and investigating different model architectures and training strategies. Through rigorous evaluation of benchmark datasets, we demonstrate the effectiveness of our approach in defending against FGSM and PGD attacks. Our results show substantial improvements in model robustness compared to baseline methods, highlighting the potential of our defense strategies in real-world applications. This study contributes to the ongoing efforts to develop secure and reliable machine learning systems, offering practical insights and paving the way for future research in adversarial defense. By bridging theoretical advancements and practical implementation, we aim to enhance the trustworthiness of AI applications in safety-critical domains.",
    "citation_count": 1,
    "summary": "This paper investigates defensive strategies against Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) adversarial attacks on image classification models, combining adversarial training with novel preprocessing techniques to improve robustness and achieving significant improvements over baseline methods. The results demonstrate enhanced resilience against these attacks, contributing to more secure and reliable machine learning systems."
  },
  {
    "url": "https://www.alignmentforum.org/posts/oPnFzfZtaoWrqTP4H/solving-adversarial-attacks-in-computer-vision-as-a-baby",
    "author": "Stanislavfort",
    "title": "Solving adversarial attacks in computer vision as a baby version of general AI alignment",
    "published_date": "2024-08-29",
    "summary": "The author argues that adversarial attacks on computer vision systems, which exploit subtle image manipulations to fool AI, mirror the broader challenge of aligning general AI with human intentions. Addressing adversarial robustness in computer vision is presented as a crucial stepping stone towards achieving safe and reliable general AI."
  },
  {
    "url": "https://www.alignmentforum.org/posts/xj5nzResmDZDqLuLo/estimating-tail-risk-in-neural-networks",
    "author": "Jacob Hilton; Mark Xu",
    "title": "Estimating Tail Risk in Neural Networks",
    "published_date": "2024-09-13",
    "summary": "Current methods for assessing the risk of catastrophic AI failures rely on identifying problematic inputs, a computationally expensive and potentially incomplete approach. The article proposes researching alternative methods for estimating the probability of such \"tail events\" without directly searching for harmful inputs, aiming to improve AI safety."
  }
]