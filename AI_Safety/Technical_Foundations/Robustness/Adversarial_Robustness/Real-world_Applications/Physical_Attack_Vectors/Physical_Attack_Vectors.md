### Mini Description

Study of methods for implementing adversarial attacks in the physical world, including adversarial patches, 3D-printed objects, and environmental modifications that persist across different viewing conditions.

### Description

Physical attack vectors in adversarial machine learning focus on methods for implementing attacks that persist in the real world, beyond purely digital manipulations. These attacks must account for physical constraints like manufacturability, environmental variations, and viewing angles while remaining effective against target AI systems. The fundamental challenge lies in creating perturbations that maintain their adversarial properties across different physical transformations and sensing pipelines.

Current research explores various implementation approaches, from carefully designed physical objects and textures to light-based projection attacks and camouflage patterns. These methods must balance attack effectiveness with practical considerations like production cost, durability, and detectability. A key area of investigation is the development of optimization techniques that account for physical world constraints and transformations during the attack generation process.

Emerging directions include the study of temporally persistent attacks that maintain effectiveness across video sequences, multi-view attacks that work from different perspectives, and adaptive physical attacks that can respond to changing conditions. Researchers are particularly interested in understanding the theoretical limits of physical attacks, including fundamental trade-offs between attack robustness, visibility, and implementation complexity. This includes exploring how physical constraints might actually provide natural defenses against certain types of attacks.

### Order

1. Object-based_Attacks
2. Projection_Attacks
3. Camouflage_Patterns
4. Dynamic_Attacks
5. Manufacturing_Constraints
