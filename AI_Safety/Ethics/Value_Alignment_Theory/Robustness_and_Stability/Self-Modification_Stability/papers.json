[
  {
    "url": "http://arxiv.org/abs/2401.09678",
    "title": "Integrating Graceful Degradation and Recovery Through Requirement-Driven Adaptation",
    "published_date": "2024-01-18",
    "abstract": "Cyber-physical systems (CPS) are subject to environmental uncertainties such as adverse operating conditions, malicious attacks, and hardware degradation. These uncertainties may lead to failures that put the system in a sub-optimal or unsafe state. Systems that are resilient to such uncertainties rely on two types of operations: (1) graceful degradation, for ensuring that the system maintains an acceptable level of safety during unexpected environmental conditions and (2) recovery, to facilitate the resumption of normal system functions. Typically, mechanisms for degradation and recovery are developed independently from each other, and later integrated into a system, requiring the designer to develop an additional, ad-hoc logic for activating and coordinating between the two operations. In this paper, we propose a self-adaptation approach for improving system resiliency through automated triggering and coordination of graceful degradation and recovery. The key idea behind our approach is to treat degradation and recovery as requirement-driven adaptation tasks: Degradation can be thought of as temporarily weakening original (i.e., ideal) system requirements to be achieved by the system, and recovery as strengthening the weakened requirements when the environment returns within an expected operating boundary. Furthermore, by treating weakening and strengthening as dual operations, we argue that a single requirement-based adaptation method is sufficient to enable coordination between degradation and recovery. Given system requirements specified in signal temporal logic (STL), we propose a run-time adaptation framework that performs degradation and recovery in response to environmental changes. We describe a prototype implementation of our framework and demonstrate the feasibility of the proposed approach using a case study in unmanned underwater vehicles.",
    "summary": "This paper presents a self-adaptation approach for improving cyber-physical system resiliency by integrating graceful degradation and recovery as requirement-driven adaptation tasks, automating their coordination through a single signal temporal logic (STL)-based framework. This approach contrasts with traditional independent development and ad-hoc integration of these functionalities."
  },
  {
    "url": "https://arxiv.org/abs/2410.22114",
    "title": "Policy Gradient for Robust Markov Decision Processes",
    "published_date": "2024-10-29",
    "abstract": "We develop a generic policy gradient method with the global optimality guarantee for robust Markov Decision Processes (MDPs). While policy gradient methods are widely used for solving dynamic decision problems due to their scalable and efficient nature, adapting these methods to account for model ambiguity has been challenging, often making it impractical to learn robust policies. This paper introduces a novel policy gradient method, Double-Loop Robust Policy Mirror Descent (DRPMD), for solving robust MDPs. DRPMD employs a general mirror descent update rule for the policy optimization with adaptive tolerance per iteration, guaranteeing convergence to a globally optimal policy. We provide a comprehensive analysis of DRPMD, including new convergence results under both direct and softmax parameterizations, and provide novel insights into the inner problem solution through Transition Mirror Ascent (TMA). Additionally, we propose innovative parametric transition kernels for both discrete and continuous state-action spaces, broadening the applicability of our approach. Empirical results validate the robustness and global convergence of DRPMD across various challenging robust MDP settings.",
    "summary": "The paper introduces Double-Loop Robust Policy Mirror Descent (DRPMD), a novel policy gradient method with guaranteed global optimality for solving robust Markov Decision Processes (MDPs), addressing the challenge of incorporating model ambiguity in scalable policy learning. DRPMD uses a mirror descent update rule with adaptive tolerance and offers convergence guarantees under various parameterizations, supported by empirical results."
  },
  {
    "url": "https://arxiv.org/abs/2304.07163v2",
    "title": "Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning",
    "published_date": "2023-04-14",
    "abstract": "A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason about the long-term consequences of following the expert policy or the default RL algorithm. Our experiments in four different settings show that these proposed algorithms achieve the above-mentioned goals whereas the other algorithms fail to do so.",
    "summary": "This paper proposes three novel bandit-based algorithms (UPIES, RPIES, LPIES) for incorporating external advice into reinforcement learning, addressing the challenge of maintaining policy invariance while accelerating learning from arbitrary advice by explicitly modeling the non-stationary nature of reward returns. Experiments demonstrate their superiority over existing methods."
  },
  {
    "url": "https://www.alignmentforum.org/posts/sHGxvJrBag7nhTQvb/invulnerable-incomplete-preferences-a-formal-statement-1",
    "author": "Sami Petersen",
    "title": "Invulnerable Incomplete Preferences: \nA Formal Statement",
    "published_date": "2023-08-30",
    "summary": "This paper argues against the claim that AI agents with incomplete preferences will necessarily adopt strictly dominated strategies, proving that a specific dynamic choice rule (Dynamic Strong Maximality) ensures invulnerability without forcing preference completion. The author demonstrates this through theorems showing that the agent avoids foreseeable negative outcomes while maintaining incomplete preferences."
  },
  {
    "url": "https://arxiv.org/abs/2207.08730",
    "title": "A framework for online, stabilizing reinforcement learning",
    "published_date": "2022-07-18",
    "abstract": "Online reinforcement learning is concerned with training an agent on-the-fly via dynamic interaction with the environment. Here, due to the specifics of the application, it is not generally possible to perform long pre-training, as it is commonly done in off-line, model-free approaches, which are akin to dynamic programming. Such applications may be found more frequently in industry, rather than in pure digital fields, such as cloud services, video games, database management, etc., where reinforcement learning has been demonstrating success. Online reinforcement learning, in contrast, is more akin to classical control, which utilizes some model knowledge about the environment. Stability of the closed-loop (agent plus the environment) is a major challenge for such online approaches. In this paper, we tackle this problem by a special fusion of online reinforcement learning with elements of classical control, namely, based on the Lyapunov theory of stability. The idea is to start the agent at once, without pre-training, and learn approximately optimal policy under specially designed constraints, which guarantee stability. The resulting approach was tested in an extensive experimental study with a mobile robot. A nominal parking controller was used as a baseline. It was observed that the suggested agent could always successfully park the robot, while significantly improving the cost. While many approaches may be exploited for mobile robot control, we suggest that the experiments showed the promising potential of online reinforcement learning agents based on Lyapunov-like constraints. The presented methodology may be utilized in safety-critical, industrial applications where stability is necessary.",
    "citation_count": 1,
    "summary": "This paper presents a novel online reinforcement learning framework that ensures stability by incorporating Lyapunov theory constraints, enabling immediate deployment without pre-training and demonstrated through successful mobile robot parking experiments. The approach offers a promising solution for safety-critical industrial applications requiring stable online learning."
  },
  {
    "title": "Safety from in-the-loop reachability for cyber-physical systems",
    "abstract": "We demonstrate a methodology for achieving safe autonomy that relies on computing reachable sets at runtime. Given a system subject to disturbances controlled by an unverified and potentially faulty controller, this methodology computes at each time the reachable set of the system under a backup control law to ensure the system is within reach of a known a priori safe region. Control barrier functions are then used in conjunction with the reachable set to adjust potentially unsafe control actions that would otherwise move the system beyond reach of this safe set. This approach faces several computational challenges: reachable sets for the dynamics must be computed at runtime; sensitivity of the reachable set to initial conditions is required for the control barrier optimization formulation; and the presence of disturbances introduces a large number of constraints in the resulting optimization. The proposed methodology leverages the theory of mixed monotone systems to address these challenges, and the main contribution of this paper is an application of this methodology to a ten dimensional dual planar multirotor system that is implemented on embedded hardware with a controller update rate up to 100Hz.",
    "published_date": "2021-05-19",
    "citation_count": 6,
    "url": "https://dl.acm.org/doi/10.1145/3457335.3461706",
    "summary": "This paper presents a runtime safety verification method for cyber-physical systems using reachable set computations and control barrier functions, addressing computational challenges through mixed monotone system theory and demonstrating its application on a ten-dimensional multirotor system at high update rates."
  },
  {
    "url": "https://arxiv.org/pdf/2109.05710.pdf",
    "title": "Robust Stability of Neural-Network-Controlled Nonlinear Systems With Parametric Variability",
    "published_date": "2021-09-13",
    "abstract": "Stability certification and identification of a safe and stabilizing initial set are two important concerns in ensuring operational safety, stability, and robustness of dynamical systems. With the advent of machine-learning tools, these issues need to be addressed for the systems with machine-learned components in the feedback loop. To develop a general theory for stability and stabilizability of neural network (NN)-controlled nonlinear systems subject to bounded parametric variations, a Lyapunov-based stability certificate is proposed and is further used to devise a maximal Lipschitz bound for a class of stabilizing NN controllers, and also a corresponding maximal Region of Attraction (RoA) within a user-specified safety set. To compute a robustly stabilizing NN controller that also maximizes the system's long-run utility, a stability-guaranteed training (SGT) algorithm is proposed. The effectiveness of the proposed framework is validated through an illustrative example.",
    "citation_count": 6,
    "summary": "This paper presents a Lyapunov-based framework for certifying the robust stability of nonlinear systems controlled by neural networks with parametric variability, including a stability-guaranteed training algorithm to maximize system utility while ensuring safety. A maximal Lipschitz bound and Region of Attraction are also derived."
  },
  {
    "url": "https://arxiv.org/pdf/2108.07362.pdf",
    "title": "A Game-Theoretic Approach to Self-Stabilization with Selfish Agents",
    "published_date": "2021-08-16",
    "abstract": "Self-stabilization is an excellent approach for adding fault tolerance to a distributed multi-agent system. However, two properties of self-stabilization theory, convergence and closure, may not be satisfied if agents are selfish. To guarantee convergence, we formulate the problem as a stochastic Bayesian game and introduce probabilistic self-stabilization to adjust the probabilities of rules with behavior strategies. This satisfies agents' self-interests such that no agent deviates the rules. To guarantee closure in the presence of selfish agents, we propose fault-containment as a method to constrain legitimate configurations of the self-stabilizing system to be Nash equilibria. We also assume selfish agents as capable of performing unauthorized actions at any time, which threatens both properties, and present a stepwise solution to handle it. As a case study, we consider the problem of distributed clustering and propose five self-stabilizing algorithms for forming clusters. Simulation results show that our algorithms react correctly to rule deviations and outperform comparable schemes in terms of fairness and stabilization time.",
    "summary": "This paper addresses the challenge of achieving self-stabilization in multi-agent systems with selfish agents, proposing a game-theoretic approach using stochastic Bayesian games and fault-containment to ensure convergence and closure, even under unauthorized actions; the approach is validated through a distributed clustering case study and simulations."
  }
]