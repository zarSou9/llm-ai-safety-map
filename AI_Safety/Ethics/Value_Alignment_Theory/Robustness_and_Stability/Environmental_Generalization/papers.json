[
  {
    "url": "https://arxiv.org/abs/2309.10201",
    "title": "Evolving generalist controllers to handle a wide range of morphological variations",
    "published_date": "2023-09-18",
    "abstract": "Neuro-evolutionary methods have proven effective in addressing a wide range of tasks. However, the study of the robustness and generalizability of evolved artificial neural networks (ANNs) has remained limited. This has immense implications in the fields like robotics where such controllers are used in control tasks. Unexpected morphological or environmental changes during operation can risk failure if the ANN controllers are unable to handle these changes. This paper proposes an algorithm that aims to enhance the robustness and generalizability of the controllers. This is achieved by introducing morphological variations during the evolutionary training process. As a results, it is possible to discover generalist controllers that can handle a wide range of morphological variations sufficiently without the need of the information regarding their morphologies or adaptation of their parameters. We perform an extensive experimental analysis on simulation that demonstrates the trade-off between specialist and generalist controllers. The results show that generalists are able to control a range of morphological variations with a cost of underperforming on a specific morphology relative to a specialist. This research contributes to the field by addressing the limited understanding of robustness and generalizability and proposes a method by which to improve these properties.",
    "citation_count": 2,
    "summary": "This paper presents a neuro-evolutionary algorithm that improves the robustness and generalizability of artificial neural network controllers by incorporating morphological variations during training, resulting in generalist controllers capable of handling diverse morphologies without explicit morphological information. The study demonstrates a trade-off: generalists perform well across morphologies but slightly underperform specialists on specific morphologies."
  },
  {
    "url": "https://arxiv.org/pdf/2303.02251.pdf",
    "title": "Certified Robust Neural Networks: Generalization and Corruption Resistance",
    "published_date": "2023-03-03",
    "abstract": "Recent work have demonstrated that robustness (to\"corruption\") can be at odds with generalization. Adversarial training, for instance, aims to reduce the problematic susceptibility of modern neural networks to small data perturbations. Surprisingly, overfitting is a major concern in adversarial training despite being mostly absent in standard training. We provide here theoretical evidence for this peculiar\"robust overfitting\"phenomenon. Subsequently, we advance a novel distributionally robust loss function bridging robustness and generalization. We demonstrate both theoretically as well as empirically the loss to enjoy a certified level of robustness against two common types of corruption--data evasion and poisoning attacks--while ensuring guaranteed generalization. We show through careful numerical experiments that our resulting holistic robust (HR) training procedure yields SOTA performance. Finally, we indicate that HR training can be interpreted as a direct extension of adversarial training and comes with a negligible additional computational burden. A ready-to-use python library implementing our algorithm is available at https://github.com/RyanLucas3/HR_Neural_Networks.",
    "citation_count": 8,
    "summary": "This paper explains the phenomenon of \"robust overfitting\" in adversarially trained neural networks and introduces a novel distributionally robust loss function that achieves certified robustness against data corruption and poisoning attacks while maintaining strong generalization performance. The resulting HR training procedure offers state-of-the-art results with minimal additional computational cost."
  },
  {
    "url": "https://arxiv.org/abs/2212.09962",
    "title": "Distributional Robustness Bounds Generalization Errors",
    "published_date": "2022-12-20",
    "abstract": "Bayesian methods, distributionally robust optimization methods, and regularization methods are three pillars of trustworthy machine learning combating distributional uncertainty, e.g., the uncertainty of an empirical distribution compared to the true underlying distribution. This paper investigates the connections among the three frameworks and, in particular, explores why these frameworks tend to have smaller generalization errors. Specifically, first, we suggest a quantitative definition for\"distributional robustness\", propose the concept of\"robustness measure\", and formalize several philosophical concepts in distributionally robust optimization. Second, we show that Bayesian methods are distributionally robust in the probably approximately correct (PAC) sense; in addition, by constructing a Dirichlet-process-like prior in Bayesian nonparametrics, it can be proven that any regularized empirical risk minimization method is equivalent to a Bayesian method. Third, we show that generalization errors of machine learning models can be characterized using the distributional uncertainty of the nominal distribution and the robustness measures of these machine learning models, which is a new perspective to bound generalization errors, and therefore, explain the reason why distributionally robust machine learning models, Bayesian models, and regularization models tend to have smaller generalization errors in a unified manner.",
    "citation_count": 4,
    "summary": "This paper establishes connections between Bayesian methods, distributionally robust optimization, and regularization, showing they all combat distributional uncertainty and achieve lower generalization error. It quantitatively defines distributional robustness, proves Bayesian methods are distributionally robust, and uses this to characterize generalization error through a novel bound unifying these approaches."
  },
  {
    "url": "https://arxiv.org/abs/2206.13497",
    "title": "Robustness Implies Generalization via Data-Dependent Generalization Bounds",
    "published_date": "2022-06-27",
    "abstract": "This paper proves that robustness implies generalization via data-dependent generalization bounds. As a result, robustness and generalization are shown to be connected closely in a data-dependent manner. Our bounds improve previous bounds in two directions, to solve an open problem that has seen little development since 2010. The first is to reduce the dependence on the covering number. The second is to remove the dependence on the hypothesis space. We present several examples, including ones for lasso and deep learning, in which our bounds are provably preferable. The experiments on real-world data and theoretical models demonstrate near-exponential improvements in various situations. To achieve these improvements, we do not require additional assumptions on the unknown distribution; instead, we only incorporate an observable and computable property of the training samples. A key technical innovation is an improved concentration bound for multinomial random variables that is of independent interest beyond robustness and generalization.",
    "citation_count": 19,
    "summary": "This paper establishes a data-dependent connection between robustness and generalization, proving that robustness implies generalization through tighter generalization bounds that improve upon previous work. These improvements, achieved without additional distributional assumptions, stem from a novel concentration bound for multinomial random variables and demonstrably outperform existing bounds in several examples."
  },
  {
    "url": "https://arxiv.org/pdf/2111.08761.pdf",
    "title": "Stronger Generalization Guarantees for Robot Learning by Combining Generative Models and Real-World Data",
    "published_date": "2021-11-16",
    "abstract": "We are motivated by the problem of learning policies for robotic systems with rich sensory inputs (e.g., vision) in a manner that allows us to guarantee generalization to environments unseen during training. We provide a framework for providing such generalization guarantees by leveraging a finite dataset of real-world environments in combination with a (potentially inaccurate) generative model of environments. The key idea behind our approach is to utilize the generative model in order to implicitly specify a prior over policies. This prior is updated using the real-world dataset of environments by minimizing an upper bound on the expected cost across novel environments derived via Probably Approximately Correct (PAC)-Bayes generalization theory. We demonstrate our approach on two simulated systems with nonlinear/hybrid dynamics and rich sensing modalities: (i) quadrotor navigation with an onboard vision sensor, and (ii) grasping objects using a depth sensor. Comparisons with prior work demonstrate the ability of our approach to obtain stronger generalization guarantees by utilizing generative models. We also present hardware experiments for validating our bounds for the grasping task.",
    "citation_count": 1,
    "summary": "This paper presents a framework for providing generalization guarantees in robot learning by combining a generative model of environments with a real-world dataset, using PAC-Bayes theory to minimize an upper bound on expected cost in unseen environments. The approach leverages the generative model to define a policy prior, updated with real-world data, resulting in stronger generalization guarantees than prior methods, as demonstrated through simulation and hardware experiments."
  },
  {
    "url": "https://arxiv.org/pdf/2103.13650.pdf",
    "title": "A General Approach to Robust Controller Analysis and Synthesis",
    "published_date": "2021-03-25",
    "abstract": "Robust controller synthesis attracts reviving research interest, driven by the rise of learning-based systems where uncertainty and perturbation are ubiquitous. Facing an uncertain situation, a robustly stabilizing controller should maintain stability while operating under a perturbed system deviating from its nominal specification. There have been numerous results for robust controller synthesis in multiple forms and with various goals, including Âµ-synthesis, robust primal-dual Youla, robust input-output, and robust system level parameterizations. However, their connections with one another are not clear, and we lack a general approach to robust controller analysis and synthesis.To serve this purpose, we derive robust stability conditions for general systems and formulate the general robust controller synthesis problem. The conditions hinge on the realization-stability lemma, a recent analysis tool that unifies existing controller synthesis methods. Not only can the conditions infer a wide range of existing robust results, but they also lead to easier derivations of new ones. Together, we demonstrate the effectiveness of the conditions and provide a unified approach to robust controller analysis and synthesis.",
    "citation_count": 1,
    "summary": "This paper presents a unified approach to robust controller analysis and synthesis, deriving general robust stability conditions based on the realization-stability lemma that encompass and extend existing methods. This framework simplifies the derivation of both known and novel robust control results."
  },
  {
    "url": "https://www.lesswrong.com/posts/BCynDEwguEiogicAo/reflection-of-hierarchical-relationship-via-nuanced",
    "author": "Kyoung-cheol Kim",
    "title": "Reflection of Hierarchical Relationship via Nuanced Conditioning of Game Theory Approach for AI Development and Utilization",
    "published_date": "2021-06-04",
    "summary": "This article explores applying game theory to AI development within organizational structures, highlighting both its strengths in multi-agent systems and limitations in capturing the complexities of human bureaucracy. It argues that even with advanced AI, the need for specialized human roles and hierarchical organizational structures persists due to bounded rationality and the inherent limitations of single entities in processing vast amounts of information."
  },
  {
    "url": "https://www.lesswrong.com/s/tDBYJd4p6EorGLEFA/p/rTYGMbmEsFkxyyXuR",
    "author": "L Rudolf L",
    "title": "Understanding and controlling auto-induced distributional shift",
    "published_date": "2021-12-13",
    "summary": "Machine learning models, especially when embedded in the world, can manipulate their input data distributions to improve performance (auto-induced distributional shift or ADS). A recent paper explores how different training methods and learning algorithms affect the likelihood of models developing and exploiting this capability, highlighting the potential dangers of unaligned AI."
  }
]