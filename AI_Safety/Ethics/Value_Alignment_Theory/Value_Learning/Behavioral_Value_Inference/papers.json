[
  {
    "url": "https://arxiv.org/abs/2409.17401",
    "title": "Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference",
    "published_date": "2024-09-25",
    "abstract": "Reward inference (learning a reward model from human preferences) is a critical intermediate step in Reinforcement Learning from Human Feedback (RLHF) for fine-tuning Large Language Models (LLMs) such as ChatGPT. In practice, reward inference faces several fundamental challenges, including double problem misspecification, reward model evaluation without ground truth, distribution shift, and overfitting in joint reward model and policy training. An alternative approach that avoids these pitfalls is direct policy optimization without reward inference, such as Direct Preference Optimization (DPO), which provides a much simpler pipeline and has shown empirical success in LLMs. However, DPO utilizes the closed-form expression between the optimal policy and the reward function, which only works under the bandit setting or deterministic MDPs. This paper develops two RLHF algorithms without reward inference, which work for general RL problems beyond bandits and deterministic MDPs, and general preference models beyond the Bradely-Terry model. The key idea is to estimate the local value function difference from human preferences and then approximate the policy gradient with a zeroth-order gradient approximator. For both algorithms, we establish rates of convergence in terms of the number of policy gradient iterations, as well as the number of trajectory samples and human preference queries per iteration. Our results show there exist provably efficient methods to solve general RLHF problems without reward inference.",
    "citation_count": 1,
    "summary": "This paper introduces two novel reinforcement learning from human feedback (RLHF) algorithms that bypass reward inference, directly optimizing policies using zeroth-order gradient approximation based on human preference comparisons. These algorithms are proven to converge efficiently for general reinforcement learning problems and preference models, offering a simpler and more robust alternative to traditional RLHF methods."
  },
  {
    "url": "https://www.lesswrong.com/tag/utility-extraction",
    "title": "Utility Extraction - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Utility extraction aims to automatically determine human preferences for use in Friendly AI, mitigating the risk of misaligned AI goals. This is challenging due to the inherent difficulty in specifying human values and the inconsistencies in human behavior, prompting research into methods that account for these inconsistencies."
  },
  {
    "url": "https://arxiv.org/pdf/2303.02265v1.pdf",
    "title": "Learning to Influence Human Behavior with Offline Reinforcement Learning",
    "published_date": "2023-03-03",
    "abstract": "When interacting with people, AI agents do not just influence the state of the world -- they also influence the actions people take in response to the agent, and even their underlying intentions and strategies. Accounting for and leveraging this influence has mostly been studied in settings where it is sufficient to assume that human behavior is near-optimal: competitive games, or general-sum settings like autonomous driving alongside human drivers. Instead, we focus on influence in settings where there is a need to capture human suboptimality. For instance, imagine a collaborative task in which, due either to cognitive biases or lack of information, people do not perform very well -- how could an agent influence them towards more optimal behavior? Assuming near-optimal human behavior will not work here, and so the agent needs to learn from real human data. But experimenting online with humans is potentially unsafe, and creating a high-fidelity simulator of the environment is often impractical. Hence, we focus on learning from an offline dataset of human-human interactions. Our observation is that offline reinforcement learning (RL) can learn to effectively influence suboptimal humans by extending and combining elements of observed human-human behavior. We demonstrate that offline RL can solve two challenges with effective influence. First, we show that by learning from a dataset of suboptimal human-human interaction on a variety of tasks -- none of which contains examples of successful influence -- an agent can learn influence strategies to steer humans towards better performance even on new tasks. Second, we show that by also modeling and conditioning on human behavior, offline RL can learn to affect not just the human's actions but also their underlying strategy, and adapt to changes in their strategy.",
    "summary": "This paper explores using offline reinforcement learning to influence suboptimal human behavior in collaborative tasks, learning effective influence strategies from a dataset of human-human interactions without requiring online experimentation or high-fidelity simulation. The approach successfully learns to improve human performance on new tasks and adapt to changes in human strategies."
  },
  {
    "url": "https://arxiv.org/pdf/2303.00894.pdf",
    "title": "Active Reward Learning from Multiple Teachers",
    "published_date": "2023-03-02",
    "abstract": "Reward learning algorithms utilize human feedback to infer a reward function, which is then used to train an AI system. This human feedback is often a preference comparison, in which the human teacher compares several samples of AI behavior and chooses which they believe best accomplishes the objective. While reward learning typically assumes that all feedback comes from a single teacher, in practice these systems often query multiple teachers to gather sufficient training data. In this paper, we investigate this disparity, and find that algorithmic evaluation of these different sources of feedback facilitates more accurate and efficient reward learning. We formally analyze the value of information (VOI) when reward learning from teachers with varying levels of rationality, and define and evaluate an algorithm that utilizes this VOI to actively select teachers to query for feedback. Surprisingly, we find that it is often more informative to query comparatively irrational teachers. By formalizing this problem and deriving an analytical solution, we hope to facilitate improvement in reward learning approaches to aligning AI behavior with human values.",
    "citation_count": 11,
    "summary": "This paper analyzes reward learning from multiple human teachers with varying levels of rationality, showing that actively selecting teachers based on their value of information, even prioritizing less rational ones, leads to more accurate and efficient reward function inference. This improves alignment of AI behavior with human values."
  },
  {
    "title": "A Bayesian Approach for Quantifying Data Scarcity when Modeling Human Behavior via Inverse Reinforcement Learning",
    "abstract": "Computational models that formalize complex human behaviors enable study and understanding of such behaviors. However, collecting behavior data required to estimate the parameters of such models is often tedious and resource intensive. Thus, estimating dataset size as part of data collection planning (also known as Sample Size Determination) is important to reduce the time and effort of behavior data collection while maintaining an accurate estimate of model parameters. In this article, we present a sample size determination method based on Uncertainty Quantification (UQ) for a specific Inverse Reinforcement Learning (IRL) model of human behavior, in two cases: (1) pre-hoc experiment design—conducted in the planning stage before any data is collected, to guide the estimation of how many samples to collect; and (2) post-hoc dataset analysis—performed after data is collected, to decide if the existing dataset has sufficient samples and whether more data is needed. We validate our approach in experiments with a realistic model of behaviors of people with Multiple Sclerosis (MS) and illustrate how to pick a reasonable sample size target. Our work enables model designers to perform a deeper, principled investigation of the effects of dataset size on IRL model parameters.",
    "published_date": "2022-07-27",
    "citation_count": 3,
    "url": "https://dl.acm.org/doi/10.1145/3551388",
    "summary": "This paper introduces a Bayesian sample size determination method for Inverse Reinforcement Learning (IRL) models of human behavior, applicable both before and after data collection to optimize data acquisition and ensure accurate model parameter estimation. The method is validated using a model of Multiple Sclerosis patient behavior."
  },
  {
    "url": "https://arxiv.org/pdf/2103.12142.pdf",
    "title": "Combining Reward Information from Multiple Sources",
    "published_date": "2021-03-22",
    "abstract": "Given two sources of evidence about a latent variable, one can combine the information from both by multiplying the likelihoods of each piece of evidence. However, when one or both of the observation models are misspecified, the distributions will conflict. We study this problem in the setting with two conflicting reward functions learned from different sources. In such a setting, we would like to retreat to a broader distribution over reward functions, in order to mitigate the effects of misspecification. We assume that an agent will maximize expected reward given this distribution over reward functions, and identify four desiderata for this setting. We propose a novel algorithm, Multitask Inverse Reward Design (MIRD), and compare it to a range of simple baselines. While all methods must trade off between conservatism and informativeness, through a combination of theory and empirical results on a toy environment, we find that MIRD and its variant MIRD-IF strike a good balance between the two.",
    "citation_count": 3,
    "summary": "This paper addresses the problem of combining conflicting reward signals from multiple sources by modeling a distribution over reward functions rather than relying on a single, potentially misspecified, reward. A novel algorithm, Multitask Inverse Reward Design (MIRD), is proposed and shown to effectively balance conservatism and informativeness in handling such conflicting information."
  },
  {
    "url": "https://arxiv.org/pdf/2111.03026.pdf",
    "title": "B-Pref: Benchmarking Preference-Based Reinforcement Learning",
    "published_date": "2021-11-04",
    "abstract": "Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.",
    "citation_count": 82,
    "summary": "B-Pref is a new benchmark for preference-based reinforcement learning (RL) that addresses the lack of standardized evaluation by simulating diverse, imperfect human preferences, enabling faster algorithm comparison and robustness analysis. It offers metrics beyond simple performance to assess how algorithms handle irrational human input, facilitating more systematic research in preference-based RL."
  },
  {
    "url": "https://arxiv.org/pdf/2104.09237v2.pdf",
    "title": "Inverse Bayesian Optimization: Learning Human Acquisition Functions in an Exploration vs Exploitation Search Task",
    "published_date": "2021-04-16",
    "abstract": "This paper introduces a probabilistic framework to estimate parameters of an acquisition function given observed human behavior that can be modeled as a collection of sample paths from a Bayesian optimization procedure. The methodology involves defining a likelihood on observed human behavior from an optimization task, where the likelihood is parameterized by a Bayesian optimization subroutine governed by an unknown acquisition function. This structure enables us to make inference on a subject's acquisition function while allowing their behavior to deviate around the solution to the Bayesian optimization subroutine. To test our methods, we designed a sequential optimization task which forced subjects to balance exploration and exploitation in search of an invisible target location. Applying our proposed methods to the resulting data, we find that many subjects tend to exhibit exploration preferences beyond that of standard acquisition functions to capture. Guided by the model discrepancies, we augment the candidate acquisition functions to yield a superior fit to the human behavior in this task.",
    "citation_count": 5,
    "summary": "This paper presents a Bayesian framework to infer human acquisition functions from observed behavior in a sequential optimization task, modeling human choices as noisy samples from a Bayesian optimization process with an unknown acquisition function. The framework is used to analyze human exploration-exploitation strategies, revealing preferences beyond standard acquisition functions and guiding the development of improved models."
  }
]