[
  {
    "url": "https://arxiv.org/abs/2402.15757",
    "title": "Batch Active Learning of Reward Functions from Human Preferences",
    "published_date": "2024-02-24",
    "abstract": "Data generation and labeling are often expensive in robot learning. Preference-based learning is a concept that enables reliable labeling by querying users with preference questions. Active querying methods are commonly employed in preference-based learning to generate more informative data at the expense of parallelization and computation time. In this article, we develop a set of novel algorithms, batch active preference-based learning methods, that enable efficient learning of reward functions using as few data samples as possible while still having short query generation times and also retaining parallelizability. We introduce a method based on determinantal point processes for active batch generation and several heuristic-based alternatives. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We showcase one of our algorithms in a study to learn human users' preferences.",
    "citation_count": 2,
    "summary": "This paper introduces novel batch active learning algorithms for efficiently learning reward functions from human preferences, balancing minimal data needs with fast query generation and parallelization capabilities. Experimental results in simulated robotics tasks demonstrate the effectiveness of these algorithms in requiring few queries with short computation times."
  },
  {
    "url": "https://arxiv.org/abs/2402.09401",
    "title": "Reinforcement Learning from Human Feedback with Active Queries",
    "published_date": "2024-02-14",
    "abstract": "Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$ regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of the state-of-the-art DPO method.",
    "citation_count": 12,
    "summary": "This paper introduces active query-based reinforcement learning from human feedback (RLHF) methods to reduce the high cost of human annotation in aligning large language models (LLMs). Their proposed ADPO algorithm, leveraging active learning techniques, achieves performance comparable to state-of-the-art DPO while requiring significantly fewer human preference queries."
  },
  {
    "url": "https://arxiv.org/abs/2403.06003",
    "title": "A Generalized Acquisition Function for Preference-based Reward Learning",
    "published_date": "2024-03-09",
    "abstract": "Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic environment, an assistive robotics environment with domain transfer, and a natural language processing problem with real datasets demonstrate the superior performance of our querying method over the state-of-the-art information gain method.",
    "citation_count": 1,
    "summary": "This paper introduces a generalized acquisition function for preference-based reward learning that optimizes for behavioral equivalence classes of reward functions, rather than precise parameter identification, leading to improved data efficiency in robotic and NLP tasks. Experiments demonstrate superior performance compared to state-of-the-art information gain methods."
  },
  {
    "url": "https://www.lesswrong.com/tag/utility-extraction",
    "title": "Utility Extraction - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Utility extraction aims to automatically learn human preferences for use in Friendly AI, mitigating the risk of misaligned AI goals. This is challenging due to the difficulty in specifying human values and the inconsistency of human behavior, leading to research focusing on methods that account for such inconsistencies."
  },
  {
    "url": "https://www.lesswrong.com/tag/preference",
    "title": "Preference - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Artificial intelligence utilizes a four-step process—preference acquisition, modeling, representation, and reasoning—to manage user preferences, modeling them mathematically and using them for tasks like decision-making and knowledge discovery. This process is particularly applicable to understanding and extrapolating complex, evolving goals."
  },
  {
    "url": "https://arxiv.org/pdf/2303.00894.pdf",
    "title": "Active Reward Learning from Multiple Teachers",
    "published_date": "2023-03-02",
    "abstract": "Reward learning algorithms utilize human feedback to infer a reward function, which is then used to train an AI system. This human feedback is often a preference comparison, in which the human teacher compares several samples of AI behavior and chooses which they believe best accomplishes the objective. While reward learning typically assumes that all feedback comes from a single teacher, in practice these systems often query multiple teachers to gather sufficient training data. In this paper, we investigate this disparity, and find that algorithmic evaluation of these different sources of feedback facilitates more accurate and efficient reward learning. We formally analyze the value of information (VOI) when reward learning from teachers with varying levels of rationality, and define and evaluate an algorithm that utilizes this VOI to actively select teachers to query for feedback. Surprisingly, we find that it is often more informative to query comparatively irrational teachers. By formalizing this problem and deriving an analytical solution, we hope to facilitate improvement in reward learning approaches to aligning AI behavior with human values.",
    "citation_count": 11,
    "summary": "This paper analyzes reward learning from multiple human teachers, showing that actively selecting which teacher to query, based on their estimated rationality and value of information, improves both accuracy and efficiency. Surprisingly, querying less rational teachers can be more informative than querying highly rational ones."
  },
  {
    "url": "https://arxiv.org/pdf/2304.08944.pdf",
    "title": "Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning",
    "published_date": "2023-04-18",
    "abstract": "An appropriate reward function is of paramount importance in specifying a task in reinforcement learning (RL). Yet, it is known to be extremely challenging in practice to design a correct reward function for even simple tasks. Human-in-the-loop (HiL) RL allows humans to communicate complex goals to the RL agent by providing various types of feedback. However, despite achieving great empirical successes, HiL RL usually requires too much feedback from a human teacher and also suffers from insufficient theoretical understanding. In this paper, we focus on addressing this issue from a theoretical perspective, aiming to provide provably feedback-efficient algorithmic frameworks that take human-in-the-loop to specify rewards of given tasks. We provide an active-learning-based RL algorithm that first explores the environment without specifying a reward function and then asks a human teacher for only a few queries about the rewards of a task at some state-action pairs. After that, the algorithm guarantees to provide a nearly optimal policy for the task with high probability. We show that, even with the presence of random noise in the feedback, the algorithm only takes $\\widetilde{O}(H{{\\dim_{R}^2}})$ queries on the reward function to provide an $\\epsilon$-optimal policy for any $\\epsilon>0$. Here $H$ is the horizon of the RL environment, and $\\dim_{R}$ specifies the complexity of the function class representing the reward function. In contrast, standard RL algorithms require to query the reward function for at least $\\Omega(\\operatorname{poly}(d, 1/\\epsilon))$ state-action pairs where $d$ depends on the complexity of the environmental transition.",
    "citation_count": 8,
    "summary": "This paper presents a novel active learning-based reinforcement learning algorithm that significantly reduces the amount of human feedback needed to learn near-optimal policies. The algorithm achieves this by strategically querying a human teacher about the reward function at only a few state-action pairs, guaranteeing an $\\epsilon$-optimal policy with $\\widetilde{O}(H\\dim_R^2)$ reward queries, a substantial improvement over standard RL methods."
  },
  {
    "url": "https://arxiv.org/pdf/2302.14630.pdf",
    "title": "Experience in Engineering Complex Systems: Active Preference Learning with Multiple Outcomes and Certainty Levels",
    "published_date": "2023-02-27",
    "abstract": "Black-box optimization refers to the optimization problem whose objective function and/or constraint sets are either unknown, inaccessible, or non-existent. In many applications, especially with the involvement of humans, the only way to access the optimization problem is through performing physical experiments with the available outcomes being the preference of one candidate with respect to one or many others. Accordingly, the algorithm so-called Active Preference Learning has been developed to exploit this specific information in constructing a surrogate function based on standard radial basis functions, and then forming an easy-to-solve acquisition function which repetitively suggests new decision vectors to search for the optimal solution. Based on this idea, our approach aims to extend the algorithm in such a way that can exploit further information effectively, which can be obtained in reality such as: 5-point Likert type scale for the outcomes of the preference query (i.e., the preference can be described in not only\"this is better than that\"but also\"this is much better than that\"level), or multiple outcomes for a single preference query with possible additive information on how certain the outcomes are. The validation of the proposed algorithm is done through some standard benchmark functions, showing a promising improvement with respect to the state-of-the-art algorithm.",
    "summary": "This paper presents an improved active preference learning algorithm for black-box optimization that incorporates multiple outcomes and certainty levels (e.g., Likert scales) from preference queries, enhancing the efficiency of finding optimal solutions compared to existing methods. The algorithm uses radial basis functions to build a surrogate model and a novel acquisition function to guide the search."
  }
]