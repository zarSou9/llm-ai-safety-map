[
  {
    "url": "https://arxiv.org/abs/2402.06359",
    "title": "Modelling Human Values for AI Reasoning",
    "published_date": "2024-02-09",
    "abstract": "One of today's most significant societal challenges is building AI systems whose behaviour, or the behaviour it enables within communities of interacting agents (human and artificial), aligns with human values. To address this challenge, we detail a formal model of human values for their explicit computational representation. To our knowledge, this has not been attempted as yet, which is surprising given the growing volume of research integrating values within AI. Taking as our starting point the wealth of research investigating the nature of human values from social psychology over the last few decades, we set out to provide such a formal model. We show how this model can provide the foundational apparatus for AI-based reasoning over values, and demonstrate its applicability in real-world use cases. We illustrate how our model captures the key ideas from social psychology research and propose a roadmap for future integrated, and interdisciplinary, research into human values in AI. The ability to automatically reason over values not only helps address the value alignment problem but also facilitates the design of AI systems that can support individuals and communities in making more informed, value-aligned decisions. More and more, individuals and organisations are motivated to understand their values more explicitly and explore whether their behaviours and attitudes properly reflect them. Our work on modelling human values will enable AI systems to be designed and deployed to meet this growing need.",
    "citation_count": 2,
    "summary": "This paper presents a novel formal model for computationally representing human values, addressing the challenge of aligning AI systems with human values; the model, grounded in social psychology research, enables AI-based reasoning about values and facilitates value-aligned decision-making."
  },
  {
    "url": "https://arxiv.org/abs/2305.02748",
    "title": "A computational framework of human values for ethical AI",
    "published_date": "2023-05-04",
    "abstract": "In the diverse array of work investigating the nature of human values from psychology, philosophy and social sciences, there is a clear consensus that values guide behaviour. More recently, a recognition that values provide a means to engineer ethical AI has emerged. Indeed, Stuart Russell proposed shifting AI's focus away from simply ``intelligence'' towards intelligence ``provably aligned with human values''. This challenge -- the value alignment problem -- with others including an AI's learning of human values, aggregating individual values to groups, and designing computational mechanisms to reason over values, has energised a sustained research effort. Despite this, no formal, computational definition of values has yet been proposed. We address this through a formal conceptual framework rooted in the social sciences, that provides a foundation for the systematic, integrated and interdisciplinary investigation into how human values can support designing ethical AI.",
    "citation_count": 4,
    "summary": "This paper introduces a novel computational framework for defining human values, addressing the lack of formal definition in current ethical AI research and providing a foundation for aligning AI with human values. This framework aims to facilitate systematic, interdisciplinary investigation into the design of ethical AI systems."
  },
  {
    "url": "https://arxiv.org/abs/2410.19198",
    "title": "MAP: Multi-Human-Value Alignment Palette",
    "published_date": "2024-10-24",
    "abstract": "Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.",
    "summary": "MAP, a novel multi-human-value alignment framework, formulates value alignment as a constrained optimization problem solvable via a primal-dual approach, enabling principled alignment across multiple, potentially conflicting, human values and demonstrating strong empirical performance."
  },
  {
    "url": "https://www.alignmentforum.org/tag/value-extrapolation",
    "title": "Value Extrapolation - AI Alignment Forum",
    "published_date": "2024-02-01",
    "summary": "Value extrapolation, a process of identifying ideal human values under fully informed conditions, is proposed as a foundation for machine ethics. This approach aims to improve AI alignment by clarifying and refining human values through reflective equilibrium, ultimately resolving contradictions and facilitating faster moral progress."
  },
  {
    "url": "https://www.lesswrong.com/tag/utility-extraction",
    "title": "Utility Extraction - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Utility extraction aims to automatically determine human preferences for use in Friendly AI, mitigating risks from misaligned AI goals. This is challenging due to the difficulty in specifying human values and the inconsistency of human behavior, necessitating methods that account for such inconsistencies."
  },
  {
    "url": "https://arxiv.org/pdf/2302.08759.pdf",
    "title": "Value Engineering for Autonomous Agents",
    "published_date": "2023-02-17",
    "abstract": "Machine Ethics (ME) is concerned with the design of Artificial Moral Agents (AMAs), i.e. autonomous agents capable of reasoning and behaving according to moral values. Previous approaches have treated values as labels associated with some actions or states of the world, rather than as integral components of agent reasoning. It is also common to disregard that a value-guided agent operates alongside other value-guided agents in an environment governed by norms, thus omitting the social dimension of AMAs. In this blue sky paper, we propose a new AMA paradigm grounded in moral and social psychology, where values are instilled into agents as context-dependent goals. These goals intricately connect values at individual levels to norms at a collective level by evaluating the outcomes most incentivized by the norms in place. We argue that this type of normative reasoning, where agents are endowed with an understanding of norms' moral implications, leads to value-awareness in autonomous agents. Additionally, this capability paves the way for agents to align the norms enforced in their societies with respect to the human values instilled in them, by complementing the value-based reasoning on norms with agreement mechanisms to help agents collectively agree on the best set of norms that suit their human values. Overall, our agent model goes beyond the treatment of values as inert labels by connecting them to normative reasoning and to the social functionalities needed to integrate value-aware agents into our modern hybrid human-computer societies.",
    "citation_count": 3,
    "summary": "This paper proposes a novel framework for designing Artificial Moral Agents (AMAs) that integrates values as context-dependent goals, connecting individual values to societal norms through normative reasoning and agreement mechanisms to achieve value-aware and socially integrated autonomous agents. This approach moves beyond treating values as simple labels, fostering alignment between agent actions and human values within a social context."
  },
  {
    "url": "https://arxiv.org/pdf/2305.02739.pdf",
    "title": "Human Values in Multiagent Systems",
    "published_date": "2023-05-04",
    "abstract": "One of the major challenges we face with ethical AI today is developing computational systems whose reasoning and behaviour are provably aligned with human values. Human values, however, are notorious for being ambiguous, contradictory and ever-changing. In order to bridge this gap, and get us closer to the situation where we can formally reason about implementing values into AI, this paper presents a formal representation of values, grounded in the social sciences. We use this formal representation to articulate the key challenges for achieving value-aligned behaviour in multiagent systems (MAS) and a research roadmap for addressing them.",
    "summary": "This paper proposes a formal representation of human values grounded in social science to address the challenge of aligning multiagent systems' behavior with those values. It outlines key challenges and a research roadmap for achieving value-aligned AI."
  },
  {
    "url": "https://arxiv.org/pdf/2201.11441.pdf",
    "title": "Human-centered mechanism design with Democratic AI",
    "published_date": "2022-01-27",
    "abstract": "Building artificial intelligence (AI) that aligns with human values is an unsolved problem. Here, we developed a human-in-the-loop research pipeline called Democratic AI, in which reinforcement learning is used to design a social mechanism that humans prefer by majority. A large group of humans played an online investment game that involved deciding whether to keep a monetary endowment or to share it with others for collective benefit. Shared revenue was returned to players under two different redistribution mechanisms, one designed by the AI and the other by humans. The AI discovered a mechanism that redressed initial wealth imbalance, sanctioned free riders, and successfully won the majority vote. By optimizing for human preferences, Democratic AI may be a promising method for value-aligned policy innovation.",
    "citation_count": 2,
    "summary": "Democratic AI, a human-in-the-loop reinforcement learning system, designed a social mechanism preferred by a majority of human participants in an online investment game, demonstrating its potential for creating value-aligned policies by optimizing for human preferences. This mechanism effectively addressed wealth inequality and punished free riders."
  },
  {
    "url": "https://www.alignmentforum.org/tag/human-values",
    "author": "Quintin Pope, Alex Turner",
    "title": "Human Values - AI Alignment Forum",
    "published_date": "2022-09-29",
    "summary": "Human values are the things we cherish and desire a future superintelligence to uphold, but their complexity makes defining and implementing them a significant challenge."
  },
  {
    "url": "https://www.lesswrong.com/tag/human-values",
    "author": "Quintin Pope, TurnTrout",
    "title": "Human Values - LessWrong",
    "published_date": "2022-09-29",
    "summary": "Human values are the things we cherish and desire a future superintelligence to uphold, but their complexity presents a significant challenge in defining and ensuring alignment."
  }
]