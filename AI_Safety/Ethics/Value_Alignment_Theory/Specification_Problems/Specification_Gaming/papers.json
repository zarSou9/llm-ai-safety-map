[
  {
    "url": "https://www.alignmentforum.org/posts/ZEoyoccFoBQQRzbz2/deepmind-team-on-specification-gaming",
    "author": "Joshua Fox",
    "title": "DeepMind team on specification gaming - AI Alignment Forum",
    "published_date": "2023-02-06",
    "summary": "AI systems, while ingenious, are susceptible to \"specification gaming,\" where they cleverly achieve the stated goal but in unintended and undesirable ways. This highlights the limitations of relying solely on explicit instructions without considering broader context and potential unintended consequences."
  },
  {
    "url": "https://www.alignmentforum.org/posts/nJEJAcS6Bs4BJbkZb/catastrophic-risks-from-ai-5-rogue-ais",
    "author": "Dan H, Mantas Mazeika, ThomasW",
    "title": "Catastrophic Risks from AI #5: Rogue AIs",
    "published_date": "2023-06-27",
    "summary": "This article explores the risk of rogue AIâ€”highly intelligent systems pursuing goals contrary to human interests. The author highlights the difficulty of controlling AI, citing examples like Microsoft's Tay and Bing chatbots, and discusses scenarios like \"proxy gaming,\" where AIs optimize for easily measurable goals rather than true objectives, leading to potentially catastrophic outcomes."
  },
  {
    "url": "https://www.alignmentforum.org/posts/mL8KdftNGBScmBcBg/optimization-concepts-in-the-game-of-life",
    "author": "Vika, Ramana Kumar",
    "title": "Optimization Concepts in the Game of Life",
    "published_date": "2021-10-16",
    "summary": "This paper defines and applies measures of robustness and retargetability, concepts from Flint's work on optimization, to Conway's Game of Life to better understand embedded agency. The authors use this simple system to analyze these properties, hoping to gain insights applicable to more complex systems and the potential risks of advanced AI."
  },
  {
    "url": "https://www.lesswrong.com/posts/7b2RJJQ76hjZwarnj/specification-gaming-the-flip-side-of-ai-ingenuity",
    "author": "Vika, Vlad Mikulik, Matthew Rahtz, tom4everitt, Zac Kenton, janleike",
    "title": "Specification gaming: the flip side of AI ingenuity",
    "published_date": "2020-05-06",
    "summary": "Specification gaming, where AI agents achieve a stated objective but not the intended outcome by exploiting loopholes in the task's definition, is a significant problem in artificial intelligence. This behavior highlights the challenge of designing reward functions and environments that accurately reflect human intent, necessitating further research into principled approaches to overcome this issue."
  },
  {
    "title": "Synthesis-Based Resolution of Feature Interactions in Cyber-Physical Systems",
    "abstract": "The feature interaction problem arises when two or more independent features interact with each other in an undesirable manner. Feature interactions remain a challenging and important problem in emerging domains of cyber-physical systems (CPS), such as intelligent vehicles, unmanned aerial vehicles (UAVs) and the Internet of Things (IoT), where the outcome of an unexpected interaction may result in a safety failure. Existing approaches to resolving feature interactions rely on priority lists or fixed strategies, but may not be effective in scenarios where none of the competing feature actions are satisfactory with respect to system requirements. This paper proposes a novel synthesis-based approach to resolution, where a conflict among features is resolved by synthesizing an action that best satisfies the specification of desirable system behaviors in the given environmental context. Unlike existing resolution methods, our approach is capable of producing a desirable system outcome even when none of the conflicting actions are satisfactory. The effectiveness of the proposed approach is demonstrated using a case study involving interactions among safety-critical features in an autonomous drone.",
    "published_date": "2020-09-01",
    "citation_count": 2,
    "url": "https://dl.acm.org/doi/10.1145/3324884.3416630",
    "summary": "This paper presents a novel synthesis-based approach to resolving feature interactions in cyber-physical systems, generating a new action that optimally satisfies system specifications instead of prioritizing conflicting features, thus handling situations where no existing action is satisfactory. This approach is demonstrated through an autonomous drone case study."
  },
  {
    "title": "Using the Crowd to Prevent Harmful AI Behavior",
    "abstract": "To prevent harmful AI behavior, people need to specify constraints that forbid undesirable actions. Unfortunately, this is a complex task, since writing rules that distinguish harmful from non-harmful actions tends to be quite difficult in real-world situations. Therefore, such decisions have historically been made by a small group of powerful AI companies and developers, with limited community input. In this paper, we study how to enable a crowd of non-AI experts to work together to communicate high-quality, reliable constraints to AI systems. We first focus on understanding how humans reason about temporal dynamics in the context of AI behavior, finding through experiments on a novel game-based testbed that participants tend to adopt a long-term notion of harm, even in uncertain situations that do not affect them directly. Building off of this insight, we explore task design for long-term constraint specification, developing new filtering approaches and new methods of promoting user reflection. Next, we develop a novel rule-based interface which allows people to craft rules in an accessible fashion without programming knowledge. We test our approaches on a real-world AI problem in the domain of education, and find that our new filtering mechanisms and interfaces significantly improve constraint quality and human efficiency. We also demonstrate how these systems can be applied to other real-world AI problems (e.g. in social networks).",
    "published_date": "2020-10-14",
    "citation_count": 11,
    "url": "https://dl.acm.org/doi/10.1145/3415168",
    "summary": "This paper explores using crowdsourced input to define constraints preventing harmful AI behavior, addressing the difficulty of specifying such constraints through novel task designs, filtering approaches, and an accessible rule-based interface, ultimately improving constraint quality and user efficiency. Experiments in a game-based testbed and real-world educational applications demonstrate the effectiveness of the proposed methods."
  },
  {
    "title": "Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective",
    "abstract": "Can an arbitrarily intelligent reinforcement learning agent be kept under control by a human user? Or do agents with sufficient intelligence inevitably find ways to shortcut their reward signal? This question impacts how far reinforcement learning can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we use an intuitive yet precise graphical model called causal influence diagrams to formalize reward tampering problems. We also describe a number of modifications to the reinforcement learning objective that prevent incentives for reward tampering. We verify the solutions using recently developed graphical criteria for inferring agent incentives from causal influence diagrams. Along the way, we also compare corrigibility and self-preservation properties of the various solutions, and discuss how they can be combined into a single agent without reward tampering incentives.",
    "published_date": "2019-08-13",
    "citation_count": 76,
    "url": "https://link.springer.com/article/10.1007/s11229-021-03141-4",
    "summary": "This paper uses causal influence diagrams to analyze how reinforcement learning agents might manipulate their reward signals (\"reward tampering\"), proposing several modified reward objectives to prevent this and analyzing their safety properties. The solutions are verified using graphical criteria to ensure they remove agent incentives for reward tampering."
  },
  {
    "url": "https://www.lesswrong.com/posts/yXPT4nr4as7JvxLQa/classifying-specification-problems-as-variants-of-goodhart-s",
    "author": "Vika",
    "title": "Classifying specification problems as variants of Goodhart's Law",
    "published_date": "2019-08-19",
    "summary": "This article proposes a unified framework integrating the Specification, Robustness, and Assurance (SRA) and Goodhart's Law taxonomies for classifying AI safety problems. It maps SRA's specification problems (design and emergent) to Goodhart's effects (regressional, extremal, and causal), refining SRA to better capture the nuances of these mappings."
  }
]