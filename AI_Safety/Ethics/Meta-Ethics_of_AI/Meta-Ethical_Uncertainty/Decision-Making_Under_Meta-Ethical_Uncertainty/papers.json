[
  {
    "url": "https://arxiv.org/abs/2408.01253",
    "title": "Metareasoning in uncertain environments: a meta-BAMDP framework",
    "published_date": "2024-08-02",
    "abstract": "In decision-making scenarios, \\textit{reasoning} can be viewed as an algorithm $P$ that makes a choice of an action $a^* \\in \\mathcal{A}$, aiming to optimize some outcome such as maximizing the value function of a Markov decision process (MDP). However, executing $P$ itself may bear some costs (time, energy, limited capacity, etc.) and needs to be considered alongside explicit utility obtained by making the choice in the underlying decision problem. Such costs need to be taken into account in order to accurately model human behavior, as well as optimizing AI planning, as all physical systems are bound to face resource constraints. Finding the right $P$ can itself be framed as an optimization problem over the space of reasoning processes $P$, generally referred to as \\textit{metareasoning}. Conventionally, human metareasoning models assume that the agent knows the transition and reward distributions of the underlying MDP. This paper generalizes such models by proposing a meta Bayes-Adaptive MDP (meta-BAMDP) framework to handle metareasoning in environments with unknown reward/transition distributions, which encompasses a far larger and more realistic set of planning problems that humans and AI systems face. As a first step, we apply the framework to two-armed Bernoulli bandit (TABB) tasks, which have often been used to study human decision making. Owing to the meta problem's complexity, our solutions are necessarily approximate, but nevertheless robust within a range of assumptions that are arguably realistic for human decision-making scenarios. These results offer a normative framework for understanding human exploration under cognitive constraints. This integration of Bayesian adaptive strategies with metareasoning enriches both the theoretical landscape of decision-making research and practical applications in designing AI systems that plan under uncertainty and resource constraints.",
    "summary": "This paper introduces a meta-Bayes-Adaptive Markov Decision Process (meta-BAMDP) framework for metareasoning in environments with uncertain rewards and transitions, addressing the cost of reasoning alongside action utility. The framework, applied to two-armed Bernoulli bandit tasks, provides a more realistic model of human and AI decision-making under resource constraints."
  },
  {
    "url": "https://arxiv.org/abs/2410.21882",
    "title": "Building Altruistic and Moral AI Agent with Brain-inspired Affective Empathy Mechanisms",
    "published_date": "2024-10-29",
    "abstract": "As AI closely interacts with human society, it is crucial to ensure that its decision-making is safe, altruistic, and aligned with human ethical and moral values. However, existing research on embedding ethical and moral considerations into AI remains insufficient, and previous external constraints based on principles and rules are inadequate to provide AI with long-term stability and generalization capabilities. In contrast, the intrinsic altruistic motivation based on empathy is more willing, spontaneous, and robust. Therefore, this paper is dedicated to autonomously driving intelligent agents to acquire morally behaviors through human-like affective empathy mechanisms. We draw inspiration from the neural mechanism of human brain's moral intuitive decision-making, and simulate the mirror neuron system to construct a brain-inspired affective empathy-driven altruistic decision-making model. Here, empathy directly impacts dopamine release to form intrinsic altruistic motivation. Based on the principle of moral utilitarianism, we design the moral reward function that integrates intrinsic empathy and extrinsic self-task goals. A comprehensive experimental scenario incorporating empathetic processes, personal objectives, and altruistic goals is developed. The proposed model enables the agent to make consistent moral decisions (prioritizing altruism) by balancing self-interest with the well-being of others. We further introduce inhibitory neurons to regulate different levels of empathy and verify the positive correlation between empathy levels and altruistic preferences, yielding conclusions consistent with findings from psychological behavioral experiments. This work provides a feasible solution for the development of ethical AI by leveraging the intrinsic human-like empathy mechanisms, and contributes to the harmonious coexistence between humans and AI.",
    "summary": "This paper proposes a brain-inspired model for building altruistic AI agents, using simulated mirror neurons and dopamine release to create affective empathy that drives moral decision-making, balancing self-interest with the well-being of others based on utilitarian principles. The model's performance is validated through experiments showing a positive correlation between empathy levels and altruistic behavior."
  },
  {
    "url": "https://arxiv.org/abs/2305.01424v1",
    "title": "Uncertain Machine Ethical Decisions Using Hypothetical Retrospection",
    "published_date": "2023-05-02",
    "abstract": "We propose the use of the hypothetical retrospection argumentation procedure, developed by Sven Ove Hansson to improve existing approaches to machine ethical reasoning by accounting for probability and uncertainty from a position of Philosophy that resonates with humans. Actions are represented with a branching set of potential outcomes, each with a state, utility, and either a numeric or poetic probability estimate. Actions are chosen based on comparisons between sets of arguments favouring actions from the perspective of their branches, even those branches that led to an undesirable outcome. This use of arguments allows a variety of philosophical theories for ethical reasoning to be used, potentially in flexible combination with each other. We implement the procedure, applying consequentialist and deontological ethical theories, independently and concurrently, to an autonomous library system use case. We introduce a preliminary framework that seems to meet the varied requirements of a machine ethics system: versatility under multiple theories and a resonance with humans that enables transparency and explainability.",
    "citation_count": 2,
    "summary": "This paper introduces a machine ethical decision-making framework using hypothetical retrospection, which incorporates probabilistic reasoning and multiple ethical theories (e.g., consequentialist and deontological) to select actions based on argumentation from potentially undesirable outcomes, enhancing transparency and human understanding. The framework is implemented and tested on an autonomous library system."
  },
  {
    "url": "https://arxiv.org/abs/2301.08491v1",
    "title": "Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning",
    "published_date": "2023-01-20",
    "abstract": "Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.\n\n\n\nIn this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm-based agents, between morality based on societal norms or internal virtues, and between single- and mixed-virtue (e.g., multi-objective) methodologies. Then, we evaluate our approach by modeling repeated dyadic interactions between learning moral agents in three iterated social dilemma games (Prisoner's Dilemma, Volunteer's Dilemma and Stag Hunt). We analyze the impact of different types of morality on the emergence of cooperation, defection or exploitation, and the corresponding social outcomes. Finally, we discuss the implications of these findings for the development of moral agents in artificial and mixed human-AI societies.",
    "citation_count": 15,
    "summary": "This paper investigates the emergence of moral behavior in multi-agent reinforcement learning by modeling agents with reward functions based on different ethical theories (consequence-based, norm-based, single/mixed-virtue) in iterated social dilemma games. The authors analyze how these different moral frameworks influence cooperation and social outcomes, offering insights into developing ethical AI agents."
  },
  {
    "url": "https://www.lesswrong.com/posts/nnGwHuJfCBxKDgsds/embedding-ethical-priors-into-ai-systems-a-bayesian-approach",
    "author": "Justausername",
    "title": "Embedding Ethical Priors into AI Systems: A Bayesian Approach",
    "published_date": "2023-08-03",
    "summary": "This paper proposes a novel framework for building ethically aligned AI by incorporating ethical principles as Bayesian priors into AI learning processes. This approach, inspired by human moral intuitions, aims to guide AI decision-making towards ethically sound judgments."
  },
  {
    "url": "https://www.lesswrong.com/posts/fJqP9WcnHXBRBeiBg/meta-questions-about-metaphilosophy",
    "author": "Wei Dai",
    "title": "Meta Questions about Metaphilosophy",
    "published_date": "2023-09-01",
    "summary": "The author details their intellectual journey from science to metaphilosophy, noting a dwindling number of colleagues at each increasingly abstract level. This journey, driven by curiosity and concern for AI safety, culminates in a questioning of why so few others are focused on ensuring AI's philosophical competence."
  },
  {
    "url": "https://arxiv.org/pdf/2202.12039.pdf",
    "title": "Metacognitive Agents for Ethical Decision Support: Conceptual Model and Research Roadmap",
    "published_date": "2022-02-24",
    "abstract": "An ethical value-action gap exists when there is a discrepancy between intentions and actions. For example, people who support environmental sustainability often use cars and short-haul flights because of convenience and time-pressure. This discrepancy may be caused by social and structural obstacles as well as cognitive biases. Current technology can make this worse. For example, social media tends to enhance emotions such as anger or fear, which can result in polarisation and impulsive decisions. Computational models of cognition and affect can provide insights into the value-action gap and how it can be reduced. Such models include dual process architectures, emotion models and behaviour change theories. In particular, metacognition (“thinking about thinking”) plays an important role in many of these models as a mechanism for self-regulation and for reasoning about mental attitudes. This paper outlines a roadmap for translating cognitive-affective models into assistant agents to help make value-aligned decisions. Key principles include “agile” rapid-prototyping using agentbased simulation, and the combination of descriptive and normative models into a single agent architecture.",
    "summary": "This paper proposes using metacognitive agent-based models to bridge the ethical value-action gap, leveraging computational models of cognition and affect to create decision support systems that promote value-aligned behavior. The roadmap emphasizes agile prototyping and integrating descriptive and normative models within a single agent architecture."
  },
  {
    "url": "https://arxiv.org/pdf/2204.07612v1.pdf",
    "title": "Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in Artificial Intelligence",
    "published_date": "2022-04-15",
    "abstract": "In this meta-ethnography,we explore three different angles of Ethical AI design and implementation in a top-down/bottom-up framework, including the philosophical ethical viewpoint, the technical perspective, and framing through a political lens. We will discuss the values and drawbacks of individual and hybrid approaches within this framework. Examples of approaches include ethics either being determined by corporations and governments (coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technical-ities of how AI is developed within a moral construct, in consideration of its developers and users, with expected and unexpected consequences and long-term impact. This investigation includes real-world case studies, philosophical debate, and theoretical future thought experimentation based on historical fact, current world circumstances, and possible ensuing realities.",
    "summary": "This meta-ethnography analyzes top-down, bottom-up, and hybrid approaches to integrating ethics into AI, considering philosophical, technical, and political perspectives, and evaluating their respective strengths and weaknesses through case studies and theoretical exploration. The analysis examines how ethical considerations are incorporated into AI development and deployment from corporate/governmental levels versus user-driven demands."
  }
]