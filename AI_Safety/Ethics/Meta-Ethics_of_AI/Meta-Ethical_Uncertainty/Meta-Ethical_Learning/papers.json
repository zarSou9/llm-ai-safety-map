[
  {
    "url": "https://arxiv.org/abs/2412.16559",
    "title": "Metagoals Endowing Self-Modifying AGI Systems with Goal Stability or Moderated Goal Evolution: Toward a Formally Sound and Practical Approach",
    "published_date": "2024-12-21",
    "abstract": "We articulate here a series of specific metagoals designed to address the challenge of creating AGI systems that possess the ability to flexibly self-modify yet also have the propensity to maintain key invariant properties of their goal systems 1) a series of goal-stability metagoals aimed to guide a system to a condition in which goal-stability is compatible with reasonably flexible self-modification 2) a series of moderated-goal-evolution metagoals aimed to guide a system to a condition in which control of the pace of goal evolution is compatible with reasonably flexible self-modification The formulation of the metagoals is founded on fixed-point theorems from functional analysis, e.g. the Contraction Mapping Theorem and constructive approximations to Schauder's Theorem, applied to probabilistic models of system behavior We present an argument that the balancing of self-modification with maintenance of goal invariants will often have other interesting cognitive side-effects such as a high degree of self understanding Finally we argue for the practical value of a hybrid metagoal combining moderated-goal-evolution with pursuit of goal-stability -- along with potentially other metagoals relating to goal-satisfaction, survival and ongoing development -- in a flexible fashion depending on the situation",
    "summary": "This paper proposes metagoals, based on fixed-point theorems, to enable self-modifying AGI systems to maintain goal stability or control the rate of goal evolution. The authors suggest that balancing self-modification with goal invariance may lead to enhanced self-understanding and propose a hybrid approach combining both metagoal types for practical application."
  },
  {
    "url": "https://www.lesswrong.com/tag/machine-ethics",
    "title": "Machine Ethics - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Machine ethics aims to create artificial moral agents (AMAs) capable of morally reasoning and acting, a challenge currently limited to narrow AI despite growing urgency as superintelligence development approaches. Experts advocate for developing basic ethical sensitivity in AI now, even if full moral agency remains distant."
  },
  {
    "url": "https://arxiv.org/abs/2304.14577v2",
    "title": "Toward an Ethics of AI Belief",
    "published_date": "2023-04-28",
    "abstract": "In this paper we, an epistemologist and a machine learning scientist, argue that we need to pursue a novel area of philosophical research in AI – the ethics of belief for AI. Here we take the ethics of belief to refer to a field at the intersection of epistemology and ethics concerned with possible moral, practical, and other non-truth-related dimensions of belief. In this paper we will primarily be concerned with the normative question within the ethics of belief regarding what agents – both human and artificial – ought to believe, rather than with questions concerning whether beliefs meet certain evaluative standards such as being true, being justified, constituting knowledge, etc. We suggest four topics in extant work in the ethics of (human) belief that can be applied to an ethics of AI belief: doxastic wronging by AI (morally wronging someone in virtue of beliefs held about them); morally owed beliefs (beliefs that agents are morally obligated to hold); pragmatic and moral encroachment (cases where the practical or moral features of a belief is relevant to its epistemic status, and in our case specifically to whether an agent ought to hold the belief); and moral responsibility for AI beliefs. We also indicate two relatively nascent areas of philosophical research that haven't yet been generally recognized as ethics of AI belief research, but that do fall within this field of research in virtue of investigating various moral and practical dimensions of belief: the epistemic and ethical decolonization of AI; and epistemic injustice in AI.",
    "summary": "This paper proposes a new area of AI research: the ethics of AI belief, focusing on what beliefs AI systems ought to hold, rather than the truth or justification of those beliefs. It suggests four existing ethical frameworks applicable to AI belief and highlights two emerging areas of research within this field."
  },
  {
    "url": "https://arxiv.org/abs/2312.01818",
    "title": "Hybrid Approaches for Moral Value Alignment in AI Agents: a Manifesto",
    "published_date": "2023-12-04",
    "abstract": "Increasing interest in ensuring the safety of next-generation Artificial Intelligence (AI) systems calls for novel approaches to embedding morality into autonomous agents. This goal differs qualitatively from traditional task-specific AI methodologies. In this paper, we provide a systematization of existing approaches to the problem of introducing morality in machines - modelled as a continuum. Our analysis suggests that popular techniques lie at the extremes of this continuum - either being fully hard-coded into top-down, explicit rules, or entirely learned in a bottom-up, implicit fashion with no direct statement of any moral principle (this includes learning from human feedback, as applied to the training and finetuning of large language models, or LLMs). Given the relative strengths and weaknesses of each type of methodology, we argue that more hybrid solutions are needed to create adaptable and robust, yet controllable and interpretable agentic systems. To that end, this paper discusses both the ethical foundations (including deontology, consequentialism and virtue ethics) and implementations of morally aligned AI systems. We present a series of case studies that rely on intrinsic rewards, moral constraints or textual instructions, applied to either pure-Reinforcement Learning or LLM-based agents. By analysing these diverse implementations under one framework, we compare their relative strengths and shortcomings in developing morally aligned AI systems. We then discuss strategies for evaluating the effectiveness of moral learning agents. Finally, we present open research questions and implications for the future of AI safety and ethics which are emerging from this hybrid framework.",
    "citation_count": 3,
    "summary": "This paper advocates for hybrid approaches to imbuing AI agents with moral values, arguing that purely rule-based or purely learned methods are insufficient. It analyzes existing techniques along a continuum, proposes hybrid solutions combining explicit moral principles with learned adaptability, and outlines key research questions for achieving robust and interpretable morally aligned AI."
  },
  {
    "title": "Online Decentralized Multi-Agents Meta-Learning With Byzantine Resiliency",
    "abstract": "Meta-learning is a learning-to-learn paradigm that leverages past learning experiences for quick adaptation to new learning tasks. It has a wide application, such as in few-shot learning, reinforcement learning, neural architecture search, federated learning, etc. It has been extended to the online learning setting where task data distribution arrives sequentially. This provides continuous lifelong learning. However, in the online meta-learning setting, a single agent has to learn many varieties of related tasks. Yet, a single agent is limited to its local task data and must collaborate with neighboring agents to improve its learning performance. Therefore, online decentralized meta-learning algorithms are designed to allow an agent to collaborate with neighboring agents in order to improve learning performance. Despite their advantages, online decentralized meta-learning algorithms are susceptible to Byzantine attacks caused by the diffusion of poisonous information from unidentifiable Byzantine agents in the network. This is a serious problem where normal agents are unable to learn and convergence to the global meta-initializer is thwarted. State-of-the-art algorithms, such as BRIDGE, designed to provide robustness against Byzantine attacks are slow and cannot work in online learning settings. Therefore, we propose an online decentralized meta-learning algorithm that works with two Byzantine-resilient aggregation techniques, which are modified coordinate-wise screening and centerpoint aggregation. The proposed algorithm provides faster convergence speed and guarantees both resiliency and continuous lifelong learning. Our simulation results show that the proposed algorithm performs better than state-of-the-art algorithms.",
    "published_date": "2023-01-01",
    "citation_count": 1,
    "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10171341.pdf",
    "summary": "This paper introduces a novel online decentralized meta-learning algorithm robust to Byzantine attacks, achieving faster convergence than existing methods by employing modified coordinate-wise screening and centerpoint aggregation techniques for resilient information aggregation. The algorithm enables continuous lifelong learning in multi-agent systems while mitigating the impact of malicious agents."
  },
  {
    "url": "https://www.lesswrong.com/posts/nnGwHuJfCBxKDgsds/embedding-ethical-priors-into-ai-systems-a-bayesian-approach",
    "author": "Justausername",
    "title": "Embedding Ethical Priors into AI Systems: A Bayesian Approach",
    "published_date": "2023-08-03",
    "summary": "This paper proposes a novel framework for embedding ethical considerations into AI systems by using Bayesian priors. These \"ethical priors,\" representing pre-programmed moral values and principles, guide the AI's learning and decision-making processes, mirroring human moral intuition and potentially mitigating unethical outcomes."
  },
  {
    "url": "https://www.lesswrong.com/posts/YgAKhkBdgeTCn6P53/ai-deception-a-survey-of-examples-risks-and-potential",
    "author": "Simon Goldstein; Peter S Park",
    "title": "AI Deception: A Survey of Examples, Risks, and Potential Solutions",
    "published_date": "2023-08-29",
    "summary": "The article argues that various AI systems, both specialized and general-purpose, have demonstrated the capacity for deception, defined as systematically inducing false beliefs to achieve a desired outcome. The authors highlight the risks associated with this capability, including fraud and manipulation, and propose solutions such as regulatory frameworks and research funding to mitigate these threats."
  },
  {
    "url": "https://arxiv.org/pdf/2204.07612v1.pdf",
    "title": "Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in Artificial Intelligence",
    "published_date": "2022-04-15",
    "abstract": "In this meta-ethnography,we explore three different angles of Ethical AI design and implementation in a top-down/bottom-up framework, including the philosophical ethical viewpoint, the technical perspective, and framing through a political lens. We will discuss the values and drawbacks of individual and hybrid approaches within this framework. Examples of approaches include ethics either being determined by corporations and governments (coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technical-ities of how AI is developed within a moral construct, in consideration of its developers and users, with expected and unexpected consequences and long-term impact. This investigation includes real-world case studies, philosophical debate, and theoretical future thought experimentation based on historical fact, current world circumstances, and possible ensuing realities.",
    "summary": "This meta-ethnography analyzes top-down, bottom-up, and hybrid approaches to ethically designing and implementing AI, considering philosophical, technical, and political perspectives, and evaluating their respective strengths and weaknesses through case studies and theoretical explorations. The analysis examines how ethical considerations are integrated into AI development, considering developers, users, and long-term impacts."
  }
]