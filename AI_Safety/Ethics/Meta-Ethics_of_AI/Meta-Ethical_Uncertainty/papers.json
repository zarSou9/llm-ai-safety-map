[
  {
    "url": "https://arxiv.org/pdf/2006.04734v3.pdf",
    "title": "Reinforcement Learning Under Moral Uncertainty",
    "published_date": "2020-06-08",
    "abstract": "An ambitious goal for artificial intelligence is to create agents that behave ethically: The capacity to abide by human moral norms would greatly expand the context in which autonomous agents could be practically and safely deployed. While ethical agents could be trained through reinforcement, by rewarding correct behavior under a specific moral theory (e.g. utilitarianism), there remains widespread disagreement (both societally and among moral philosophers) about the nature of morality and what ethical theory (if any) is objectively correct. Acknowledging such disagreement, recent work in moral philosophy proposes that ethical behavior requires acting under moral uncertainty, i.e. to take into account when acting that one's credence is split across several plausible ethical theories. Inspired by such work, this paper proposes a formalism that translates such insights to the field of reinforcement learning. Demonstrating the formalism's potential, we then train agents in simple environments to act under moral uncertainty, highlighting how such uncertainty can help curb extreme behavior from commitment to single theories. The overall aim is to draw productive connections from the fields of moral philosophy and machine ethics to that of machine learning, to inspire further research by highlighting a spectrum of machine learning research questions relevant to training ethically capable reinforcement learning agents.",
    "citation_count": 27,
    "summary": "This paper introduces a reinforcement learning formalism for training ethically-minded agents under moral uncertainty, addressing the lack of societal consensus on ethical theories by incorporating multiple plausible moral frameworks into the agent's decision-making process. This approach aims to mitigate extreme behavior resulting from adherence to a single, potentially flawed, ethical theory."
  },
  {
    "url": "https://arxiv.org/abs/2411.02478",
    "title": "Imagining and building wise machines: The centrality of AI metacognition",
    "published_date": "2024-11-04",
    "abstract": "Recent advances in artificial intelligence (AI) have produced systems capable of increasingly sophisticated performance on cognitive tasks. However, AI systems still struggle in critical ways: unpredictable and novel environments (robustness), lack of transparency in their reasoning (explainability), challenges in communication and commitment (cooperation), and risks due to potential harmful actions (safety). We argue that these shortcomings stem from one overarching failure: AI systems lack wisdom. Drawing from cognitive and social sciences, we define wisdom as the ability to navigate intractable problems - those that are ambiguous, radically uncertain, novel, chaotic, or computationally explosive - through effective task-level and metacognitive strategies. While AI research has focused on task-level strategies, metacognition - the ability to reflect on and regulate one's thought processes - is underdeveloped in AI systems. In humans, metacognitive strategies such as recognizing the limits of one's knowledge, considering diverse perspectives, and adapting to context are essential for wise decision-making. We propose that integrating metacognitive capabilities into AI systems is crucial for enhancing their robustness, explainability, cooperation, and safety. By focusing on developing wise AI, we suggest an alternative to aligning AI with specific human values - a task fraught with conceptual and practical difficulties. Instead, wise AI systems can thoughtfully navigate complex situations, account for diverse human values, and avoid harmful actions. We discuss potential approaches to building wise AI, including benchmarking metacognitive abilities and training AI systems to employ wise reasoning. Prioritizing metacognition in AI research will lead to systems that act not only intelligently but also wisely in complex, real-world situations.",
    "citation_count": 1,
    "summary": "The authors argue that current AI systems lack wisdom due to underdeveloped metacognitive abilities, proposing that integrating metacognition – the ability to reflect on and regulate one's thought processes – is crucial for improving AI robustness, explainability, cooperation, and safety. This approach offers an alternative to directly aligning AI with human values by enabling AI systems to navigate complex situations thoughtfully and avoid harmful actions."
  },
  {
    "url": "https://arxiv.org/abs/2411.15147",
    "title": "Delegating Responsibilities to Intelligent Autonomous Systems: Challenges and Benefits",
    "published_date": "2024-11-06",
    "abstract": "As AI systems increasingly operate with autonomy and adaptability, the traditional boundaries of moral responsibility in techno-social systems are being challenged. This paper explores the evolving discourse on the delegation of responsibilities to intelligent autonomous agents and the ethical implications of such practices. Synthesizing recent developments in AI ethics, including concepts of distributed responsibility and ethical AI by design, the paper proposes a functionalist perspective as a framework. This perspective views moral responsibility not as an individual trait but as a role within a socio-technical system, distributed among human and artificial agents. As an example of 'AI ethical by design,' we present Basti and Vitiello's implementation. They suggest that AI can act as artificial moral agents by learning ethical guidelines and using Deontic Higher-Order Logic to assess decisions ethically. Motivated by the possible speed and scale beyond human supervision and ethical implications, the paper argues for 'AI ethical by design', while acknowledging the distributed, shared, and dynamic nature of responsibility. This functionalist approach offers a practical framework for navigating the complexities of AI ethics in a rapidly evolving technological landscape.",
    "summary": "This paper examines the ethical challenges of delegating responsibility to increasingly autonomous AI systems, proposing a functionalist framework that distributes moral responsibility among human and artificial agents within a socio-technical system. It advocates for \"AI ethical by design,\" using Deontic Higher-Order Logic as a potential mechanism for ethical decision-making in AI."
  },
  {
    "url": "https://arxiv.org/abs/2408.01253",
    "title": "Metareasoning in uncertain environments: a meta-BAMDP framework",
    "published_date": "2024-08-02",
    "abstract": "In decision-making scenarios, \\textit{reasoning} can be viewed as an algorithm $P$ that makes a choice of an action $a^* \\in \\mathcal{A}$, aiming to optimize some outcome such as maximizing the value function of a Markov decision process (MDP). However, executing $P$ itself may bear some costs (time, energy, limited capacity, etc.) and needs to be considered alongside explicit utility obtained by making the choice in the underlying decision problem. Such costs need to be taken into account in order to accurately model human behavior, as well as optimizing AI planning, as all physical systems are bound to face resource constraints. Finding the right $P$ can itself be framed as an optimization problem over the space of reasoning processes $P$, generally referred to as \\textit{metareasoning}. Conventionally, human metareasoning models assume that the agent knows the transition and reward distributions of the underlying MDP. This paper generalizes such models by proposing a meta Bayes-Adaptive MDP (meta-BAMDP) framework to handle metareasoning in environments with unknown reward/transition distributions, which encompasses a far larger and more realistic set of planning problems that humans and AI systems face. As a first step, we apply the framework to two-armed Bernoulli bandit (TABB) tasks, which have often been used to study human decision making. Owing to the meta problem's complexity, our solutions are necessarily approximate, but nevertheless robust within a range of assumptions that are arguably realistic for human decision-making scenarios. These results offer a normative framework for understanding human exploration under cognitive constraints. This integration of Bayesian adaptive strategies with metareasoning enriches both the theoretical landscape of decision-making research and practical applications in designing AI systems that plan under uncertainty and resource constraints.",
    "summary": "This paper introduces a meta-Bayes-Adaptive Markov Decision Process (meta-BAMDP) framework for metareasoning in environments with unknown reward/transition distributions, addressing the cost of reasoning and generalizing existing models that assume perfect knowledge. The framework, applied to two-armed Bernoulli bandit tasks, provides a more realistic model of human and AI decision-making under resource constraints and uncertainty."
  },
  {
    "url": "https://arxiv.org/abs/2305.01424v1",
    "title": "Uncertain Machine Ethical Decisions Using Hypothetical Retrospection",
    "published_date": "2023-05-02",
    "abstract": "We propose the use of the hypothetical retrospection argumentation procedure, developed by Sven Ove Hansson to improve existing approaches to machine ethical reasoning by accounting for probability and uncertainty from a position of Philosophy that resonates with humans. Actions are represented with a branching set of potential outcomes, each with a state, utility, and either a numeric or poetic probability estimate. Actions are chosen based on comparisons between sets of arguments favouring actions from the perspective of their branches, even those branches that led to an undesirable outcome. This use of arguments allows a variety of philosophical theories for ethical reasoning to be used, potentially in flexible combination with each other. We implement the procedure, applying consequentialist and deontological ethical theories, independently and concurrently, to an autonomous library system use case. We introduce a preliminary framework that seems to meet the varied requirements of a machine ethics system: versatility under multiple theories and a resonance with humans that enables transparency and explainability.",
    "citation_count": 2,
    "summary": "This paper introduces a machine ethical decision-making framework using hypothetical retrospection, representing actions as branching outcomes with probabilistic utilities assessed via numeric or poetic estimations and comparing arguments for different actions from a human-resonant philosophical perspective. The framework accommodates multiple ethical theories (e.g., consequentialist and deontological) for versatile and explainable decision-making."
  },
  {
    "url": "https://arxiv.org/abs/2304.14577",
    "title": "Toward an Ethics of AI Belief",
    "published_date": "2023-04-28",
    "abstract": "In this paper we, an epistemologist and a machine learning scientist, argue that we need to pursue a novel area of philosophical research in AI – the ethics of belief for AI. Here we take the ethics of belief to refer to a field at the intersection of epistemology and ethics concerned with possible moral, practical, and other non-truth-related dimensions of belief. In this paper we will primarily be concerned with the normative question within the ethics of belief regarding what agents – both human and artificial – ought to believe, rather than with questions concerning whether beliefs meet certain evaluative standards such as being true, being justified, constituting knowledge, etc. We suggest four topics in extant work in the ethics of (human) belief that can be applied to an ethics of AI belief: doxastic wronging by AI (morally wronging someone in virtue of beliefs held about them); morally owed beliefs (beliefs that agents are morally obligated to hold); pragmatic and moral encroachment (cases where the practical or moral features of a belief is relevant to its epistemic status, and in our case specifically to whether an agent ought to hold the belief); and moral responsibility for AI beliefs. We also indicate two relatively nascent areas of philosophical research that haven't yet been generally recognized as ethics of AI belief research, but that do fall within this field of research in virtue of investigating various moral and practical dimensions of belief: the epistemic and ethical decolonization of AI; and epistemic injustice in AI.",
    "summary": "This paper proposes a new field of AI ethics: the ethics of AI belief, focusing on what beliefs AI agents *ought* to hold, rather than the truth or justification of those beliefs. It suggests applying existing ethical frameworks concerning human belief—such as doxastic wronging and moral encroachment—to AI, and highlights emerging areas like decolonization and epistemic injustice as relevant considerations."
  },
  {
    "url": "https://www.lesswrong.com/posts/nnGwHuJfCBxKDgsds/embedding-ethical-priors-into-ai-systems-a-bayesian-approach",
    "author": "Justausername",
    "title": "Embedding Ethical Priors into AI Systems: A Bayesian Approach",
    "published_date": "2023-08-03",
    "summary": "This paper proposes a novel framework for building ethically aligned AI systems by incorporating ethical principles as Bayesian priors into AI learning processes. This approach, inspired by human moral intuitions, aims to guide AI decision-making through initial ethical biases, rather than rigid rules."
  },
  {
    "url": "https://www.lesswrong.com/posts/fJqP9WcnHXBRBeiBg/meta-questions-about-metaphilosophy",
    "author": "Wei Dai",
    "title": "Meta Questions about Metaphilosophy",
    "published_date": "2023-09-01",
    "summary": "The author details their intellectual journey from science to metaphilosophy, noting a dwindling number of peers at each increasingly meta level of inquiry. This leads them to question the lack of focus on metaphilosophy in AI safety, despite its crucial role in ensuring beneficial outcomes for the human-AI transition."
  },
  {
    "url": "https://arxiv.org/pdf/2204.05151v1.pdf",
    "title": "Metaethical Perspectives on 'Benchmarking' AI Ethics",
    "published_date": "2022-04-11",
    "abstract": "Benchmarks are seen as the cornerstone for measuring technical progress in Artificial Intelligence (AI) research and have been developed for a variety of tasks ranging from question answering to facial recognition. An increasingly prominent research area in AI is ethics, which currently has no set of benchmarks nor commonly accepted way for measuring the 'ethicality' of an AI system. In this paper, drawing upon research in moral philosophy and metaethics, we argue that it is impossible to develop such a benchmark. As such, alternative mechanisms are necessary for evaluating whether an AI system is 'ethical'. This is especially pressing in light of the prevalence of applied, industrial AI research. We argue that it makes more sense to talk about 'values' (and 'value alignment') rather than 'ethics' when considering the possible actions of present and future AI systems. We further highlight that, because values are unambiguously relative, focusing on values forces us to consider explicitly what the values are and whose values they are. Shifting the emphasis from ethics to values therefore gives rise to several new ways of understanding how researchers might advance research programmes for robustly safe or beneficial AI. We conclude by highlighting a number of possible ways forward for the field as a whole, and we advocate for different approaches towards more value-aligned AI research.",
    "citation_count": 5,
    "summary": "The paper argues that creating a benchmark for measuring the \"ethicality\" of AI systems is impossible due to the inherent subjectivity of ethics, advocating instead for a focus on values and value alignment, which necessitates explicit consideration of whose values are prioritized. This shift emphasizes the relative nature of values and proposes alternative approaches to developing safe and beneficial AI."
  },
  {
    "url": "https://arxiv.org/pdf/2204.07612v1.pdf",
    "title": "Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in Artificial Intelligence",
    "published_date": "2022-04-15",
    "abstract": "In this meta-ethnography,we explore three different angles of Ethical AI design and implementation in a top-down/bottom-up framework, including the philosophical ethical viewpoint, the technical perspective, and framing through a political lens. We will discuss the values and drawbacks of individual and hybrid approaches within this framework. Examples of approaches include ethics either being determined by corporations and governments (coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technical-ities of how AI is developed within a moral construct, in consideration of its developers and users, with expected and unexpected consequences and long-term impact. This investigation includes real-world case studies, philosophical debate, and theoretical future thought experimentation based on historical fact, current world circumstances, and possible ensuing realities.",
    "summary": "This meta-ethnography analyzes top-down, bottom-up, and hybrid approaches to integrating ethical considerations into AI development, examining philosophical, technical, and political perspectives and their respective strengths and weaknesses. The analysis incorporates case studies and theoretical explorations to assess the impacts of different ethical frameworks on AI's development and deployment."
  }
]