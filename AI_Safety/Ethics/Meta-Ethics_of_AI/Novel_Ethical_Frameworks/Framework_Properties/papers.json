[
  {
    "url": "https://arxiv.org/abs/2410.21882",
    "title": "Building Altruistic and Moral AI Agent with Brain-inspired Affective Empathy Mechanisms",
    "published_date": "2024-10-29",
    "abstract": "As AI closely interacts with human society, it is crucial to ensure that its decision-making is safe, altruistic, and aligned with human ethical and moral values. However, existing research on embedding ethical and moral considerations into AI remains insufficient, and previous external constraints based on principles and rules are inadequate to provide AI with long-term stability and generalization capabilities. In contrast, the intrinsic altruistic motivation based on empathy is more willing, spontaneous, and robust. Therefore, this paper is dedicated to autonomously driving intelligent agents to acquire morally behaviors through human-like affective empathy mechanisms. We draw inspiration from the neural mechanism of human brain's moral intuitive decision-making, and simulate the mirror neuron system to construct a brain-inspired affective empathy-driven altruistic decision-making model. Here, empathy directly impacts dopamine release to form intrinsic altruistic motivation. Based on the principle of moral utilitarianism, we design the moral reward function that integrates intrinsic empathy and extrinsic self-task goals. A comprehensive experimental scenario incorporating empathetic processes, personal objectives, and altruistic goals is developed. The proposed model enables the agent to make consistent moral decisions (prioritizing altruism) by balancing self-interest with the well-being of others. We further introduce inhibitory neurons to regulate different levels of empathy and verify the positive correlation between empathy levels and altruistic preferences, yielding conclusions consistent with findings from psychological behavioral experiments. This work provides a feasible solution for the development of ethical AI by leveraging the intrinsic human-like empathy mechanisms, and contributes to the harmonious coexistence between humans and AI.",
    "summary": "This paper proposes a brain-inspired model for creating altruistic AI agents, using simulated mirror neurons and dopamine reward to foster empathetic decision-making based on utilitarian principles, enabling the agent to balance self-interest with the well-being of others. The model's performance is validated through experiments showing a correlation between empathy levels and altruistic behavior."
  },
  {
    "url": "https://arxiv.org/abs/2410.19198",
    "title": "MAP: Multi-Human-Value Alignment Palette",
    "published_date": "2024-10-24",
    "abstract": "Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.",
    "summary": "The Multi-Human-Value Alignment Palette (MAP) framework addresses the challenge of aligning generative AI with multiple, potentially conflicting human values by formulating the problem as a constrained optimization task. MAP uses a primal-dual approach to determine achievable value alignment targets and efficiently achieve them, demonstrating strong empirical performance across various tasks."
  },
  {
    "url": "https://arxiv.org/abs/2411.15147",
    "title": "Delegating Responsibilities to Intelligent Autonomous Systems: Challenges and Benefits",
    "published_date": "2024-11-06",
    "abstract": "As AI systems increasingly operate with autonomy and adaptability, the traditional boundaries of moral responsibility in techno-social systems are being challenged. This paper explores the evolving discourse on the delegation of responsibilities to intelligent autonomous agents and the ethical implications of such practices. Synthesizing recent developments in AI ethics, including concepts of distributed responsibility and ethical AI by design, the paper proposes a functionalist perspective as a framework. This perspective views moral responsibility not as an individual trait but as a role within a socio-technical system, distributed among human and artificial agents. As an example of 'AI ethical by design,' we present Basti and Vitiello's implementation. They suggest that AI can act as artificial moral agents by learning ethical guidelines and using Deontic Higher-Order Logic to assess decisions ethically. Motivated by the possible speed and scale beyond human supervision and ethical implications, the paper argues for 'AI ethical by design', while acknowledging the distributed, shared, and dynamic nature of responsibility. This functionalist approach offers a practical framework for navigating the complexities of AI ethics in a rapidly evolving technological landscape.",
    "summary": "This paper examines the ethical challenges of delegating responsibilities to increasingly autonomous AI systems, proposing a functionalist framework that distributes moral responsibility among human and artificial agents within a socio-technical system. It advocates for \"AI ethical by design,\" emphasizing proactive ethical considerations in AI development to address the speed and scale of AI decision-making beyond human oversight."
  },
  {
    "url": "https://arxiv.org/abs/2408.12250",
    "title": "Can Artificial Intelligence Embody Moral Values?",
    "published_date": "2024-08-22",
    "abstract": "The neutrality thesis holds that technology cannot be laden with values. This long-standing view has faced critiques, but much of the argumentation against neutrality has focused on traditional, non-smart technologies like bridges and razors. In contrast, AI is a smart technology increasingly used in high-stakes domains like healthcare, finance, and policing, where its decisions can cause moral harm. In this paper, we argue that artificial intelligence, particularly artificial agents that autonomously make decisions to pursue their goals, challenge the neutrality thesis. Our central claim is that the computational models underlying artificial agents can integrate representations of moral values such as fairness, honesty and avoiding harm. We provide a conceptual framework discussing the neutrality thesis, values, and AI. Moreover, we examine two approaches to designing computational models of morality, artificial conscience and ethical prompting, and present empirical evidence from text-based game environments that artificial agents with such models exhibit more ethical behavior compared to agents without these models. The findings support that AI can embody moral values, which contradicts the claim that all technologies are necessarily value-neutral.",
    "summary": "This paper challenges the neutrality thesis, arguing that artificial agents, through computational models incorporating moral values like fairness and avoiding harm, can embody and exhibit ethical behavior, contradicting the assertion that technology is inherently value-neutral. Empirical evidence from text-based games supports this claim."
  },
  {
    "url": "https://www.lesswrong.com/posts/324pQjqoHEHeF2vPs/ai-clarity-an-initial-research-agenda",
    "author": "Justin Bullock, Corin Katzke, Zershaaneh Qureshi, David_Kristoffersson",
    "title": "AI Clarity: An Initial Research Agenda",
    "published_date": "2024-05-03",
    "summary": "The AI Clarity research program uses scenario planning to analyze potential trajectories of transformative AI (TAI), focusing on scenarios where TAI emerges within the next decade. The program aims to identify existential hazards, evaluate strategies for AI safety and governance, and recommend mitigation strategies across plausible scenarios."
  },
  {
    "url": "https://arxiv.org/pdf/2302.08759.pdf",
    "title": "Value Engineering for Autonomous Agents",
    "published_date": "2023-02-17",
    "abstract": "Machine Ethics (ME) is concerned with the design of Artificial Moral Agents (AMAs), i.e. autonomous agents capable of reasoning and behaving according to moral values. Previous approaches have treated values as labels associated with some actions or states of the world, rather than as integral components of agent reasoning. It is also common to disregard that a value-guided agent operates alongside other value-guided agents in an environment governed by norms, thus omitting the social dimension of AMAs. In this blue sky paper, we propose a new AMA paradigm grounded in moral and social psychology, where values are instilled into agents as context-dependent goals. These goals intricately connect values at individual levels to norms at a collective level by evaluating the outcomes most incentivized by the norms in place. We argue that this type of normative reasoning, where agents are endowed with an understanding of norms' moral implications, leads to value-awareness in autonomous agents. Additionally, this capability paves the way for agents to align the norms enforced in their societies with respect to the human values instilled in them, by complementing the value-based reasoning on norms with agreement mechanisms to help agents collectively agree on the best set of norms that suit their human values. Overall, our agent model goes beyond the treatment of values as inert labels by connecting them to normative reasoning and to the social functionalities needed to integrate value-aware agents into our modern hybrid human-computer societies.",
    "citation_count": 3,
    "summary": "This paper proposes a novel framework for Artificial Moral Agents (AMAs) that integrates values as context-dependent goals, connecting individual values to societal norms through normative reasoning and agreement mechanisms to achieve value-alignment in human-computer societies. This approach moves beyond treating values as simple labels, fostering value-awareness and enabling AMAs to adapt norms based on instilled human values."
  },
  {
    "url": "https://www.lesswrong.com/posts/nnGwHuJfCBxKDgsds/embedding-ethical-priors-into-ai-systems-a-bayesian-approach",
    "author": "Justausername",
    "title": "Embedding Ethical Priors into AI Systems: A BayesianÂ Approach",
    "published_date": "2023-08-03",
    "summary": "This paper proposes a novel framework for building ethical AI by incorporating ethical principles as Bayesian priors into AI learning processes. This approach, inspired by human moral intuition, aims to guide AI decision-making towards ethically sound judgments."
  },
  {
    "url": "https://www.alignmentforum.org/tag/ai",
    "author": "Evan Hubinger",
    "title": "AI - AI Alignment Forum",
    "published_date": "2023-02-06",
    "summary": "Artificial intelligence alignment focuses on ensuring powerful AI systems act according to human values, preventing unintended consequences like existential threats. This involves diverse approaches, from narrow goals (e.g., curing diseases) to broader aims (e.g., creating a beneficial future), all tackling the challenge of AI's potential for misalignment."
  }
]