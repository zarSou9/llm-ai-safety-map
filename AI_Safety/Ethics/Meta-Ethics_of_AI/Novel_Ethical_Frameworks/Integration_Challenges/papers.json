[
  {
    "url": "https://arxiv.org/abs/2501.09021",
    "title": "Navigating Ethical Challenges in Generative AI-Enhanced Research: The ETHICAL Framework for Responsible Generative AI Use",
    "published_date": "2024-12-11",
    "abstract": "The rapid adoption of generative artificial intelligence (GenAI) in research presents both opportunities and ethical challenges that should be carefully navigated. Although GenAI tools can enhance research efficiency through automation of tasks such as literature review and data analysis, their use raises concerns about aspects such as data accuracy, privacy, bias, and research integrity. This paper develops the ETHICAL framework, which is a practical guide for responsible GenAI use in research. Employing a constructivist case study examining multiple GenAI tools in real research contexts, the framework consists of seven key principles: Examine policies and guidelines, Think about social impacts, Harness understanding of the technology, Indicate use, Critically engage with outputs, Access secure versions, and Look at user agreements. Applying these principles will enable researchers to uphold research integrity while leveraging GenAI benefits. The framework addresses a critical gap between awareness of ethical issues and practical action steps, providing researchers with concrete guidance for ethical GenAI integration. This work has implications for research practice, institutional policy development, and the broader academic community while adapting to an AI-enhanced research landscape. The ETHICAL framework can serve as a foundation for developing AI literacy in academic settings and promoting responsible innovation in research methodologies.",
    "summary": "The ETHICAL framework provides researchers with seven practical principles for responsibly using generative AI in research, addressing concerns about accuracy, bias, privacy, and integrity while maximizing the technology's benefits. This framework bridges the gap between ethical awareness and actionable steps for ethical GenAI integration in research."
  },
  {
    "url": "https://arxiv.org/abs/2405.07076",
    "title": "Integrating Emotional and Linguistic Models for Ethical Compliance in Large Language Models",
    "published_date": "2024-05-11",
    "abstract": "This research develops advanced methodologies for Large Language Models (LLMs) to better manage linguistic behaviors related to emotions and ethics. We introduce DIKE, an adversarial framework that enhances the LLMs' ability to internalize and reflect global human values, adapting to varied cultural contexts to promote transparency and trust among users. The methodology involves detailed modeling of emotions, classification of linguistic behaviors, and implementation of ethical guardrails. Our innovative approaches include mapping emotions and behaviors using self-supervised learning techniques, refining these guardrails through adversarial reviews, and systematically adjusting outputs to ensure ethical alignment. This framework establishes a robust foundation for AI systems to operate with ethical integrity and cultural sensitivity, paving the way for more responsible and context-aware AI interactions.",
    "citation_count": 3,
    "summary": "DIKE, a novel adversarial framework, improves Large Language Model ethical compliance by integrating emotional and linguistic models, enabling context-aware, culturally sensitive outputs aligned with global human values. This is achieved through self-supervised learning, adversarial review, and ethical guardrail adjustments."
  },
  {
    "url": "https://arxiv.org/abs/2404.15058?utm_campaign=%F0%9F%9F%A1%20Preparing%20for%20AI%20Newsletter&utm_medium=email&_hsenc=p2ANqtz-_gUw_9cQjdn6wt8vtDekBij1SZaoLHhBVU8fg6tJUrmVKQA1CEXIjf6aM1AAvZB_WnKTEslWasnMIanJ8b_s7HXFIu9w&_hsmi=304334294&utm_content=304334294&utm_source=hs_email",
    "title": "A Mechanism-Based Approach to Mitigating Harms from Persuasive Generative AI",
    "published_date": "2024-04-23",
    "abstract": "Recent generative AI systems have demonstrated more advanced persuasive capabilities and are increasingly permeating areas of life where they can influence decision-making. Generative AI presents a new risk profile of persuasion due the opportunity for reciprocal exchange and prolonged interactions. This has led to growing concerns about harms from AI persuasion and how they can be mitigated, highlighting the need for a systematic study of AI persuasion. The current definitions of AI persuasion are unclear and related harms are insufficiently studied. Existing harm mitigation approaches prioritise harms from the outcome of persuasion over harms from the process of persuasion. In this paper, we lay the groundwork for the systematic study of AI persuasion. We first put forward definitions of persuasive generative AI. We distinguish between rationally persuasive generative AI, which relies on providing relevant facts, sound reasoning, or other forms of trustworthy evidence, and manipulative generative AI, which relies on taking advantage of cognitive biases and heuristics or misrepresenting information. We also put forward a map of harms from AI persuasion, including definitions and examples of economic, physical, environmental, psychological, sociocultural, political, privacy, and autonomy harm. We then introduce a map of mechanisms that contribute to harmful persuasion. Lastly, we provide an overview of approaches that can be used to mitigate against process harms of persuasion, including prompt engineering for manipulation classification and red teaming. Future work will operationalise these mitigations and study the interaction between different types of mechanisms of persuasion.",
    "citation_count": 11,
    "summary": "This paper proposes a framework for understanding and mitigating harms from persuasive generative AI by defining rational and manipulative AI persuasion, mapping potential harms across various domains, and outlining mechanisms contributing to harmful persuasion and potential mitigation strategies like prompt engineering and red teaming. The focus is on mitigating harms from the *process* of persuasion, not just its outcome."
  },
  {
    "url": "https://arxiv.org/abs/2408.00965",
    "title": "Integrating ESG and AI: A Comprehensive Responsible AI Assessment Framework",
    "published_date": "2024-08-02",
    "abstract": "Artificial Intelligence (AI) is a widely developed and adopted technology across entire industry sectors. Integrating environmental, social, and governance (ESG) considerations with AI investments is crucial for ensuring ethical and sustainable technological advancement. Particularly from an investor perspective, this integration not only mitigates risks but also enhances long-term value creation by aligning AI initiatives with broader societal goals. Yet, this area has been less explored in both academia and industry. To bridge the gap, we introduce a novel ESG-AI framework, which is developed based on insights from engagements with 28 companies and comprises three key components. The framework provides a structured approach to this integration, developed in collaboration with industry practitioners. The ESG-AI framework provides an overview of the environmental and social impacts of AI applications, helping users such as investors assess the materiality of AI use. Moreover, it enables investors to evaluate a company's commitment to responsible AI through structured engagements and thorough assessment of specific risk areas. We have publicly released the framework and toolkit in April 2024, which has received significant attention and positive feedback from the investment community. This paper details each component of the framework, demonstrating its applicability in real-world contexts and its potential to guide ethical AI investments.",
    "citation_count": 2,
    "summary": "This paper introduces a novel ESG-AI framework, developed through industry collaboration, to help investors assess the environmental and social impacts of AI applications and evaluate companies' commitment to responsible AI development. The framework, publicly released with a toolkit, offers a structured approach to integrating ESG considerations into AI investments for mitigating risks and enhancing long-term value."
  },
  {
    "url": "https://arxiv.org/abs/2312.01818",
    "title": "Hybrid Approaches for Moral Value Alignment in AI Agents: a Manifesto",
    "published_date": "2023-12-04",
    "abstract": "Increasing interest in ensuring the safety of next-generation Artificial Intelligence (AI) systems calls for novel approaches to embedding morality into autonomous agents. This goal differs qualitatively from traditional task-specific AI methodologies. In this paper, we provide a systematization of existing approaches to the problem of introducing morality in machines - modelled as a continuum. Our analysis suggests that popular techniques lie at the extremes of this continuum - either being fully hard-coded into top-down, explicit rules, or entirely learned in a bottom-up, implicit fashion with no direct statement of any moral principle (this includes learning from human feedback, as applied to the training and finetuning of large language models, or LLMs). Given the relative strengths and weaknesses of each type of methodology, we argue that more hybrid solutions are needed to create adaptable and robust, yet controllable and interpretable agentic systems. To that end, this paper discusses both the ethical foundations (including deontology, consequentialism and virtue ethics) and implementations of morally aligned AI systems. We present a series of case studies that rely on intrinsic rewards, moral constraints or textual instructions, applied to either pure-Reinforcement Learning or LLM-based agents. By analysing these diverse implementations under one framework, we compare their relative strengths and shortcomings in developing morally aligned AI systems. We then discuss strategies for evaluating the effectiveness of moral learning agents. Finally, we present open research questions and implications for the future of AI safety and ethics which are emerging from this hybrid framework.",
    "citation_count": 3,
    "summary": "This paper advocates for hybrid approaches to imbue AI agents with moral values, arguing that current methods—either purely rule-based or solely learned—are insufficient. It proposes a framework encompassing diverse techniques, including reinforcement learning and large language models, to create more robust, interpretable, and controllable morally aligned AI systems."
  },
  {
    "title": "Six Human-Centered Artificial Intelligence Grand Challenges",
    "abstract": "Abstract Widespread adoption of artificial intelligence (AI) technologies is substantially affecting the human condition in ways that are not yet well understood. Negative unintended consequences abound including the perpetuation and exacerbation of societal inequalities and divisions via algorithmic decision making. We present six grand challenges for the scientific community to create AI technologies that are human-centered, that is, ethical, fair, and enhance the human condition. These grand challenges are the result of an international collaboration across academia, industry and government and represent the consensus views of a group of 26 experts in the field of human-centered artificial intelligence (HCAI). In essence, these challenges advocate for a human-centered approach to AI that (1) is centered in human well-being, (2) is designed responsibly, (3) respects privacy, (4) follows human-centered design principles, (5) is subject to appropriate governance and oversight, and (6) interacts with individuals while respecting human's cognitive capacities. We hope that these challenges and their associated research directions serve as a call for action to conduct research and development in AI that serves as a force multiplier towards more fair, equitable and sustainable societies.",
    "published_date": "2023-01-02",
    "citation_count": 152,
    "url": "https://www.tandfonline.com/doi/full/10.1080/10447318.2022.2153320",
    "summary": "This paper identifies six grand challenges for creating human-centered AI, focusing on ethical, fair, and well-being-enhancing AI technologies to mitigate negative societal impacts. These challenges emphasize responsible design, privacy, human-centered design principles, governance, and respect for human cognitive capacities."
  },
  {
    "url": "https://www.lesswrong.com/posts/nnGwHuJfCBxKDgsds/embedding-ethical-priors-into-ai-systems-a-bayesian-approach",
    "author": "Justausername",
    "title": "Embedding Ethical Priors into AI Systems: A Bayesian Approach",
    "published_date": "2023-08-03",
    "summary": "This paper proposes a novel framework for building ethical AI by incorporating ethical principles as Bayesian priors into AI learning processes. This approach, inspired by human moral intuition, aims to guide AI decision-making towards ethically sound judgments."
  },
  {
    "url": "https://www.lesswrong.com/posts/W7sEv69cQzW8D8SMr/the-human-ai-reflective-equilibrium",
    "author": "Allison Duettmann",
    "title": "The Human-AI Reflective Equilibrium",
    "published_date": "2023-01-24",
    "summary": "This article argues that aligning artificial general intelligence (AGI) with human values requires addressing both the technical challenge of communication and the philosophical challenge of defining those values. It proposes a pluralist approach, advocating for a diverse set of AGIs trained on individual moral frameworks, fostering cooperation and reflective equilibrium among them and with humans."
  }
]