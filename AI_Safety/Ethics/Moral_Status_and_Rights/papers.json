[
  {
    "title": "Novel Beings: Moral Status and Regulation",
    "abstract": "A world of 'posthumans,' synthetic human embryos, perhaps even thinking artificial intelligences (AI) becomes increasingly inevitable. A vast range of technologies, including genomics, synthetic biology, advanced pharmaceuticals, neurotechnologies, and breakthroughs in computer science and artificial intelligence research promise to create new forms of sentient, even sapient intelligent life in the not-too-distant future; and perhaps change our understanding of life itself. These types of technology promise significant effects on our way of life, of working, and of interacting with others— perhaps even as significant as in the science fiction worlds they were once relegated to. This special section explores these effects, and what they maymean for us in terms of our understandings of ourselves and the assumptions on which we base so many facets of our lives and society. We are already capable of the fundamentals of creation, products of artifice which may in future possess moral value or approach forms of sentient or even sapient 'life.' These developments promise to draw into question the nature of what it is to be human and what it is to be a person before the law. These are concepts upon which many significant institutions are founded. The advent of these new intelligent life forms – 'novel beings'—may require us to re-evaluate Homo sapiens' position as the sole bearer of human rights; and will pose disruptive global challenges for society and the law regarding their moral status, protections, and freedoms, as well as their obligations, and ours towards them. Should a synthetic animal fall under 'natural' animal protection provisions? Should AI who fulfil the criteria for moral value to greater degrees be afforded greater consideration? Reasoning creatures resulting from either technological route may equally deserve their own protections and freedoms; they may be moral agents, or perhaps they should share in natural personhood—i.e. to be 'legally human.'1 It is vitally important to establish whether existing lawwill remain sufficient tomanage the potential development of new types of intelligent being from these technologies, and if not, how it ought to be adapted tomeet the requirements of the future. Critics of our stance suggest that these are not problems for today; but are instead so distant that we ought to be concerned with more immediate social issues. Certainly, there is some merit to this point. However, it is a continuous cycle. There is always a current need that deserves to be addressed, and so we leave the future problems until they themselves are current—which may be too late. The advent of the novel being is an event of such magnitude that it warrants breaking this cycle. The law, particularly in a common law system, has a predilection for reactivity rather than being proactive. Doubtless, many legal experts will criticize so sweeping a statement, and there is increasing legal scholarship supporting the ex ante view.2 Nonetheless, law as an entity necessarily must act in retrospect in an attempt to prevent given situations, actions, or circumstances from recurring. This is a sensible and logical approach: only through experience can we know for certain what weaknesses the system may have, and thus seek to patch them with new rulings or even legislation. It would not do to legislate blindly in fear of the unforeseen; that waylays totalitarianism and dystopia. For this reason, the law often trails behind the world—eventually papering the cracks, but leaving them to grow in the meantime. Some of these cracks are unfortunate, even unsightly, but to push ametaphor, not structurally",
    "published_date": "2021-06-10",
    "citation_count": 1,
    "url": "https://www.cambridge.org/core/product/identifier/S0963180120000961/type/journal_article",
    "summary": "The paper argues that advancements in biotechnology and AI will soon create \"novel beings\" with potential moral status, necessitating a proactive, rather than reactive, legal and ethical framework to address their rights and responsibilities. Failure to do so risks significant societal disruption and injustice."
  },
  {
    "url": "https://arxiv.org/abs/2411.15147",
    "title": "Delegating Responsibilities to Intelligent Autonomous Systems: Challenges and Benefits",
    "published_date": "2024-11-06",
    "abstract": "As AI systems increasingly operate with autonomy and adaptability, the traditional boundaries of moral responsibility in techno-social systems are being challenged. This paper explores the evolving discourse on the delegation of responsibilities to intelligent autonomous agents and the ethical implications of such practices. Synthesizing recent developments in AI ethics, including concepts of distributed responsibility and ethical AI by design, the paper proposes a functionalist perspective as a framework. This perspective views moral responsibility not as an individual trait but as a role within a socio-technical system, distributed among human and artificial agents. As an example of 'AI ethical by design,' we present Basti and Vitiello's implementation. They suggest that AI can act as artificial moral agents by learning ethical guidelines and using Deontic Higher-Order Logic to assess decisions ethically. Motivated by the possible speed and scale beyond human supervision and ethical implications, the paper argues for 'AI ethical by design', while acknowledging the distributed, shared, and dynamic nature of responsibility. This functionalist approach offers a practical framework for navigating the complexities of AI ethics in a rapidly evolving technological landscape.",
    "summary": "This paper examines the ethical challenges of delegating responsibilities to increasingly autonomous AI systems, proposing a functionalist framework that distributes moral responsibility among humans and AI agents within a socio-technical system. It advocates for \"AI ethical by design,\" using Deontic Higher-Order Logic as a potential mechanism for ethical decision-making in AI."
  },
  {
    "url": "https://arxiv.org/abs/2412.17114",
    "title": "Decentralized Governance of Autonomous AI Agents",
    "published_date": "2024-12-22",
    "abstract": "Autonomous AI agents present transformative opportunities and significant governance challenges. Existing frameworks, such as the EU AI Act and the NIST AI Risk Management Framework, fall short of addressing the complexities of these agents, which are capable of independent decision-making, learning, and adaptation. To bridge these gaps, we propose the ETHOS (Ethical Technology and Holistic Oversight System) framework, a decentralized governance (DeGov) model leveraging Web3 technologies, including blockchain, smart contracts, and decentralized autonomous organizations (DAOs). ETHOS establishes a global registry for AI agents, enabling dynamic risk classification, proportional oversight, and automated compliance monitoring through tools like soulbound tokens and zero-knowledge proofs. Furthermore, the framework incorporates decentralized justice systems for transparent dispute resolution and introduces AI specific legal entities to manage limited liability, supported by mandatory insurance to ensure financial accountability and incentivize ethical design. By integrating philosophical principles of rationality, ethical grounding, and goal alignment, ETHOS aims to create a robust research agenda for promoting trust, transparency, and participatory governance. This innovative framework offers a scalable and inclusive strategy for regulating AI agents, balancing innovation with ethical responsibility to meet the demands of an AI-driven future.",
    "summary": "The ETHOS framework proposes a decentralized governance model for autonomous AI agents using Web3 technologies, aiming to address the limitations of existing regulatory frameworks through a global registry, dynamic risk classification, and decentralized justice systems. This approach leverages blockchain, smart contracts, and DAOs to promote transparency, accountability, and ethical development of AI."
  },
  {
    "url": "https://arxiv.org/abs/2408.12250",
    "title": "Can Artificial Intelligence Embody Moral Values?",
    "published_date": "2024-08-22",
    "abstract": "The neutrality thesis holds that technology cannot be laden with values. This long-standing view has faced critiques, but much of the argumentation against neutrality has focused on traditional, non-smart technologies like bridges and razors. In contrast, AI is a smart technology increasingly used in high-stakes domains like healthcare, finance, and policing, where its decisions can cause moral harm. In this paper, we argue that artificial intelligence, particularly artificial agents that autonomously make decisions to pursue their goals, challenge the neutrality thesis. Our central claim is that the computational models underlying artificial agents can integrate representations of moral values such as fairness, honesty and avoiding harm. We provide a conceptual framework discussing the neutrality thesis, values, and AI. Moreover, we examine two approaches to designing computational models of morality, artificial conscience and ethical prompting, and present empirical evidence from text-based game environments that artificial agents with such models exhibit more ethical behavior compared to agents without these models. The findings support that AI can embody moral values, which contradicts the claim that all technologies are necessarily value-neutral.",
    "summary": "This paper challenges the neutrality thesis, arguing that artificial agents, through computational models incorporating moral values like fairness and avoiding harm, can embody ethical behavior, contradicting the assertion that technology is inherently value-neutral. Empirical evidence from text-based games supports this claim."
  },
  {
    "url": "https://www.lesswrong.com/tag/machine-ethics",
    "title": "Machine Ethics - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Machine ethics seeks to create Artificial Moral Agents (AMAs) capable of morally sound actions, a challenge highlighted by limitations in current narrow AI applications and the urgent need to develop ethical guidelines for future artificial general intelligence. Despite early conceptualizations like Asimov's Three Laws of Robotics, creating truly ethical machines remains a complex and ongoing pursuit."
  },
  {
    "url": "https://www.lesswrong.com/tag/artificial-general-intelligence-agi",
    "title": "Artificial General Intelligence (AGI) - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Artificial General Intelligence (AGI) refers to machines capable of intelligent behavior across diverse domains, unlike narrow AI which excels only in specific tasks. While AGI's creation remains uncertain, experts predict its emergence sometime between 2050 and 2150, raising concerns about potential risks and the alignment of its values with humanity's."
  },
  {
    "url": "https://arxiv.org/pdf/2302.08759.pdf",
    "title": "Value Engineering for Autonomous Agents",
    "published_date": "2023-02-17",
    "abstract": "Machine Ethics (ME) is concerned with the design of Artificial Moral Agents (AMAs), i.e. autonomous agents capable of reasoning and behaving according to moral values. Previous approaches have treated values as labels associated with some actions or states of the world, rather than as integral components of agent reasoning. It is also common to disregard that a value-guided agent operates alongside other value-guided agents in an environment governed by norms, thus omitting the social dimension of AMAs. In this blue sky paper, we propose a new AMA paradigm grounded in moral and social psychology, where values are instilled into agents as context-dependent goals. These goals intricately connect values at individual levels to norms at a collective level by evaluating the outcomes most incentivized by the norms in place. We argue that this type of normative reasoning, where agents are endowed with an understanding of norms' moral implications, leads to value-awareness in autonomous agents. Additionally, this capability paves the way for agents to align the norms enforced in their societies with respect to the human values instilled in them, by complementing the value-based reasoning on norms with agreement mechanisms to help agents collectively agree on the best set of norms that suit their human values. Overall, our agent model goes beyond the treatment of values as inert labels by connecting them to normative reasoning and to the social functionalities needed to integrate value-aware agents into our modern hybrid human-computer societies.",
    "citation_count": 3,
    "summary": "This paper proposes a novel paradigm for Artificial Moral Agents (AMAs) that integrates values as context-dependent goals, connecting individual values to collective norms through normative reasoning and agreement mechanisms, thereby fostering value-awareness and social integration of AMAs. This approach moves beyond treating values as simple labels, embedding them within a socially-aware agent architecture."
  },
  {
    "url": "https://arxiv.org/abs/2302.08157",
    "title": "Human-Centered Responsible Artificial Intelligence: Current & Future Trends",
    "published_date": "2023-02-16",
    "abstract": "In recent years, the CHI community has seen significant growth in research on Human-Centered Responsible Artificial Intelligence. While different research communities may use different terminology to discuss similar topics, all of this work is ultimately aimed at developing AI that benefits humanity while being grounded in human rights and ethics, and reducing the potential harms of AI. In this special interest group, we aim to bring together researchers from academia and industry interested in these topics to map current and future research trends to advance this important area of research by fostering collaboration and sharing ideas.",
    "citation_count": 29,
    "summary": "This paper explores the growing field of Human-Centered Responsible Artificial Intelligence within the CHI community, focusing on aligning AI development with human benefits, ethical considerations, and the mitigation of potential harms. It aims to foster collaboration and identify future research trends in this area."
  },
  {
    "url": "https://www.lesswrong.com/tag/artificial-general-intelligence",
    "title": "Artificial General Intelligence - LessWrong",
    "published_date": "2023-02-06",
    "summary": "Artificial General Intelligence (AGI) refers to machines capable of intelligent behavior across diverse domains, unlike narrow AI which excels only in specific tasks. While the creation of AGI remains uncertain, experts predict its emergence sometime in the coming decades, though significant debate surrounds its potential benefits and risks."
  },
  {
    "url": "https://arxiv.org/pdf/2208.14788.pdf",
    "title": "Negative Human Rights as a Basis for Long-term AI Safety and Regulation",
    "published_date": "2022-08-31",
    "abstract": "If autonomous AI systems are to be reliably safe in novel situations, they will need to incorporate general principles guiding them to recognize and avoid harmful behaviours. Such principles may need to be supported by a binding system of regulation, which would need the underlying principles to be widely accepted. They should also be specific enough for technical implementation. Drawing inspiration from law, this article explains how negative human rights could fulfil the role of such principles and serve as a foundation both for an international regulatory system and for building technical safety constraints for future AI systems.\nThis article appears in the AI & Society track.",
    "citation_count": 9,
    "summary": "This paper proposes using negative human rights—rights that protect against harm—as foundational principles for both regulating and building safety constraints into autonomous AI systems, arguing that this approach offers a robust, widely accepted, and technically implementable framework for long-term AI safety."
  },
  {
    "url": "https://arxiv.org/abs/2110.14419v2",
    "title": "Toward a Theory of Justice for Artificial Intelligence",
    "published_date": "2021-10-27",
    "abstract": "Abstract This essay explores the relationship between artificial intelligence and principles of distributive justice. Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of sociotechnical systems, and that the operation of these systems is increasingly shaped and influenced by AI. Consequently, egalitarian norms of justice apply to the technology when it is deployed in these contexts. These norms entail that the relevant AI systems must meet a certain standard of public justification, support citizens' rights, and promote substantively fair outcomes, something that requires particular attention to the impact they have on the worst-off members of society.",
    "citation_count": 40,
    "summary": "This essay argues that principles of distributive justice, particularly Rawls's theory, should govern the design and implementation of artificial intelligence systems, emphasizing fair outcomes and protection of the most vulnerable. AI's influence on societal structures necessitates its evaluation through an egalitarian lens, requiring public justification and the upholding of citizens' rights."
  },
  {
    "url": "https://arxiv.org/pdf/2109.07906v1.pdf",
    "title": "Ethics of AI: A Systematic Literature Review of Principles and Challenges",
    "published_date": "2021-09-12",
    "abstract": "Ethics in AI becomes a global topic of interest for both policymakers and academic researchers. In the last few years, various research organizations, lawyers, think tankers, and regulatory bodies get involved in developing AI ethics guidelines and principles. However, there is still debate about the implications of these principles. We conducted a systematic literature review (SLR) study to investigate the agreement on the significance of AI principles and identify the challenging factors that could negatively impact the adoption of AI ethics principles. The results reveal that the global convergence set consists of 22 ethical principles and 15 challenges. Transparency, privacy, accountability and fairness are identified as the most common AI ethics principles. Similarly, lack of ethical knowledge and vague principles are reported as the significant challenges for considering ethics in AI. The findings of this study are the preliminary inputs for proposing a maturity model that assesses the ethical capabilities of AI systems and provides best practices for further improvements.",
    "citation_count": 57,
    "summary": "This systematic literature review identifies 22 commonly agreed-upon ethical principles for AI, with transparency, privacy, accountability, and fairness prominent, alongside 15 challenges to their adoption, including lack of ethical knowledge and vague principles. The findings inform the development of an AI ethics maturity model."
  },
  {
    "title": "The future of work: freedom, justice and capital in the age of artificial intelligence",
    "abstract": "ABSTRACT Artificial Intelligence (AI) is predicted to have a deep impact on the future of work and employment. The paper outlines a normative framework to understand and protect human freedom and justice in this transition. The proposed framework is based on four main ideas: going beyond the idea of a Basic Income to compensate the losers in the transition towards AI-driven work, towards a Responsible Innovation approach, in which the development of AI technologies is governed by an inclusive and deliberate societal judgment; going beyond a philosophical conceptualisation of social justice only focused on the distribution of 'primary goods', towards one focused on the different goals, values, and virtues of various social practices (Walzer's 'spheres of justice') and the different individual capabilities of persons (Sen's 'capabilities'); going beyond a classical understanding of capital, towards one explicitly including mental capacities as a source of value for AI-driven activities. In an effort to promote an interdisciplinary approach, the paper combines political and economic theories of freedom, justice and capital with recent approaches in applied ethics of technology, and starts applying its normative framework to some concrete example of AI-based systems: healthcare robotics, 'citizen science', social media and platform economy.",
    "published_date": "2021-12-13",
    "citation_count": 15,
    "url": "https://www.tandfonline.com/doi/full/10.1080/13698230.2021.2008204",
    "summary": "This paper proposes a normative framework for ensuring freedom and justice in the AI-driven future of work, advocating for responsible AI innovation, a broader conception of social justice encompassing individual capabilities and diverse social practices, and a revised understanding of capital that includes mental capacities. It applies this framework to analyze several AI-based systems."
  },
  {
    "title": "Moral Disagreement and Artificial Intelligence",
    "abstract": "Artificially intelligent systems will be used to make increasingly important decisions about us. Many of these decisions will have to be made without consensus about the relevant moral facts. I argue that what makes moral disagreement especially challenging is that there are two different ways of handling it: political solutions, which aim to find a fair compromise, and epistemic solutions, which aim at moral truth.",
    "published_date": "2021-07-21",
    "citation_count": 6,
    "url": "https://dl.acm.org/doi/10.1145/3461702.3462534",
    "summary": "The increasing use of AI in decision-making necessitates addressing moral disagreements, which are complex because they require navigating both the need for fair compromise (political solutions) and the pursuit of moral truth (epistemic solutions). This dual challenge complicates the ethical integration of AI."
  },
  {
    "url": "https://www.alignmentforum.org/tag/ethics-and-morality",
    "author": "Wei Dai",
    "title": "Ethics & Morality - AI Alignment Forum",
    "published_date": "2021-12-02",
    "summary": "The article is a directory of discussions related to ethics and morality, including various philosophical viewpoints and thought experiments. It links to related articles on consequentialism, deontology, metaethics, and moral uncertainty."
  }
]