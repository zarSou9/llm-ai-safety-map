[
  {
    "url": "https://arxiv.org/abs/2402.16869",
    "title": "Considering Fundamental Rights in the European Standardisation of Artificial Intelligence: Nonsense or Strategic Alliance?",
    "published_date": "2024-01-23",
    "abstract": "In the European context, both the EU AI Act proposal and the draft Standardisation Request on safe and trustworthy AI link standardisation to fundamental rights. However, these texts do not provide any guidelines that specify and detail the relationship between AI standards and fundamental rights, its meaning or implication. This chapter aims to clarify this critical regulatory blind spot. The main issue tackled is whether the adoption of AI harmonised standards, based on the future AI Act, should take into account fundamental rights. In our view, the response is yes. The high risks posed by certain AI systems relate in particular to infringements of fundamental rights. Therefore, mitigating such risks involves fundamental rights considerations and this is what future harmonised standards should reflect. At the same time, valid criticisms of the European standardisation process have to be addressed. Finally, the practical incorporation of fundamental rights considerations in the ongoing European standardisation of AI systems is discussed.",
    "summary": "The paper argues that European AI standardization must explicitly incorporate fundamental rights considerations to mitigate the high risks of AI systems infringing upon them, addressing a current regulatory gap despite valid criticisms of the standardization process."
  },
  {
    "url": "https://www.alignmentforum.org/posts/6nNwMbdRXZDuNd4Gx/analysis-of-global-ai-governance-strategies",
    "author": "Sammy Martin, Justin Bullock, Corin Katzke",
    "title": "Analysis of Global AI Governance Strategies",
    "published_date": "2024-12-04",
    "summary": "The article analyzes three AI governance strategies—Cooperative Development, Strategic Advantage, and Global Moratorium—evaluating their effectiveness based on the difficulty of aligning AI and development timelines. The optimal strategy shifts depending on these factors, with Cooperative Development preferred for longer timelines and easier alignment, Strategic Advantage for shorter timelines or moderate difficulty, and Global Moratorium reserved for scenarios with extremely difficult alignment or short timelines."
  },
  {
    "url": "https://www.lesswrong.com/posts/324pQjqoHEHeF2vPs/ai-clarity-an-initial-research-agenda",
    "author": "Justin Bullock, Corin Katzke, Zershaaneh Qureshi, David_Kristoffersson",
    "title": "AI Clarity: An Initial Research Agenda",
    "published_date": "2024-05-03",
    "summary": "The AI Clarity research program uses scenario planning to explore potential trajectories of transformative AI (TAI), focusing on the possibility of TAI emerging within the next decade. The program aims to identify existential risks posed by TAI, evaluate strategies for mitigating those risks across different scenarios, and recommend effective safety and governance measures."
  },
  {
    "url": "https://www.alignmentforum.org/posts/XSqntCNMafhcy9irf/third-party-testing-as-a-key-ingredient-of-ai-policy",
    "author": "Zac Hatfield-Dodds",
    "title": "Third-party testing as a key ingredient of AI policy",
    "published_date": "2024-03-25",
    "summary": "The authors advocate for third-party testing of large-scale AI systems to mitigate societal harm, arguing that such a regime is necessary to manage the risks of powerful, adaptable AI models while fostering innovation and avoiding overly restrictive regulation. This would involve establishing trusted testing procedures and standards, applicable to a select group of high-impact systems, to build public trust and facilitate international cooperation."
  },
  {
    "url": "https://www.lesswrong.com/posts/wwioAJHTeaGqBvtjd/update-on-developing-an-ethics-calculator-to-align-an-agi-to",
    "author": "Sweenesm",
    "title": "Update on Developing an Ethics Calculator to Align an AGI to",
    "published_date": "2024-03-12",
    "summary": "This article details the ongoing development of an \"ethics calculator\" designed to align artificial general intelligence (AGI) with ethical principles. The calculator uses a utilitarian framework, incorporating measures of positive experiences, rights, and self-esteem, aiming for quantitative, consistent ethical predictions across diverse scenarios."
  },
  {
    "url": "https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research",
    "author": "Roman Leventov",
    "title": "A multi-disciplinary view on AI safety research",
    "published_date": "2023-02-08",
    "summary": "The article advocates for a multidisciplinary approach to AI safety research, arguing that a solely technical focus is insufficient and that progress requires integrating diverse fields like neuroscience, sociology, and political science to design \"safe\" and beneficial artificial general intelligence. This approach prioritizes a top-down design of \"civilizational intelligence\" rather than passively reacting to AI's technological evolution."
  },
  {
    "url": "https://www.lesswrong.com/posts/nnGwHuJfCBxKDgsds/embedding-ethical-priors-into-ai-systems-a-bayesian-approach",
    "author": "Justausername",
    "title": "Embedding Ethical Priors into AI Systems: A Bayesian Approach",
    "published_date": "2023-08-03",
    "summary": "This paper proposes a novel framework for building ethically aligned AI systems by incorporating ethical principles as Bayesian priors. These \"ethical priors,\" analogous to human moral intuitions, guide the AI's learning and decision-making process, shaping its judgments in a way consistent with human values."
  },
  {
    "url": "https://arxiv.org/pdf/2208.14788.pdf",
    "title": "Negative Human Rights as a Basis for Long-term AI Safety and Regulation",
    "published_date": "2022-08-31",
    "abstract": "If autonomous AI systems are to be reliably safe in novel situations, they will need to incorporate general principles guiding them to recognize and avoid harmful behaviours. Such principles may need to be supported by a binding system of regulation, which would need the underlying principles to be widely accepted. They should also be specific enough for technical implementation. Drawing inspiration from law, this article explains how negative human rights could fulfil the role of such principles and serve as a foundation both for an international regulatory system and for building technical safety constraints for future AI systems.\nThis article appears in the AI & Society track.",
    "citation_count": 9,
    "summary": "This paper proposes using negative human rights – rights that protect individuals from harm – as foundational principles for both international AI regulation and the technical safety constraints of autonomous AI systems, ensuring their safe operation in novel situations. These principles offer a widely-accepted and practically implementable basis for AI safety."
  },
  {
    "url": "https://arxiv.org/abs/2210.02667",
    "title": "A Human Rights-Based Approach to Responsible AI",
    "published_date": "2022-10-06",
    "abstract": "Research on fairness, accountability, transparency and ethics of AI-based interventions in society has gained much-needed momen-tum in recent years. However it lacks an explicit alignment with a set of normative values and principles that guide this research and interventions. Rather, an implicit consensus is often assumed to hold for the values we impart into our models – something that is at odds with the pluralistic world we live in. In this paper, we put forth the doctrine of universal human rights as a set of globally salient and cross-culturally recognized set of values that can serve as a grounding framework for explicit value alignment in responsible AI – and discuss its eﬃcacy as a framework for civil society partnership and participation. We argue that a human rights framework orients the research in this space away from the machines and the risks of their biases, and towards humans and the risks to their rights, essentially helping to center the conversation around who is harmed, what harms they face, and how those harms may be mitigated.",
    "citation_count": 30,
    "summary": "This paper advocates for a human rights-based framework to guide responsible AI development, arguing that prioritizing universally recognized human rights offers a more robust and globally applicable approach to mitigating AI harms than relying on implicit or culturally specific values. This framework shifts the focus from technical biases to the impact on human rights, centering those potentially harmed by AI systems."
  },
  {
    "title": "The future of work: freedom, justice and capital in the age of artificial intelligence",
    "abstract": "ABSTRACT Artificial Intelligence (AI) is predicted to have a deep impact on the future of work and employment. The paper outlines a normative framework to understand and protect human freedom and justice in this transition. The proposed framework is based on four main ideas: going beyond the idea of a Basic Income to compensate the losers in the transition towards AI-driven work, towards a Responsible Innovation approach, in which the development of AI technologies is governed by an inclusive and deliberate societal judgment; going beyond a philosophical conceptualisation of social justice only focused on the distribution of 'primary goods', towards one focused on the different goals, values, and virtues of various social practices (Walzer's 'spheres of justice') and the different individual capabilities of persons (Sen's 'capabilities'); going beyond a classical understanding of capital, towards one explicitly including mental capacities as a source of value for AI-driven activities. In an effort to promote an interdisciplinary approach, the paper combines political and economic theories of freedom, justice and capital with recent approaches in applied ethics of technology, and starts applying its normative framework to some concrete example of AI-based systems: healthcare robotics, 'citizen science', social media and platform economy.",
    "published_date": "2021-12-13",
    "citation_count": 15,
    "url": "https://www.tandfonline.com/doi/full/10.1080/13698230.2021.2008204",
    "summary": "This paper proposes a normative framework for ensuring freedom and justice in the AI-driven future of work, advocating for responsible AI innovation, a broader conception of social justice encompassing individual capabilities and diverse social practices, and a redefined understanding of capital that includes mental capacities. It applies this framework to various AI-based systems to guide ethical development and deployment."
  }
]