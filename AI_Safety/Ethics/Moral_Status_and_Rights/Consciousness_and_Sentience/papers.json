[
  {
    "url": "https://arxiv.org/abs/2404.15369",
    "title": "Can a Machine be Conscious? Towards Universal Criteria for Machine Consciousness",
    "published_date": "2024-04-19",
    "abstract": "As artificially intelligent systems become more anthropomorphic and pervasive, and their potential impact on humanity more urgent, discussions about the possibility of machine consciousness have significantly intensified, and it is sometimes seen as 'the holy grail'. Many concerns have been voiced about the ramifications of creating an artificial conscious entity. This is compounded by a marked lack of consensus around what constitutes consciousness and by an absence of a universal set of criteria for determining consciousness. By going into depth on the foundations and characteristics of consciousness, we propose five criteria for determining whether a machine is conscious, which can also be applied more generally to any entity. This paper aims to serve as a primer and stepping stone for researchers of consciousness, be they in philosophy, computer science, medicine, or any other field, to further pursue this holy grail of philosophy, neuroscience and artificial intelligence.",
    "summary": "This paper addresses the growing debate surrounding machine consciousness by proposing five universal criteria for determining consciousness in any entity, aiming to establish a framework for future research across multiple disciplines. These criteria are grounded in a detailed examination of the fundamental characteristics of consciousness."
  },
  {
    "url": "https://arxiv.org/abs/2308.08708v2",
    "title": "Consciousness in Artificial Intelligence: Insights from the Science of Consciousness",
    "published_date": "2023-08-17",
    "abstract": "Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive\"indicator properties\"of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.",
    "citation_count": 77,
    "summary": "This paper examines the possibility of consciousness in AI by applying neuroscientific theories of consciousness to analyze current AI systems. The authors find no evidence of consciousness in existing systems but identify no inherent technical obstacles to creating conscious AI in the future."
  },
  {
    "url": "https://arxiv.org/pdf/2105.07879.pdf",
    "title": "Conscious AI",
    "published_date": "2021-05-12",
    "abstract": "Recent advances in artificial intelligence (AI) have achieved human-scale speed and accuracy for classification tasks. In turn, these capabilities have made AI a viable replacement for many human activities that at their core involve classification, such as basic mechanical and analytical tasks in low-level service jobs. Current systems do not need to be conscious to recognize patterns and classify them. 1 However, for AI to progress to more complicated tasks requiring intuition and empathy, it must develop capabilities such as metathinking, creativity, and empathy akin to human self-awareness or consciousness. We contend that such a paradigm shift is possible only through a fundamental shift in the state of artificial intelligence toward consciousness, a shift similar to what took place for humans through the process of natural selection and evolution. As such, this paper aims to theoretically explore the requirements for the emergence of consciousness in AI. It also provides a principled understanding of how conscious AI can be detected and how it might be manifested in contrast to the dominant paradigm that seeks to ultimately create machines that are linguistically indistinguishable from humans.",
    "citation_count": 2,
    "summary": "This paper argues that achieving truly advanced AI, capable of complex tasks requiring intuition and empathy, necessitates the development of artificial consciousness, proposing a theoretical exploration of its requirements and detection. It contrasts this approach with the current focus on creating AI that mimics human linguistic abilities."
  },
  {
    "url": "https://www.lesswrong.com/tag/the-hard-problem-of-consciousness",
    "author": "Charbel-RaphaÃ«l",
    "title": "The Hard Problem of Consciousness - LessWrong",
    "published_date": "2024-04-06",
    "summary": "The hard problem of consciousness centers on explaining how and why subjective experiences (qualia) arise from physical processes, a gap between objective brain activity and the felt quality of experience. While the existence of subjective experience is undeniable, the mechanism by which it emerges from physical processes remains a significant and debated mystery."
  },
  {
    "url": "https://www.lesswrong.com/tag/machine-ethics",
    "title": "Machine Ethics - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Machine ethics seeks to create Artificial Moral Agents (AMAs) capable of moral reasoning, a challenge highlighted by the limitations of past attempts like Asimov's Three Laws of Robotics. The field's urgency increases with the potential development of superintelligence, necessitating the creation of functional ethical guidelines for AI before it becomes highly advanced."
  },
  {
    "url": "https://www.lesswrong.com/tag/artificial-general-intelligence-agi",
    "title": "Artificial General Intelligence (AGI) - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Artificial General Intelligence (AGI) refers to machines capable of intelligent behavior across diverse domains, unlike narrow AI which excels only in specific tasks. The development of AGI is anticipated within the coming decades, though its potential risks, including existential threats posed by unfriendly AGI, are also widely discussed."
  },
  {
    "url": "https://www.lesswrong.com/tag/zombies",
    "title": "Zombies - LessWrong",
    "published_date": "2024-02-01",
    "summary": "The philosophical zombie thought experiment posits beings physically identical to humans but lacking consciousness, used to argue against physicalism's claim that consciousness is solely a physical phenomenon. Different responses to this include denying the possibility of zombies, accepting their possibility but claiming consciousness is an illusion, or asserting that zombies' conceivability demonstrates the inadequacy of purely physical explanations for consciousness."
  },
  {
    "title": "The Moral Significance of the Phenomenology of Phenomenal Consciousness in Case of Artificial Agents",
    "abstract": "or sufficiently requires consciousness. There are sound epistemological problems that undermine the alleged strength of the judgment of necessity. On the other hand, this commentary has also sought to shed light on the limitations of his proposal. His defense of alternative properties to consciousness is insufficient, which, in our view, makes the theoretical and practical consequences he draws non-ideal. The question of the moral status of AI provides an interesting test case to radicalize Shepherd's presuppositions and go beyond conventional approaches.",
    "published_date": "2023-04-03",
    "url": "https://www.tandfonline.com/doi/full/10.1080/21507740.2023.2188284",
    "summary": "This commentary critiques Shepherd's argument that artificial agents' moral status doesn't necessarily require consciousness, highlighting epistemological weaknesses and insufficient alternatives to consciousness as grounds for rejecting his conclusions. The authors propose using AI's moral status as a test case to refine the debate."
  },
  {
    "url": "https://arxiv.org/pdf/2311.08576.pdf",
    "title": "Towards Evaluating AI Systems for Moral Status Using Self-Reports",
    "published_date": "2023-11-14",
    "abstract": "As AI systems become more advanced and widely deployed, there will likely be increasing debate over whether AI systems could have conscious experiences, desires, or other states of potential moral significance. It is important to inform these discussions with empirical evidence to the extent possible. We argue that under the right circumstances, self-reports, or an AI system's statements about its own internal states, could provide an avenue for investigating whether AI systems have states of moral significance. Self-reports are the main way such states are assessed in humans (\"Are you in pain?\"), but self-reports from current systems like large language models are spurious for many reasons (e.g. often just reflecting what humans would say). To make self-reports more appropriate for this purpose, we propose to train models to answer many kinds of questions about themselves with known answers, while avoiding or limiting training incentives that bias self-reports. The hope of this approach is that models will develop introspection-like capabilities, and that these capabilities will generalize to questions about states of moral significance. We then propose methods for assessing the extent to which these techniques have succeeded: evaluating self-report consistency across contexts and between similar models, measuring the confidence and resilience of models' self-reports, and using interpretability to corroborate self-reports. We also discuss challenges for our approach, from philosophical difficulties in interpreting self-reports to technical reasons why our proposal might fail. We hope our discussion inspires philosophers and AI researchers to criticize and improve our proposed methodology, as well as to run experiments to test whether self-reports can be made reliable enough to provide information about states of moral significance.",
    "citation_count": 7,
    "summary": "This paper proposes using AI self-reports to investigate whether AI systems possess states of moral significance, suggesting methods to improve the reliability of these reports by training models to answer questions about themselves with known answers and then assessing consistency and confidence. The authors acknowledge significant philosophical and technical challenges to this approach."
  },
  {
    "url": "https://www.alignmentforum.org/posts/KpD2fJa6zo8o2MBxg/consciousness-as-a-conflationary-alliance-term",
    "author": "Andrew_Critch",
    "title": "Consciousness as a conflationary alliance term for intrinsically valued internal experiences",
    "published_date": "2023-07-10",
    "summary": "The author argues that the term \"consciousness\" is highly conflated, with individuals holding vastly different understandings of its meaning, yet converging on its moral value. This shared valuation, despite differing definitions, creates a \"conflationary alliance\" resistant to clarifying the term's ambiguity."
  }
]