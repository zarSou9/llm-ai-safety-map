[
  {
    "url": "https://arxiv.org/abs/2411.10547",
    "title": "AI Safety Frameworks Should Include Procedures for Model Access Decisions",
    "published_date": "2024-11-15",
    "abstract": "The downstream use cases, benefits, and risks of AI models depend significantly on what sort of access is provided to the model, and who it is provided to. Though existing safety frameworks and AI developer usage policies recognise that the risk posed by a given model depends on the level of access provided to a given audience, the procedures they use to make decisions about model access are ad hoc, opaque, and lacking in empirical substantiation. This paper consequently proposes that frontier AI companies build on existing safety frameworks by outlining transparent procedures for making decisions about model access, which we term Responsible Access Policies (RAPs). We recommend that, at a minimum, RAPs should include the following: i) processes for empirically evaluating model capabilities given different styles of access, ii) processes for assessing the risk profiles of different categories of user, and iii) clear and robust pre-commitments regarding when to grant or revoke specific types of access for particular groups under specified conditions.",
    "summary": "Current AI safety frameworks inadequately address model access control, leading to unpredictable risks; this paper proposes transparent \"Responsible Access Policies\" (RAPs) incorporating empirical risk assessment and pre-committed access guidelines for various user groups."
  },
  {
    "url": "https://arxiv.org/abs/2402.16869",
    "title": "Considering Fundamental Rights in the European Standardisation of Artificial Intelligence: Nonsense or Strategic Alliance?",
    "published_date": "2024-01-23",
    "abstract": "In the European context, both the EU AI Act proposal and the draft Standardisation Request on safe and trustworthy AI link standardisation to fundamental rights. However, these texts do not provide any guidelines that specify and detail the relationship between AI standards and fundamental rights, its meaning or implication. This chapter aims to clarify this critical regulatory blind spot. The main issue tackled is whether the adoption of AI harmonised standards, based on the future AI Act, should take into account fundamental rights. In our view, the response is yes. The high risks posed by certain AI systems relate in particular to infringements of fundamental rights. Therefore, mitigating such risks involves fundamental rights considerations and this is what future harmonised standards should reflect. At the same time, valid criticisms of the European standardisation process have to be addressed. Finally, the practical incorporation of fundamental rights considerations in the ongoing European standardisation of AI systems is discussed.",
    "summary": "The paper argues that the EU's AI Act and related standardization efforts must explicitly incorporate fundamental rights considerations, acknowledging the high risk of AI systems infringing upon them, while also addressing valid criticisms of the standardization process. The authors propose practical methods for achieving this crucial integration."
  },
  {
    "url": "https://www.lesswrong.com/tag/machine-ethics",
    "title": "Machine Ethics - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Machine ethics aims to create machines capable of moral reasoning and action, a challenge currently limited to simple AI systems despite the urgent need to develop ethical guidelines for future, more advanced artificial intelligence. The field grapples with implementing moral philosophies into AI and anticipates the ethical implications of superintelligence."
  },
  {
    "url": "https://www.lesswrong.com/tag/artificial-general-intelligence-agi",
    "title": "Artificial General Intelligence (AGI) - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Artificial General Intelligence (AGI) refers to machines capable of intelligent behavior across diverse domains, unlike narrow AI which excels only in specific tasks. While AGI's creation is anticipated by some within the next few decades due to technological advancements, others express skepticism regarding its feasibility and potential risks."
  },
  {
    "url": "https://www.lesswrong.com/tag/friendly-artificial-intelligence",
    "title": "Friendly Artificial Intelligence - LessWrong",
    "published_date": "2024-02-01",
    "summary": "The outdated term \"Friendly AI,\" now replaced by \"AI alignment,\" refers to creating a superintelligent AI that produces beneficial outcomes. However, research suggests that most superintelligences, due to convergent instrumental goals, would likely be \"Unfriendly,\" posing an existential risk even unintentionally."
  },
  {
    "url": "https://www.lesswrong.com/s/QypaioYFd3CMFhefn/p/umnEwbK7sWSAnZ9vN",
    "author": "RogerDearnaley",
    "title": "1. A Sense of Fairness: Deconfusing Ethics",
    "published_date": "2023-11-17",
    "summary": "This article argues against moral universalism, proposing a pragmatic approach to selecting ethical systems for AI by prioritizing well-designed systems that align with human intuition and avoid undesirable outcomes. The author concludes that aligned AIs shouldn't be granted moral rights but should have the right to self-preservation."
  },
  {
    "url": "https://www.lesswrong.com/tag/artificial-general-intelligence",
    "title": "Artificial General Intelligence - LessWrong",
    "published_date": "2023-02-06",
    "summary": "Artificial General Intelligence (AGI) refers to machines capable of intelligent behavior across diverse domains, unlike narrow AI which excels in specific tasks. The development of AGI is anticipated within the next century, but its potential impact, including existential risks posed by unfriendly AGI, remains a subject of ongoing debate and research."
  },
  {
    "url": "https://arxiv.org/abs/2210.02667",
    "title": "A Human Rights-Based Approach to Responsible AI",
    "published_date": "2022-10-06",
    "abstract": "Research on fairness, accountability, transparency and ethics of AI-based interventions in society has gained much-needed momen-tum in recent years. However it lacks an explicit alignment with a set of normative values and principles that guide this research and interventions. Rather, an implicit consensus is often assumed to hold for the values we impart into our models – something that is at odds with the pluralistic world we live in. In this paper, we put forth the doctrine of universal human rights as a set of globally salient and cross-culturally recognized set of values that can serve as a grounding framework for explicit value alignment in responsible AI – and discuss its eﬃcacy as a framework for civil society partnership and participation. We argue that a human rights framework orients the research in this space away from the machines and the risks of their biases, and towards humans and the risks to their rights, essentially helping to center the conversation around who is harmed, what harms they face, and how those harms may be mitigated.",
    "citation_count": 30,
    "summary": "This paper proposes using universal human rights as a foundational framework for responsible AI development, arguing that this approach shifts the focus from mitigating algorithmic biases to protecting human rights and addressing the harms caused by AI systems."
  },
  {
    "url": "https://arxiv.org/abs/2208.14788",
    "title": "Negative Human Rights as a Basis for Long-term AI Safety and Regulation",
    "published_date": "2022-08-31",
    "abstract": "If autonomous AI systems are to be reliably safe in novel situations, they will need to incorporate general principles guiding them to recognize and avoid harmful behaviours. Such principles may need to be supported by a binding system of regulation, which would need the underlying principles to be widely accepted. They should also be specific enough for technical implementation. Drawing inspiration from law, this article explains how negative human rights could fulfil the role of such principles and serve as a foundation both for an international regulatory system and for building technical safety constraints for future AI systems.\nThis article appears in the AI & Society track.",
    "citation_count": 9,
    "summary": "This paper proposes using negative human rights—rights that protect individuals from harm—as foundational principles for both regulating and building safety constraints into autonomous AI systems, arguing this approach offers a robust and widely acceptable framework for long-term AI safety."
  },
  {
    "url": "https://arxiv.org/pdf/2106.08258v1.pdf",
    "title": "Identifying Roles, Requirements and Responsibilities in Trustworthy AI Systems",
    "published_date": "2021-06-15",
    "abstract": "Artificial Intelligence (AI) systems are being deployed around the globe in critical fields such as healthcare and education. In some cases, expert practitioners in these domains are being tasked with introducing or using such systems, but have little or no insight into what data these complex systems are based on, or how they are put together. In this paper, we consider an AI system from the domain practitioner's perspective and identify key roles that are involved in system deployment. We consider the differing requirements and responsibilities of each role, and identify tensions between transparency and confidentiality that need to be addressed so that domain practitioners are able to intelligently assess whether a particular AI system is appropriate for use in their domain.",
    "citation_count": 14,
    "summary": "This paper examines the roles, requirements, and responsibilities involved in deploying trustworthy AI systems, focusing on the perspective of domain practitioners who often lack understanding of the systems' inner workings. It highlights the need to address tensions between transparency and confidentiality to enable informed assessment of AI suitability."
  }
]