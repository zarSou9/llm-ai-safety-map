[
  {
    "url": "https://arxiv.org/abs/2404.04059",
    "title": "On the Quest for Effectiveness in Human Oversight: Interdisciplinary Perspectives",
    "published_date": "2024-04-05",
    "abstract": "Human oversight is currently discussed as a potential safeguard to counter some of the negative aspects of high-risk AI applications. This prompts a critical examination of the role and conditions necessary for what is prominently termed effective or meaningful human oversight of these systems. This paper investigates effective human oversight by synthesizing insights from psychological, legal, philosophical, and technical domains. Based on the claim that the main objective of human oversight is risk mitigation, we propose a viable understanding of effectiveness in human oversight: for human oversight to be effective, the oversight person has to have (a) sufficient causal power with regard to the system and its effects, (b) suitable epistemic access to relevant aspects of the situation, (c) self-control, and (d) fitting intentions for their role. Furthermore, we argue that this is equivalent to saying that an oversight person is effective if and only if they are morally responsible and have fitting intentions. Against this backdrop, we suggest facilitators and inhibitors of effectiveness in human oversight when striving for practical applicability. We discuss factors in three domains, namely, the technical design of the system, individual factors of oversight persons, and the environmental circumstances in which they operate. Finally, this paper scrutinizes the upcoming AI Act of the European Union – in particular Article 14 on Human Oversight – as an exemplary regulatory framework in which we study the practicality of our understanding of effective human oversight. By analyzing the provisions and implications of the European AI Act proposal, we pinpoint how far that proposal aligns with our analyses regarding effective human oversight as well as how it might get enriched by our conceptual understanding of effectiveness in human oversight.",
    "citation_count": 7,
    "summary": "This paper examines effective human oversight of high-risk AI, arguing that effectiveness requires sufficient causal power, epistemic access, self-control, and appropriate intentions—essentially, moral responsibility. It analyzes these factors across technical, individual, and environmental domains, using the EU AI Act as a case study to assess practical implications."
  },
  {
    "url": "http://arxiv.org/abs/2401.14908",
    "title": "A Framework for Assurance Audits of Algorithmic Systems",
    "published_date": "2024-01-26",
    "abstract": "An increasing number of regulations propose 'AI audits' as a mechanism for achieving transparency and accountability for artificial intelligence (AI) systems. Despite some converging norms around various forms of AI auditing, auditing for the purpose of compliance and assurance currently lacks agreed-upon practices, procedures, taxonomies, and standards. We propose the 'criterion audit' as an operationalizable compliance and assurance external audit framework. We model elements of this approach after financial auditing practices, and argue that AI audits should similarly provide assurance to their stakeholders about AI organizations' ability to govern their algorithms in ways that mitigate harms and uphold human values. We discuss the necessary conditions for the criterion audit and provide a procedural blueprint for performing an audit engagement in practice. We illustrate how this framework can be adapted to current regulations by deriving the criteria on which 'bias audits' can be performed for in-scope hiring algorithms, as required by the recently effective New York City Local Law 144 of 2021. We conclude by offering a critical discussion on the benefits, inherent limitations, and implementation challenges of applying practices of the more mature financial auditing industry to AI auditing where robust guardrails against quality assurance issues are only starting to emerge. Our discussion—informed by experiences in performing these audits in practice—highlights the critical role that an audit ecosystem plays in ensuring the effectiveness of audits.",
    "citation_count": 2,
    "summary": "This paper proposes a \"criterion audit\" framework for assuring algorithmic system compliance, drawing parallels with financial auditing practices to provide stakeholders with assurance of responsible AI governance and harm mitigation. The framework is illustrated through a bias audit example and discusses its benefits, limitations, and implementation challenges within a broader audit ecosystem."
  },
  {
    "url": "https://arxiv.org/abs/2402.17350",
    "title": "Towards an Enforceable GDPR Specification",
    "published_date": "2024-02-27",
    "abstract": "While Privacy by Design (PbD) is prescribed by modern privacy regulations such as the EU's GDPR, achieving PbD in real software systems is a notoriously difficult task. One emerging technique to realize PbD is Runtime enforcement (RE), in which an enforcer, loaded with a specification of a system's privacy requirements, observes the actions performed by the system and instructs it to perform actions that will ensure compliance with these requirements at all times. To be able to use RE techniques for PbD, privacy regulations first need to be translated into an enforceable specification. In this paper, we report on our ongoing work in formalizing the GDPR. We first present a set of requirements and an iterative methodology for creating enforceable formal specifications of legal provisions. Then, we report on a preliminary case study in which we used our methodology to derive an enforceable specification of part of the GDPR. Our case study suggests that our methodology can be effectively used to develop accurate enforceable specifications.",
    "summary": "This paper proposes a methodology for translating the GDPR into an enforceable formal specification using runtime enforcement, aiming to bridge the gap between Privacy by Design principles and practical software implementation. A preliminary case study demonstrates the feasibility of this methodology for creating accurate and enforceable specifications of GDPR provisions."
  },
  {
    "url": "https://arxiv.org/abs/2303.03174",
    "title": "Both eyes open: Vigilant Incentives help Regulatory Markets improve AI Safety",
    "published_date": "2023-03-06",
    "abstract": "In the context of rapid discoveries by leaders in AI, governments must consider how to design regulation that matches the increasing pace of new AI capabilities. Regulatory Markets for AI is a proposal designed with adaptability in mind. It involves governments setting outcome-based targets for AI companies to achieve, which they can show by purchasing services from a market of private regulators. We use an evolutionary game theory model to explore the role governments can play in building a Regulatory Market for AI systems that deters reckless behaviour. We warn that it is alarmingly easy to stumble on incentives which would prevent Regulatory Markets from achieving this goal. These 'Bounty Incentives' only reward private regulators for catching unsafe behaviour. We argue that AI companies will likely learn to tailor their behaviour to how much effort regulators invest, discouraging regulators from innovating. Instead, we recommend that governments always reward regulators, except when they find that those regulators failed to detect unsafe behaviour that they should have. These 'Vigilant Incentives' could encourage private regulators to find innovative ways to evaluate cutting-edge AI systems.",
    "citation_count": 3,
    "summary": "The paper models a Regulatory Market for AI safety, arguing that \"Vigilant Incentives,\" which reward private regulators unless they miss detectable unsafe AI behavior, are superior to \"Bounty Incentives\" for encouraging regulator innovation and deterring reckless AI development. This contrasts with simpler incentive schemes that inadvertently stifle innovation."
  },
  {
    "url": "https://arxiv.org/abs/2308.04448",
    "title": "Dual Governance: The intersection of centralized regulation and crowdsourced safety mechanisms for Generative AI",
    "published_date": "2023-08-02",
    "abstract": "Generative Artificial Intelligence (AI) has seen mainstream adoption lately, especially in the form of consumer-facing, open-ended, text and image generating models. However, the use of such systems raises significant ethical and safety concerns, including privacy violations, misinformation and intellectual property theft. The potential for generative AI to displace human creativity and livelihoods has also been under intense scrutiny. To mitigate these risks, there is an urgent need of policies and regulations responsible and ethical development in the field of generative AI. Existing and proposed centralized regulations by governments to rein in AI face criticisms such as not having sufficient clarity or uniformity, lack of interoperability across lines of jurisdictions, restricting innovation, and hindering free market competition. Decentralized protections via crowdsourced safety tools and mechanisms are a potential alternative. However, they have clear deficiencies in terms of lack of adequacy of oversight and difficulty of enforcement of ethical and safety standards, and are thus not enough by themselves as a regulation mechanism. We propose a marriage of these two strategies via a framework we call Dual Governance. This framework proposes a cooperative synergy between centralized government regulations in a U.S. specific context and safety mechanisms developed by the community to protect stakeholders from the harms of generative AI. By implementing the Dual Governance framework, we posit that innovation and creativity can be promoted while ensuring safe and ethical deployment of generative AI.",
    "citation_count": 3,
    "summary": "This paper proposes \"Dual Governance\" for generative AI, a framework combining centralized government regulation with decentralized, crowdsourced safety mechanisms to mitigate ethical and safety risks while fostering innovation. This approach aims to address the shortcomings of solely relying on either centralized or decentralized control."
  },
  {
    "url": "https://www.lesswrong.com/posts/cCbybRT8bgiMbEHEv/a-list-of-all-the-deadlines-in-biden-s-executive-order-on-ai",
    "author": "Ricki Heicklen",
    "title": "Toward a Broader Conception of Adverse Selection",
    "published_date": "2023-11-01",
    "summary": "Biden's October 30, 2023 Executive Order on AI outlines numerous deadlines for various federal agencies to submit reports, develop strategies, and implement initiatives related to AI development, deployment, workforce needs, and risk mitigation, spanning from November 2023 to January 2024 and beyond. These actions aim to address ethical considerations, talent acquisition, and responsible AI governance across multiple sectors."
  },
  {
    "url": "https://arxiv.org/pdf/2105.01434.pdf",
    "title": "Towards Accountability in the Use of Artificial Intelligence for Public Administrations",
    "published_date": "2021-05-04",
    "abstract": "We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared.",
    "citation_count": 39,
    "summary": "The paper argues that using AI in public administration creates challenges to accountability due to distributed responsibility and lack of transparency, advocating for both direct public and indirect auditor transparency to ensure ethical and democratic governance. It finds current guidelines insufficient, suggesting a need for clearer definitions of auditor roles and auditing goals to develop meaningful ethical standards for AI use in the public sector."
  },
  {
    "url": "https://arxiv.org/pdf/2004.07213.pdf",
    "title": "Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims",
    "published_date": "2020-04-15",
    "abstract": "With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",
    "citation_count": 293,
    "summary": "This report examines mechanisms for verifying claims about AI system safety, security, fairness, and privacy, arguing that verifiable claims are crucial for building trust in responsible AI development. It analyzes ten mechanisms—institutional, software, and hardware—and offers recommendations for their implementation and improvement."
  }
]