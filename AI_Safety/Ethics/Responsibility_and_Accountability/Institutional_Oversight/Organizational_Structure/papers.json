[
  {
    "url": "https://arxiv.org/abs/2412.17114",
    "title": "Decentralized Governance of Autonomous AI Agents",
    "published_date": "2024-12-22",
    "abstract": "Autonomous AI agents present transformative opportunities and significant governance challenges. Existing frameworks, such as the EU AI Act and the NIST AI Risk Management Framework, fall short of addressing the complexities of these agents, which are capable of independent decision-making, learning, and adaptation. To bridge these gaps, we propose the ETHOS (Ethical Technology and Holistic Oversight System) framework, a decentralized governance (DeGov) model leveraging Web3 technologies, including blockchain, smart contracts, and decentralized autonomous organizations (DAOs). ETHOS establishes a global registry for AI agents, enabling dynamic risk classification, proportional oversight, and automated compliance monitoring through tools like soulbound tokens and zero-knowledge proofs. Furthermore, the framework incorporates decentralized justice systems for transparent dispute resolution and introduces AI specific legal entities to manage limited liability, supported by mandatory insurance to ensure financial accountability and incentivize ethical design. By integrating philosophical principles of rationality, ethical grounding, and goal alignment, ETHOS aims to create a robust research agenda for promoting trust, transparency, and participatory governance. This innovative framework offers a scalable and inclusive strategy for regulating AI agents, balancing innovation with ethical responsibility to meet the demands of an AI-driven future.",
    "summary": "The ETHOS framework proposes a decentralized governance model for autonomous AI agents using Web3 technologies, addressing the limitations of existing regulatory frameworks by establishing a global registry, automated compliance monitoring, and decentralized justice systems. This approach aims to balance AI innovation with ethical responsibility through transparency, accountability, and participatory governance."
  },
  {
    "url": "http://arxiv.org/abs/2401.13138",
    "title": "Visibility into AI Agents",
    "published_date": "2024-01-23",
    "abstract": "Increased delegation of commercial, scientific, governmental, and personal activities to AI agents—systems capable of pursuing complex goals with limited supervision—may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as visibility, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: agent identifiers, real-time monitoring, and activity logging. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for various actors in the supply chain including hardware and software service providers. Finally, we discuss the implications of our measures for privacy and concentration of power. Further work into understanding the measures and mitigating their negative impacts can help to build a foundation for the governance of AI agents.",
    "citation_count": 12,
    "summary": "This paper explores methods to increase transparency (\"visibility\") into the use of AI agents by examining agent identifiers, real-time monitoring, and activity logging, analyzing their implementation across various deployment contexts and considering privacy implications. The authors argue that increased visibility is crucial for mitigating societal risks associated with growing AI agent autonomy."
  },
  {
    "url": "https://arxiv.org/abs/2410.02769",
    "title": "Fundamentals of legislation for autonomous artificial intelligence systems",
    "published_date": "2024-09-14",
    "abstract": "The paper proposes a method for defining a dedicated operational context as part of the development and deployment of autonomous corporate governance systems. The case study of autonomous board of directors systems is examined. A significant part of the operational context for the autonomous corporate governance systems consists of the regulatory and legal framework that regulates the company's operations. A special operational context for autonomous artificial intelligence systems can be defined by simultaneously formulating local regulatory documents in two versions, i.e., to be used by people and by autonomous systems. In such a case, the artificial intelligence system receives a clearly defined operational context that allows such a system to perform its functions with a required operational quality. Local regulations that take into account the specificity of operations involving individuals and autonomous artificial intelligence systems can become the foundation of the relevant legislation that would regulate the development and deployment of autonomous systems.",
    "summary": "This paper suggests creating dual-version local regulations—one for humans, one for AI—to establish a clear operational context for autonomous corporate governance systems, arguing this approach forms a foundation for legislation governing AI development and deployment."
  },
  {
    "url": "https://arxiv.org/abs/2307.04699",
    "title": "International Institutions for Advanced AI",
    "published_date": "2023-07-10",
    "abstract": "International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.",
    "citation_count": 12,
    "summary": "This paper proposes four international institutional models—a commission, a governance organization, a collaborative, and a safety project—to manage the risks and benefits of advanced AI, addressing both its potential for global good and its potential dangers. These models aim to foster collaboration, establish safety standards, and ensure equitable access to AI technology."
  },
  {
    "url": "https://arxiv.org/pdf/2304.13081.pdf",
    "title": "Organizational Governance of Emerging Technologies: AI Adoption in Healthcare",
    "published_date": "2023-04-25",
    "abstract": "Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance (i.e. institutional governance) surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working with the consultancy IDEO.org, we were able to conduct usability-testing sessions with healthcare and AI ethics professionals. Usability analysis revealed a prototype structured around mock key decision points that align with how organizational leaders approach technology adoption. Concurrently, we conducted semi-structured interviews with 89 professionals in healthcare and other relevant fields. Using a modified grounded theory approach, we were able to identify 8 key decision points and comprehensive procedures throughout the AI adoption lifecycle. This is one of the most detailed qualitative analyses to date of the current governance structures and processes involved in AI adoption by health systems in the United States. We hope these findings can inform future efforts to build capabilities to promote the safe, effective, and responsible adoption of emerging technologies in healthcare.",
    "citation_count": 21,
    "summary": "This research investigates organizational governance of AI adoption in US healthcare, identifying eight key decision points throughout the AI lifecycle via qualitative analysis of interviews and usability testing with healthcare and AI professionals to inform safer and more effective AI implementation. The study aims to improve understanding and decision-making around AI adoption within healthcare systems."
  },
  {
    "url": "https://arxiv.org/pdf/2309.14876.pdf",
    "title": "APPRAISE: a governance framework for innovation with AI systems",
    "published_date": "2023-09-26",
    "abstract": "As artificial intelligence (AI) systems increasingly impact society, the EU Artificial Intelligence Act (AIA) is the first serious legislative attempt to contain the harmful effects of AI systems. This paper proposes a governance framework for AI innovation. The framework bridges the gap between strategic variables and responsible value creation, recommending audit as an enforcement mechanism. Strategic variables include, among others, organization size, exploration versus exploitation -, and build versus buy dilemmas. The proposed framework is based on primary and secondary research; the latter describes four pressures that organizations innovating with AI experience. Primary research includes an experimental setup, using which 34 organizations in the Netherlands are surveyed, followed up by 2 validation interviews. The survey measures the extent to which organizations coordinate technical elements of AI systems to ultimately comply with the AIA. The validation interviews generated additional in-depth insights and provided root causes. The moderating effect of the strategic variables is tested and found to be statistically significant for variables such as organization size. Relevant insights from primary and secondary research are eventually combined to propose the APPRAISE framework.",
    "summary": "The APPRAISE framework proposes a governance structure for AI innovation, bridging strategic organizational factors with responsible value creation and using audits for enforcement, informed by both a survey of 34 Dutch organizations and secondary research on pressures faced during AI innovation. The framework's effectiveness is statistically validated, particularly concerning organizational size."
  },
  {
    "url": "https://arxiv.org/pdf/2305.14865.pdf",
    "title": "A Game-Theoretic Framework for AI Governance",
    "published_date": "2023-05-24",
    "abstract": "As a transformative general-purpose technology, AI has empowered various industries and will continue to shape our lives through ubiquitous applications. Despite the enormous benefits from wide-spread AI deployment, it is crucial to address associated downside risks and therefore ensure AI advances are safe, fair, responsible, and aligned with human values. To do so, we need to establish effective AI governance. In this work, we show that the strategic interaction between the regulatory agencies and AI firms has an intrinsic structure reminiscent of a Stackelberg game, which motivates us to propose a game-theoretic modeling framework for AI governance. In particular, we formulate such interaction as a Stackelberg game composed of a leader and a follower, which captures the underlying game structure compared to its simultaneous play counterparts. Furthermore, the choice of the leader naturally gives rise to two settings. And we demonstrate that our proposed model can serves as a unified AI governance framework from two aspects: firstly we can map one setting to the AI governance of civil domains and the other to the safety-critical and military domains, secondly, the two settings of governance could be chosen contingent on the capability of the intelligent systems. To the best of our knowledge, this work is the first to use game theory for analyzing and structuring AI governance. We also discuss promising directions and hope this can help stimulate research interest in this interdisciplinary area. On a high, we hope this work would contribute to develop a new paradigm for technology policy: the quantitative and AI-driven methods for the technology policy field, which holds significant promise for overcoming many shortcomings of existing qualitative approaches.",
    "citation_count": 2,
    "summary": "This paper proposes a game-theoretic framework, specifically a Stackelberg game model, to analyze AI governance, considering the strategic interaction between regulatory agencies and AI firms. This framework offers a unified approach applicable to both civil and safety-critical AI domains, adaptable based on AI system capabilities."
  },
  {
    "url": "https://arxiv.org/pdf/2305.11528.pdf",
    "title": "The Global Governance of Artificial Intelligence: Next Steps for Empirical and Normative Research",
    "published_date": "2023-05-19",
    "abstract": "Artificial intelligence (AI) represents a technological upheaval with the potential to change human society. Because of its transformative potential, AI is increasingly becoming subject to regulatory initiatives at the global level. Yet, so far, scholarship in political science and international relations has focused more on AI applications than on the emerging architecture of global AI regulation. The purpose of this article is to outline an agenda for research into the global governance of AI. The article distinguishes between two broad perspectives: an empirical approach, aimed at mapping and explaining global AI governance; and a normative approach, aimed at developing and applying standards for appropriate global AI governance. The two approaches offer questions, concepts, and theories that are helpful in gaining an understanding of the emerging global governance of AI. Conversely, exploring AI as a regulatory issue offers a critical opportunity to refine existing general approaches to the study of global governance.",
    "citation_count": 25,
    "summary": "This article proposes a research agenda for understanding the global governance of artificial intelligence, advocating for both empirical mapping of existing regulatory initiatives and normative development of appropriate global standards. It argues that studying AI governance can refine existing theories of global governance while also informing the development of effective AI regulation."
  }
]