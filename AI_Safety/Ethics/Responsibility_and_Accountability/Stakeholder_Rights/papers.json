[
  {
    "url": "https://arxiv.org/abs/2004.11434",
    "title": "Responsible AI and Its Stakeholders",
    "published_date": "2020-04-23",
    "abstract": "Responsible Artificial Intelligence (AI) proposes a framework that holds all stakeholders involved in the development of AI to be responsible for their systems. It, however, fails to accommodate the possibility of holding AI responsible per se, which could close some legal and moral gaps concerning the deployment of autonomous and self-learning systems. We discuss three notions of responsibility (i.e., blameworthiness, accountability, and liability) for all stakeholders, including AI, and suggest the roles of jurisdiction and the general public in this matter.",
    "citation_count": 7,
    "summary": "The paper argues that while current Responsible AI frameworks focus on stakeholder responsibility, they neglect the potential for holding AI itself accountable; it proposes expanding responsibility frameworks to include AI alongside human stakeholders, considering blameworthiness, accountability, and liability."
  },
  {
    "url": "https://arxiv.org/abs/2408.12047",
    "title": "Do Responsible AI Artifacts Advance Stakeholder Goals? Four Key Barriers Perceived by Legal and Civil Stakeholders",
    "published_date": "2024-08-22",
    "abstract": "The responsible AI (RAI) community has introduced numerous processes and artifacts---such as Model Cards, Transparency Notes, and Data Cards---to facilitate transparency and support the governance of AI systems. While originally designed to scaffold and document AI development processes in technology companies, these artifacts are becoming central components of regulatory compliance under recent regulations such as the EU AI Act. Much of the existing literature has focussed primarily on the design of new RAI artifacts, or an examination of their use by practitioners within technology companies. However, as RAI artifacts begin to play key roles in enabling external oversight, it becomes critical to understand how stakeholders---particularly stakeholders situated outside of technology companies who govern and audit industry AI deployments---perceive the efficacy of RAI artifacts. In this study, we conduct semi-structured interviews and design activities with 19 government, legal, and civil society stakeholders who inform policy and advocacy around responsible AI efforts. While participants believe that RAI artifacts are a valuable contribution to the RAI ecosystem, many have concerns around their potential unintended and longer-term impacts on actors outside of technology companies (e.g., downstream end-users, policymakers, civil society stakeholders). We organized these beliefs into four barriers that help explain how RAI artifacts may (inadvertently) reconfigure power relations across civil society, government, and industry, impeding civil society and legal stakeholders' ability to protect downstream end-users from potential AI harms. Participants envision how structural changes, along with changes in how RAI artifacts are designed, used, and governed, could help re-direct the role and impacts of artifacts in the RAI ecosystem. Drawing on these findings, we discuss research and policy implications for RAI artifacts.",
    "citation_count": 1,
    "summary": "This study finds that while legal and civil stakeholders value responsible AI (RAI) artifacts, they also identify four key barriers to their effectiveness in advancing stakeholder goals, primarily concerning unintended consequences and the reconfiguration of power dynamics between industry and external oversight bodies. These barriers hinder the protection of end-users from potential AI harms."
  },
  {
    "url": "https://arxiv.org/abs/2411.15147",
    "title": "Delegating Responsibilities to Intelligent Autonomous Systems: Challenges and Benefits",
    "published_date": "2024-11-06",
    "abstract": "As AI systems increasingly operate with autonomy and adaptability, the traditional boundaries of moral responsibility in techno-social systems are being challenged. This paper explores the evolving discourse on the delegation of responsibilities to intelligent autonomous agents and the ethical implications of such practices. Synthesizing recent developments in AI ethics, including concepts of distributed responsibility and ethical AI by design, the paper proposes a functionalist perspective as a framework. This perspective views moral responsibility not as an individual trait but as a role within a socio-technical system, distributed among human and artificial agents. As an example of 'AI ethical by design,' we present Basti and Vitiello's implementation. They suggest that AI can act as artificial moral agents by learning ethical guidelines and using Deontic Higher-Order Logic to assess decisions ethically. Motivated by the possible speed and scale beyond human supervision and ethical implications, the paper argues for 'AI ethical by design', while acknowledging the distributed, shared, and dynamic nature of responsibility. This functionalist approach offers a practical framework for navigating the complexities of AI ethics in a rapidly evolving technological landscape.",
    "summary": "This paper examines the ethical implications of delegating responsibilities to increasingly autonomous AI systems, proposing a functionalist framework that distributes moral responsibility among human and artificial agents within a socio-technical system. It advocates for \"AI ethical by design,\" emphasizing proactive ethical considerations in AI development to address the challenges of speed and scale beyond human oversight."
  },
  {
    "url": "https://arxiv.org/abs/2404.16957",
    "title": "Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability",
    "published_date": "2024-04-25",
    "abstract": "The pervasive integration of Artificial Intelligence (AI) has introduced complex challenges in the responsibility and accountability in the event of incidents involving AI-enabled systems. The interconnectivity of these systems, ethical concerns of AI-induced incidents, coupled with uncertainties in AI technology and the absence of corresponding regulations, have made traditional responsibility attribution challenging. To this end, this work proposes a Computational Reflective Equilibrium (CRE) approach to establish a coherent and ethically acceptable responsibility attribution framework for all stakeholders. The computational approach provides a structured analysis that overcomes the limitations of conceptual approaches in dealing with dynamic and multifaceted scenarios, showcasing the framework's explainability, coherence, and adaptivity properties in the responsibility attribution process. We examine the pivotal role of the initial activation level associated with claims in equilibrium computation. Using an AI-assisted medical decision-support system as a case study, we illustrate how different initializations lead to diverse responsibility distributions. The framework offers valuable insights into accountability in AI-induced incidents, facilitating the development of a sustainable and resilient system through continuous monitoring, revision, and reflection.",
    "summary": "This paper proposes a Computational Reflective Equilibrium (CRE) framework for attributing responsibility in AI-induced incidents, addressing challenges posed by complex AI systems and the lack of clear regulations by offering a structured, explainable, and adaptable approach to accountability. The framework's effectiveness is demonstrated through a case study, highlighting the impact of initial assumptions on responsibility distribution."
  },
  {
    "url": "https://arxiv.org/pdf/2308.02608.pdf",
    "title": "Unravelling Responsibility for AI",
    "published_date": "2023-08-04",
    "abstract": "It is widely acknowledged that we need to establish where responsibility lies for the outputs and impacts of AI-enabled systems. But without a clear and precise understanding of what\"responsibility\"means, deliberations about where responsibility lies will be, at best, unfocused and incomplete and, at worst, misguided. To address this concern, this paper draws upon central distinctions in philosophy and law to clarify the concept of responsibility for AI for policymakers, practitioners, researchers and students from non-philosophical and non-legal backgrounds. Taking the three-part formulation\"Actor A is responsible for Occurrence O,\"the paper unravels the concept of responsibility to clarify that there are different possibilities of who is responsible for AI, the senses in which they are responsible, and aspects of events they are responsible for. Criteria and conditions for fitting attributions of responsibility in the core senses (causal responsibility, role-responsibility, liability responsibility and moral responsibility) are articulated to promote an understanding of when responsibility attributions would be inappropriate or unjust. The analysis is presented with a graphical notation to facilitate informal diagrammatic reasoning and discussion about specific cases. It is illustrated by application to a scenario of a fatal collision between an autonomous AI-enabled ship and a traditional, crewed vessel at sea.",
    "summary": "This paper clarifies the ambiguous concept of responsibility for AI outcomes by distinguishing between causal, role, liability, and moral responsibility, offering criteria for appropriate attribution and using a graphical notation to aid analysis of specific cases. Its aim is to provide policymakers, practitioners, and researchers with a framework for understanding and assigning responsibility in AI-related incidents."
  },
  {
    "url": "http://arxiv.org/abs/2401.09459",
    "title": "What's my role? Modelling responsibility for AI-based safety-critical systems",
    "published_date": "2023-12-30",
    "abstract": "AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in the real world. These can pose a risk of harm to people and the environment. Reducing that risk is an overarching priority during development and operation. As more AI-SCS become autonomous, a layer of risk management via human intervention has been removed. Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Many authors have commented on the\"responsibility gap\"where it is difficult for developers and manufacturers to be held responsible for harmful behaviour of an AI-SCS. This is due to the complex development cycle for AI, uncertainty in AI performance, and dynamic operating environment. A human operator can become a\"liability sink\"absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating, and may not have understanding of. This cross-disciplinary paper considers different senses of responsibility (role, moral, legal and causal), and how they apply in the context of AI-SCS safety. We use a core concept (Actor(A) is responsible for Occurrence(O)) to create role responsibility models, producing a practical method to capture responsibility relationships and provide clarity on the previously identified responsibility issues. Our paper demonstrates the approach with two examples: a retrospective analysis of the Tempe Arizona fatal collision involving an autonomous vehicle, and a safety focused predictive role-responsibility analysis for an AI-based diabetes co-morbidity predictor. In both examples our primary focus is on safety, aiming to reduce unfair or disproportionate blame being placed on operators or developers. We present a discussion and avenues for future research.",
    "summary": "This paper addresses the \"responsibility gap\" in AI-based safety-critical systems by developing role-responsibility models that clarify the contributions of various actors to system outcomes, aiming to improve accountability and prevent unfair blame allocation after incidents. The approach is demonstrated through case studies of autonomous vehicle accidents and AI-based medical prediction systems."
  },
  {
    "url": "https://www.lesswrong.com/posts/BCynDEwguEiogicAo/reflection-of-hierarchical-relationship-via-nuanced?commentId=LxbpnsnaqWD3xEwZc",
    "author": "Kyoung-cheol Kim",
    "title": "Reflection of Hierarchical Relationship via Nuanced Conditioning of Game Theory Approach for AI Development and Utilization - LessWrong",
    "published_date": "2023-02-07",
    "summary": "This article explores applying game theory to AI development within organizational structures, highlighting the limitations of a purely game-theoretic approach and emphasizing the enduring relevance of bureaucratic principles—hierarchical authority and specialization—even with the integration of AI agents. The authors argue that human-AI collaboration, leveraging comparative advantage, is crucial for efficient complex problem-solving, necessitating ongoing organizational structures."
  },
  {
    "url": "https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research",
    "author": "Roman Leventov",
    "title": "A multi-disciplinary view on AI safety research",
    "published_date": "2023-02-08",
    "summary": "The article advocates for a multidisciplinary approach to AI safety research, arguing that a solely technical focus is insufficient and that progress requires integrating diverse fields like neuroscience, sociology, and political science to design \"safe\" AGI systems within a framework of \"civilisational intelligence.\" This approach prioritizes pragmatism and collaboration over isolated theoretical development."
  },
  {
    "url": "https://arxiv.org/pdf/2106.08258v1.pdf",
    "title": "Identifying Roles, Requirements and Responsibilities in Trustworthy AI Systems",
    "published_date": "2021-06-15",
    "abstract": "Artificial Intelligence (AI) systems are being deployed around the globe in critical fields such as healthcare and education. In some cases, expert practitioners in these domains are being tasked with introducing or using such systems, but have little or no insight into what data these complex systems are based on, or how they are put together. In this paper, we consider an AI system from the domain practitioner's perspective and identify key roles that are involved in system deployment. We consider the differing requirements and responsibilities of each role, and identify tensions between transparency and confidentiality that need to be addressed so that domain practitioners are able to intelligently assess whether a particular AI system is appropriate for use in their domain.",
    "citation_count": 14,
    "summary": "This paper examines the roles, requirements, and responsibilities involved in deploying trustworthy AI systems, focusing on the perspective of domain practitioners and highlighting the tensions between transparency and confidentiality in assessing AI suitability. The authors identify key roles and their associated responsibilities to enable informed decision-making by practitioners."
  },
  {
    "title": "Towards Accountability in the Use of Artificial Intelligence for Public Administrations",
    "abstract": "We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared.",
    "published_date": "2021-05-04",
    "citation_count": 39,
    "url": "https://dl.acm.org/doi/10.1145/3461702.3462631",
    "summary": "The paper argues that using AI in public administration creates challenges to accountability due to distributed responsibility and lack of transparency, advocating for both public and auditor transparency to uphold democratic self-government. It analyzes existing AI guidelines, finding a need for clearer definitions of auditor entitlements and auditing goals to develop ethically sound standards."
  }
]