[
  {
    "url": "https://arxiv.org/pdf/1911.08005.pdf",
    "title": "The AI Liability Puzzle and a Fund-Based Work-Around",
    "published_date": "2019-11-18",
    "abstract": "Certainty around the regulatory environment is crucial to facilitate responsible AI innovation and its social acceptance. However, the existing legal liability system is inapt to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability of how courts would handle such cases makes the risks involved in the investment and use of AI incalculable, creating an environment that is not conducive to innovation and may deprive society of some benefits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory best-practices and establish a system of AI guarantee schemes. We envisage the system to form part of the broader market-structuring regulatory framework, with the primary function to provide a readily available, clear, and transparent funding mechanism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. We propose at least partial industry-funding, with funding arrangements depending on whether it would pursue other potential policy goals.",
    "citation_count": 12,
    "summary": "Current legal frameworks inadequately address liability for unforeseen harms caused by AI, hindering innovation; the authors propose a fund-based guarantee scheme, inspired by financial regulation, to compensate victims and mitigate this risk."
  },
  {
    "title": "The AI Liability Puzzle and a Fund-Based Work-Around",
    "abstract": "Certainty around the regulatory environment is crucial to facilitate responsible AI innovation and its social acceptance. However, the existing legal liability system is inapt to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability of how courts would handle such cases makes the risks involved in the investment and use of AI incalculable, creating an environment that is not conducive to innovation and may deprive society of some benefits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory best-practices and establish a system of AI guarantee schemes. We envisage the system to form part of the broader market-structuring regulatory framework, with the primary function to provide a readily available, clear, and transparent funding mechanism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. We propose at least partial industry-funding, with funding arrangements depending on whether it would pursue other potential policy goals.",
    "published_date": "2019-11-18",
    "citation_count": 12,
    "url": "https://dl.acm.org/doi/10.1145/3375627.3375806",
    "summary": "Current legal frameworks inadequately address AI liability for unforeseen harms, hindering innovation and investment; a proposed solution involves creating industry-funded guarantee schemes to compensate victims, mirroring financial regulatory best practices."
  },
  {
    "url": "https://www.alignmentforum.org/posts/X8NhKh2g2ECPrm5eo/post-series-on-liability-law-for-reducing-existential-risk",
    "author": "Nora_Ammann",
    "title": "Post series on \"Liability Law for reducing Existential Risk from AI\"",
    "published_date": "2024-02-29",
    "summary": "Gabriel Weil argues that current liability law inadequately addresses existential risks from AI due to its negligence requirement and limited punitive damages. He proposes implementing strict liability and expanding punitive damages, potentially through legislative action and insurance mandates, to better incentivize AI safety."
  },
  {
    "url": "https://arxiv.org/pdf/2308.02608.pdf",
    "title": "Unravelling Responsibility for AI",
    "published_date": "2023-08-04",
    "abstract": "It is widely acknowledged that we need to establish where responsibility lies for the outputs and impacts of AI-enabled systems. But without a clear and precise understanding of what\"responsibility\"means, deliberations about where responsibility lies will be, at best, unfocused and incomplete and, at worst, misguided. To address this concern, this paper draws upon central distinctions in philosophy and law to clarify the concept of responsibility for AI for policymakers, practitioners, researchers and students from non-philosophical and non-legal backgrounds. Taking the three-part formulation\"Actor A is responsible for Occurrence O,\"the paper unravels the concept of responsibility to clarify that there are different possibilities of who is responsible for AI, the senses in which they are responsible, and aspects of events they are responsible for. Criteria and conditions for fitting attributions of responsibility in the core senses (causal responsibility, role-responsibility, liability responsibility and moral responsibility) are articulated to promote an understanding of when responsibility attributions would be inappropriate or unjust. The analysis is presented with a graphical notation to facilitate informal diagrammatic reasoning and discussion about specific cases. It is illustrated by application to a scenario of a fatal collision between an autonomous AI-enabled ship and a traditional, crewed vessel at sea.",
    "summary": "This paper clarifies the multifaceted concept of responsibility in the context of AI systems, differentiating between causal, role, liability, and moral responsibility to provide a framework for determining accountability for AI-enabled actions and their consequences. The authors utilize philosophical and legal distinctions and a graphical notation to aid in assigning responsibility justly and avoiding inappropriate attributions."
  },
  {
    "url": "http://arxiv.org/abs/2401.09459",
    "title": "What's my role? Modelling responsibility for AI-based safety-critical systems",
    "published_date": "2023-12-30",
    "abstract": "AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in the real world. These can pose a risk of harm to people and the environment. Reducing that risk is an overarching priority during development and operation. As more AI-SCS become autonomous, a layer of risk management via human intervention has been removed. Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Many authors have commented on the\"responsibility gap\"where it is difficult for developers and manufacturers to be held responsible for harmful behaviour of an AI-SCS. This is due to the complex development cycle for AI, uncertainty in AI performance, and dynamic operating environment. A human operator can become a\"liability sink\"absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating, and may not have understanding of. This cross-disciplinary paper considers different senses of responsibility (role, moral, legal and causal), and how they apply in the context of AI-SCS safety. We use a core concept (Actor(A) is responsible for Occurrence(O)) to create role responsibility models, producing a practical method to capture responsibility relationships and provide clarity on the previously identified responsibility issues. Our paper demonstrates the approach with two examples: a retrospective analysis of the Tempe Arizona fatal collision involving an autonomous vehicle, and a safety focused predictive role-responsibility analysis for an AI-based diabetes co-morbidity predictor. In both examples our primary focus is on safety, aiming to reduce unfair or disproportionate blame being placed on operators or developers. We present a discussion and avenues for future research.",
    "summary": "This paper addresses the \"responsibility gap\" in AI-based safety-critical systems by proposing role-responsibility models that clarify causal relationships and allocate responsibility among actors involved in development and operation, aiming to prevent unfair blame assignment following accidents. The approach is demonstrated through case studies, analyzing responsibility in both retrospective and predictive contexts."
  },
  {
    "url": "https://arxiv.org/pdf/2004.11434v1.pdf",
    "title": "Responsible AI and Its Stakeholders",
    "published_date": "2020-04-23",
    "abstract": "Responsible Artificial Intelligence (AI) proposes a framework that holds all stakeholders involved in the development of AI to be responsible for their systems. It, however, fails to accommodate the possibility of holding AI responsible per se, which could close some legal and moral gaps concerning the deployment of autonomous and self-learning systems. We discuss three notions of responsibility (i.e., blameworthiness, accountability, and liability) for all stakeholders, including AI, and suggest the roles of jurisdiction and the general public in this matter.",
    "citation_count": 7,
    "summary": "The paper argues that while existing Responsible AI frameworks focus on stakeholder responsibility, they neglect the potential for assigning responsibility directly to AI systems themselves. It proposes incorporating AI's responsibility alongside that of stakeholders, considering blameworthiness, accountability, and liability within a broader legal and societal context."
  },
  {
    "url": "https://arxiv.org/abs/2411.15147",
    "title": "Delegating Responsibilities to Intelligent Autonomous Systems: Challenges and Benefits",
    "published_date": "2024-11-06",
    "abstract": "As AI systems increasingly operate with autonomy and adaptability, the traditional boundaries of moral responsibility in techno-social systems are being challenged. This paper explores the evolving discourse on the delegation of responsibilities to intelligent autonomous agents and the ethical implications of such practices. Synthesizing recent developments in AI ethics, including concepts of distributed responsibility and ethical AI by design, the paper proposes a functionalist perspective as a framework. This perspective views moral responsibility not as an individual trait but as a role within a socio-technical system, distributed among human and artificial agents. As an example of 'AI ethical by design,' we present Basti and Vitiello's implementation. They suggest that AI can act as artificial moral agents by learning ethical guidelines and using Deontic Higher-Order Logic to assess decisions ethically. Motivated by the possible speed and scale beyond human supervision and ethical implications, the paper argues for 'AI ethical by design', while acknowledging the distributed, shared, and dynamic nature of responsibility. This functionalist approach offers a practical framework for navigating the complexities of AI ethics in a rapidly evolving technological landscape.",
    "summary": "This paper examines the ethical implications of delegating responsibilities to increasingly autonomous AI systems, proposing a functionalist framework that distributes moral responsibility across human and artificial agents within a socio-technical system. It advocates for \"AI ethical by design,\" emphasizing proactive ethical considerations in AI development to address the challenges of speed and scale beyond human oversight."
  },
  {
    "url": "https://arxiv.org/abs/2404.16957",
    "title": "Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability",
    "published_date": "2024-04-25",
    "abstract": "The pervasive integration of Artificial Intelligence (AI) has introduced complex challenges in the responsibility and accountability in the event of incidents involving AI-enabled systems. The interconnectivity of these systems, ethical concerns of AI-induced incidents, coupled with uncertainties in AI technology and the absence of corresponding regulations, have made traditional responsibility attribution challenging. To this end, this work proposes a Computational Reflective Equilibrium (CRE) approach to establish a coherent and ethically acceptable responsibility attribution framework for all stakeholders. The computational approach provides a structured analysis that overcomes the limitations of conceptual approaches in dealing with dynamic and multifaceted scenarios, showcasing the framework's explainability, coherence, and adaptivity properties in the responsibility attribution process. We examine the pivotal role of the initial activation level associated with claims in equilibrium computation. Using an AI-assisted medical decision-support system as a case study, we illustrate how different initializations lead to diverse responsibility distributions. The framework offers valuable insights into accountability in AI-induced incidents, facilitating the development of a sustainable and resilient system through continuous monitoring, revision, and reflection.",
    "summary": "This paper proposes a Computational Reflective Equilibrium (CRE) framework for attributing responsibility in AI-induced incidents, addressing the complexities of interconnected AI systems and ethical concerns through a structured, explainable, and adaptable computational approach. A case study demonstrates how different initial conditions within the CRE framework can yield varying responsibility distributions among stakeholders."
  },
  {
    "url": "https://arxiv.org/abs/2412.17114",
    "title": "Decentralized Governance of Autonomous AI Agents",
    "published_date": "2024-12-22",
    "abstract": "Autonomous AI agents present transformative opportunities and significant governance challenges. Existing frameworks, such as the EU AI Act and the NIST AI Risk Management Framework, fall short of addressing the complexities of these agents, which are capable of independent decision-making, learning, and adaptation. To bridge these gaps, we propose the ETHOS (Ethical Technology and Holistic Oversight System) framework, a decentralized governance (DeGov) model leveraging Web3 technologies, including blockchain, smart contracts, and decentralized autonomous organizations (DAOs). ETHOS establishes a global registry for AI agents, enabling dynamic risk classification, proportional oversight, and automated compliance monitoring through tools like soulbound tokens and zero-knowledge proofs. Furthermore, the framework incorporates decentralized justice systems for transparent dispute resolution and introduces AI specific legal entities to manage limited liability, supported by mandatory insurance to ensure financial accountability and incentivize ethical design. By integrating philosophical principles of rationality, ethical grounding, and goal alignment, ETHOS aims to create a robust research agenda for promoting trust, transparency, and participatory governance. This innovative framework offers a scalable and inclusive strategy for regulating AI agents, balancing innovation with ethical responsibility to meet the demands of an AI-driven future.",
    "summary": "The ETHOS framework proposes a decentralized governance model for autonomous AI agents using Web3 technologies to establish a global registry, dynamic risk classification, and automated compliance, addressing limitations of existing regulatory frameworks. It incorporates decentralized justice and AI-specific legal entities to ensure accountability and incentivize ethical AI development."
  },
  {
    "url": "https://www.lesswrong.com/posts/gZBgmDFqqyw3Lghok/ai-regulatory-landscape-review-incident-reporting",
    "author": "Deric Cheng, Elliot_Mckernon",
    "title": "AI Incident Reporting: A Regulatory Review",
    "published_date": "2024-03-11",
    "summary": "This article begins a series analyzing the emerging global AI regulatory landscape, focusing on the US, EU, and China. The first installment examines AI incident reporting, exploring its rationale, existing examples from other industries (aviation, workplace safety), and current regulatory developments, particularly in China's proposed cybersecurity incident reporting measures."
  }
]