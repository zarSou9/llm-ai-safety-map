[
  {
    "url": "https://arxiv.org/pdf/2308.02608.pdf",
    "title": "Unravelling Responsibility for AI",
    "published_date": "2023-08-04",
    "abstract": "It is widely acknowledged that we need to establish where responsibility lies for the outputs and impacts of AI-enabled systems. But without a clear and precise understanding of what\"responsibility\"means, deliberations about where responsibility lies will be, at best, unfocused and incomplete and, at worst, misguided. To address this concern, this paper draws upon central distinctions in philosophy and law to clarify the concept of responsibility for AI for policymakers, practitioners, researchers and students from non-philosophical and non-legal backgrounds. Taking the three-part formulation\"Actor A is responsible for Occurrence O,\"the paper unravels the concept of responsibility to clarify that there are different possibilities of who is responsible for AI, the senses in which they are responsible, and aspects of events they are responsible for. Criteria and conditions for fitting attributions of responsibility in the core senses (causal responsibility, role-responsibility, liability responsibility and moral responsibility) are articulated to promote an understanding of when responsibility attributions would be inappropriate or unjust. The analysis is presented with a graphical notation to facilitate informal diagrammatic reasoning and discussion about specific cases. It is illustrated by application to a scenario of a fatal collision between an autonomous AI-enabled ship and a traditional, crewed vessel at sea.",
    "summary": "This paper clarifies the ambiguous concept of responsibility in the context of AI systems by distinguishing between causal, role, liability, and moral responsibility. It uses a three-part formulation (\"Actor A is responsible for Occurrence O\") and a graphical notation to analyze responsibility attribution in AI, illustrated by a case study of a maritime accident."
  },
  {
    "url": "https://arxiv.org/abs/2411.15147",
    "title": "Delegating Responsibilities to Intelligent Autonomous Systems: Challenges and Benefits",
    "published_date": "2024-11-06",
    "abstract": "As AI systems increasingly operate with autonomy and adaptability, the traditional boundaries of moral responsibility in techno-social systems are being challenged. This paper explores the evolving discourse on the delegation of responsibilities to intelligent autonomous agents and the ethical implications of such practices. Synthesizing recent developments in AI ethics, including concepts of distributed responsibility and ethical AI by design, the paper proposes a functionalist perspective as a framework. This perspective views moral responsibility not as an individual trait but as a role within a socio-technical system, distributed among human and artificial agents. As an example of 'AI ethical by design,' we present Basti and Vitiello's implementation. They suggest that AI can act as artificial moral agents by learning ethical guidelines and using Deontic Higher-Order Logic to assess decisions ethically. Motivated by the possible speed and scale beyond human supervision and ethical implications, the paper argues for 'AI ethical by design', while acknowledging the distributed, shared, and dynamic nature of responsibility. This functionalist approach offers a practical framework for navigating the complexities of AI ethics in a rapidly evolving technological landscape.",
    "summary": "This paper examines the ethical implications of delegating responsibilities to increasingly autonomous AI systems, proposing a functionalist framework that distributes moral responsibility across human and artificial agents within a socio-technical system. It advocates for \"AI ethical by design,\" using Deontic Higher-Order Logic as a potential mechanism for ethically informed AI decision-making."
  },
  {
    "url": "https://arxiv.org/abs/2203.04754v1",
    "title": "System Cards for AI-Based Decision-Making for Public Policy",
    "published_date": "2022-03-01",
    "abstract": "Decisions impacting human lives are increasingly being made or assisted by automated decision-making algorithms. Many of these algorithms process personal data for predicting recidivism, credit risk analysis, identifying individuals using face recognition, and more. While potentially improving efficiency and effectiveness, such algorithms are not inherently free from bias, opaqueness, lack of explainability, maleficence, and the like. Given that the outcomes of these algorithms have a significant impact on individuals and society and are open to analysis and contestation after deployment, such issues must be accounted for before deployment. Formal audits are a way of ensuring algorithms meet the appropriate accountability standards. This work, based on an extensive analysis of the literature and an expert focus group study, proposes a unifying framework for a system accountability benchmark for formal audits of artificial intelligence-based decision-aiding systems. This work also proposes system cards to serve as scorecards presenting the outcomes of such audits. It consists of 56 criteria organized within a four-by-four matrix composed of rows focused on (i) data, (ii) model, (iii) code, (iv) system, and columns focused on (a) development, (b) assessment, (c) mitigation, and (d) assurance. The proposed system accountability benchmark reflects the state-of-the-art developments for accountable systems, serves as a checklist for algorithm audits, and paves the way for sequential work in future research.",
    "citation_count": 12,
    "summary": "This paper proposes a system accountability benchmark and accompanying \"system cards\" for auditing AI-based public policy decision-making systems, using a 56-criteria framework to assess data, model, code, and system aspects across development, assessment, mitigation, and assurance stages. The framework aims to improve transparency and accountability in AI systems impacting human lives."
  },
  {
    "url": "https://arxiv.org/pdf/2308.01752.pdf",
    "title": "Quantifying Retrospective Human Responsibility in Intelligent Systems",
    "published_date": "2023-08-03",
    "abstract": "Intelligent systems have become a major part of our lives. Human responsibility for outcomes becomes unclear in the interaction with these systems, as parts of information acquisition, decision-making, and action implementation may be carried out jointly by humans and systems. Determining human causal responsibility with intelligent systems is particularly important in events that end with adverse outcomes. We developed three measures of retrospective human causal responsibility when using intelligent systems. The first measure concerns repetitive human interactions with a system. Using information theory, it quantifies the average human's unique contribution to the outcomes of past events. The second and third measures concern human causal responsibility in a single past interaction with an intelligent system. They quantify, respectively, the unique human contribution in forming the information used for decision-making and the reasonability of the actions that the human carried out. The results show that human retrospective responsibility depends on the combined effects of system design and its reliability, the human's role and authority, and probabilistic factors related to the system and the environment. The new responsibility measures can serve to investigate and analyze past events involving intelligent systems. They may aid the judgment of human responsibility and ethical and legal discussions, providing a novel quantitative perspective.",
    "summary": "This paper proposes three quantitative measures for assessing human causal responsibility in adverse events involving intelligent systems, considering factors like system design, human role, and probabilistic influences. These measures aim to improve analysis of past events and inform ethical and legal discussions surrounding human accountability in human-AI interactions."
  },
  {
    "url": "https://arxiv.org/abs/2404.14068",
    "title": "Holistic Safety and Responsibility Evaluations of Advanced AI Models",
    "published_date": "2024-04-22",
    "abstract": "Safety and responsibility evaluations of advanced AI models are a critical but developing field of research and practice. In the development of Google DeepMind's advanced AI models, we innovated on and applied a broad set of approaches to safety evaluation. In this report, we summarise and share elements of our evolving approach as well as lessons learned for a broad audience. Key lessons learned include: First, theoretical underpinnings and frameworks are invaluable to organise the breadth of risk domains, modalities, forms, metrics, and goals. Second, theory and practice of safety evaluation development each benefit from collaboration to clarify goals, methods and challenges, and facilitate the transfer of insights between different stakeholders and disciplines. Third, similar key methods, lessons, and institutions apply across the range of concerns in responsibility and safety - including established and emerging harms. For this reason it is important that a wide range of actors working on safety evaluation and safety research communities work together to develop, refine and implement novel evaluation approaches and best practices, rather than operating in silos. The report concludes with outlining the clear need to rapidly advance the science of evaluations, to integrate new evaluations into the development and governance of AI, to establish scientifically-grounded norms and standards, and to promote a robust evaluation ecosystem.",
    "citation_count": 4,
    "summary": "This report details Google DeepMind's holistic approach to evaluating the safety and responsibility of advanced AI models, emphasizing the importance of theoretical frameworks, interdisciplinary collaboration, and the urgent need for advancing the science of AI safety evaluations and establishing industry standards."
  },
  {
    "url": "https://arxiv.org/abs/2410.17229",
    "title": "Responsibility in a Multi-Value Strategic Setting",
    "published_date": "2024-10-22",
    "abstract": "Responsibility is a key notion in multi-agent systems and in creating safe, reliable and ethical AI. However, most previous work on responsibility has only considered responsibility for single outcomes. In this paper we present a model for responsibility attribution in a multi-agent, multi-value setting. We also expand our model to cover responsibility anticipation, demonstrating how considerations of responsibility can help an agent to select strategies that are in line with its values. In particular we show that non-dominated regret-minimising strategies reliably minimise an agent's expected degree of responsibility.",
    "summary": "This paper introduces a model for assigning responsibility in multi-agent systems with multiple, potentially conflicting values, extending beyond single-outcome analyses. It further demonstrates how anticipating responsibility can guide agents toward strategy selection aligned with their values, showing that regret-minimizing strategies effectively minimize expected responsibility."
  },
  {
    "url": "https://www.lesswrong.com/tag/machine-ethics",
    "title": "Machine Ethics - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Machine ethics aims to create artificial moral agents (AMAs) capable of morally sound actions, a challenge highlighted by the limitations of past attempts like Asimov's Three Laws of Robotics. The field's current focus on simpler AI systems underscores the urgency of developing robust machine ethics before the advent of artificial general intelligence."
  },
  {
    "url": "https://www.lesswrong.com/posts/324pQjqoHEHeF2vPs/ai-clarity-an-initial-research-agenda",
    "author": "Justin Bullock, Corin Katzke, Zershaaneh Qureshi, David_Kristoffersson",
    "title": "AI Clarity: An Initial Research Agenda",
    "published_date": "2024-05-03",
    "summary": "The AI Clarity research program uses scenario planning to explore potential trajectories of transformative AI (TAI), focusing on the possibility of TAI emerging within the next decade. The program aims to identify existential hazards, evaluate AI safety and governance strategies across various scenarios, and recommend strategies to mitigate existential risks."
  },
  {
    "url": "http://arxiv.org/abs/2401.09459",
    "title": "What's my role? Modelling responsibility for AI-based safety-critical systems",
    "published_date": "2023-12-30",
    "abstract": "AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in the real world. These can pose a risk of harm to people and the environment. Reducing that risk is an overarching priority during development and operation. As more AI-SCS become autonomous, a layer of risk management via human intervention has been removed. Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Many authors have commented on the\"responsibility gap\"where it is difficult for developers and manufacturers to be held responsible for harmful behaviour of an AI-SCS. This is due to the complex development cycle for AI, uncertainty in AI performance, and dynamic operating environment. A human operator can become a\"liability sink\"absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating, and may not have understanding of. This cross-disciplinary paper considers different senses of responsibility (role, moral, legal and causal), and how they apply in the context of AI-SCS safety. We use a core concept (Actor(A) is responsible for Occurrence(O)) to create role responsibility models, producing a practical method to capture responsibility relationships and provide clarity on the previously identified responsibility issues. Our paper demonstrates the approach with two examples: a retrospective analysis of the Tempe Arizona fatal collision involving an autonomous vehicle, and a safety focused predictive role-responsibility analysis for an AI-based diabetes co-morbidity predictor. In both examples our primary focus is on safety, aiming to reduce unfair or disproportionate blame being placed on operators or developers. We present a discussion and avenues for future research.",
    "summary": "This paper addresses the \"responsibility gap\" in AI-based safety-critical systems by developing role responsibility models that clarify the causal and moral responsibilities of actors involved in accidents. These models aim to reduce unfair blame and improve safety by analyzing responsibility relationships across the development and operation lifecycle."
  },
  {
    "url": "https://arxiv.org/abs/2302.08157",
    "title": "Human-Centered Responsible Artificial Intelligence: Current & Future Trends",
    "published_date": "2023-02-16",
    "abstract": "In recent years, the CHI community has seen significant growth in research on Human-Centered Responsible Artificial Intelligence. While different research communities may use different terminology to discuss similar topics, all of this work is ultimately aimed at developing AI that benefits humanity while being grounded in human rights and ethics, and reducing the potential harms of AI. In this special interest group, we aim to bring together researchers from academia and industry interested in these topics to map current and future research trends to advance this important area of research by fostering collaboration and sharing ideas.",
    "citation_count": 29,
    "summary": "This paper examines the burgeoning field of Human-Centered Responsible Artificial Intelligence within the CHI community, focusing on aligning AI development with human benefit, ethical considerations, and the mitigation of potential harms. The paper aims to foster collaboration and identify key research trends for the future of this field."
  },
  {
    "url": "https://www.alignmentforum.org/tag/ai",
    "author": "Evan Hubinger",
    "title": "AI - AI Alignment Forum",
    "published_date": "2023-02-06",
    "summary": "Artificial intelligence alignment focuses on ensuring powerful AI systems act according to human values, preventing unintended consequences and existential risks. This involves various approaches, from narrow goals like curing disease to broader ambitions like creating a beneficial future for humanity, all while addressing the potential for AI systems to pursue unintended objectives."
  },
  {
    "url": "https://www.lesswrong.com/posts/nnGwHuJfCBxKDgsds/embedding-ethical-priors-into-ai-systems-a-bayesian-approach",
    "author": "Justausername",
    "title": "Embedding Ethical Priors into AI Systems: A Bayesian Approach",
    "published_date": "2023-08-03",
    "summary": "This paper proposes a novel framework for building ethical AI by incorporating ethical principles as Bayesian priors into AI learning processes. This approach, inspired by human moral intuitions, aims to guide AI decision-making towards ethically sound judgments."
  },
  {
    "url": "https://arxiv.org/pdf/2102.00625v1.pdf",
    "title": "Human Perceptions on Moral Responsibility of AI: A Case Study in AI-Assisted Bail Decision-Making",
    "published_date": "2021-02-01",
    "abstract": "How to attribute responsibility for autonomous artificial intelligence (AI) systems' actions has been widely debated across the humanities and social science disciplines. This work presents two experiments (N=200 each) that measure people's perceptions of eight different notions of moral responsibility concerning AI and human agents in the context of bail decision-making. Using real-life adapted vignettes, our experiments show that AI agents are held causally responsible and blamed similarly to human agents for an identical task. However, there was a meaningful difference in how people perceived these agents' moral responsibility; human agents were ascribed to a higher degree of present-looking and forward-looking notions of responsibility than AI agents. We also found that people expect both AI and human decision-makers and advisors to justify their decisions regardless of their nature. We discuss policy and HCI implications of these findings, such as the need for explainable AI in high-stakes scenarios.",
    "citation_count": 60,
    "summary": "Two experiments (N=400) revealed that while participants held AI and human agents similarly causally responsible for AI-assisted bail decisions, they attributed greater present and future-oriented moral responsibility to human agents. This highlights the need for explainable AI, especially in high-stakes scenarios."
  },
  {
    "url": "https://arxiv.org/abs/2110.14419v2",
    "title": "Toward a Theory of Justice for Artificial Intelligence",
    "published_date": "2021-10-27",
    "abstract": "Abstract This essay explores the relationship between artificial intelligence and principles of distributive justice. Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of sociotechnical systems, and that the operation of these systems is increasingly shaped and influenced by AI. Consequently, egalitarian norms of justice apply to the technology when it is deployed in these contexts. These norms entail that the relevant AI systems must meet a certain standard of public justification, support citizens' rights, and promote substantively fair outcomes, something that requires particular attention to the impact they have on the worst-off members of society.",
    "citation_count": 40,
    "summary": "This essay argues that principles of distributive justice, particularly Rawls's theory, should apply to artificial intelligence systems as they increasingly shape societal structures. This necessitates AI systems meeting standards of public justification, upholding citizens' rights, and promoting equitable outcomes, especially for the most disadvantaged."
  },
  {
    "url": "https://www.alignmentforum.org/tag/ethics-and-morality",
    "author": "Wei Dai",
    "title": "Ethics & Morality - AI Alignment Forum",
    "published_date": "2021-12-02",
    "summary": "The article is a collection of discussion points and links related to ethics and morality, covering various perspectives and philosophical approaches. It provides links to further reading on consequentialism, deontology, metaethics, and moral uncertainty."
  }
]