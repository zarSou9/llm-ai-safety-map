[
  {
    "url": "https://arxiv.org/abs/2404.16957",
    "title": "Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability",
    "published_date": "2024-04-25",
    "abstract": "The pervasive integration of Artificial Intelligence (AI) has introduced complex challenges in the responsibility and accountability in the event of incidents involving AI-enabled systems. The interconnectivity of these systems, ethical concerns of AI-induced incidents, coupled with uncertainties in AI technology and the absence of corresponding regulations, have made traditional responsibility attribution challenging. To this end, this work proposes a Computational Reflective Equilibrium (CRE) approach to establish a coherent and ethically acceptable responsibility attribution framework for all stakeholders. The computational approach provides a structured analysis that overcomes the limitations of conceptual approaches in dealing with dynamic and multifaceted scenarios, showcasing the framework's explainability, coherence, and adaptivity properties in the responsibility attribution process. We examine the pivotal role of the initial activation level associated with claims in equilibrium computation. Using an AI-assisted medical decision-support system as a case study, we illustrate how different initializations lead to diverse responsibility distributions. The framework offers valuable insights into accountability in AI-induced incidents, facilitating the development of a sustainable and resilient system through continuous monitoring, revision, and reflection.",
    "summary": "This paper proposes a Computational Reflective Equilibrium (CRE) framework for attributing responsibility in AI-induced incidents, addressing the challenges posed by complex AI systems and the lack of clear regulations through a structured, explainable, and adaptable computational approach. A case study demonstrates how different initial conditions within the CRE framework lead to varying responsibility distributions among stakeholders."
  },
  {
    "url": "http://arxiv.org/abs/2401.09459",
    "title": "What's my role? Modelling responsibility for AI-based safety-critical systems",
    "published_date": "2023-12-30",
    "abstract": "AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in the real world. These can pose a risk of harm to people and the environment. Reducing that risk is an overarching priority during development and operation. As more AI-SCS become autonomous, a layer of risk management via human intervention has been removed. Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Many authors have commented on the\"responsibility gap\"where it is difficult for developers and manufacturers to be held responsible for harmful behaviour of an AI-SCS. This is due to the complex development cycle for AI, uncertainty in AI performance, and dynamic operating environment. A human operator can become a\"liability sink\"absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating, and may not have understanding of. This cross-disciplinary paper considers different senses of responsibility (role, moral, legal and causal), and how they apply in the context of AI-SCS safety. We use a core concept (Actor(A) is responsible for Occurrence(O)) to create role responsibility models, producing a practical method to capture responsibility relationships and provide clarity on the previously identified responsibility issues. Our paper demonstrates the approach with two examples: a retrospective analysis of the Tempe Arizona fatal collision involving an autonomous vehicle, and a safety focused predictive role-responsibility analysis for an AI-based diabetes co-morbidity predictor. In both examples our primary focus is on safety, aiming to reduce unfair or disproportionate blame being placed on operators or developers. We present a discussion and avenues for future research.",
    "summary": "This paper addresses the challenge of assigning responsibility in AI-based safety-critical systems, proposing role-responsibility models to clarify accountability across developers, operators, and AI systems, thereby mitigating unfair blame and improving safety. The approach is demonstrated through analyses of real-world incidents involving autonomous vehicles and AI-driven medical prediction."
  },
  {
    "url": "https://arxiv.org/pdf/2308.02608.pdf",
    "title": "Unravelling Responsibility for AI",
    "published_date": "2023-08-04",
    "abstract": "It is widely acknowledged that we need to establish where responsibility lies for the outputs and impacts of AI-enabled systems. But without a clear and precise understanding of what\"responsibility\"means, deliberations about where responsibility lies will be, at best, unfocused and incomplete and, at worst, misguided. To address this concern, this paper draws upon central distinctions in philosophy and law to clarify the concept of responsibility for AI for policymakers, practitioners, researchers and students from non-philosophical and non-legal backgrounds. Taking the three-part formulation\"Actor A is responsible for Occurrence O,\"the paper unravels the concept of responsibility to clarify that there are different possibilities of who is responsible for AI, the senses in which they are responsible, and aspects of events they are responsible for. Criteria and conditions for fitting attributions of responsibility in the core senses (causal responsibility, role-responsibility, liability responsibility and moral responsibility) are articulated to promote an understanding of when responsibility attributions would be inappropriate or unjust. The analysis is presented with a graphical notation to facilitate informal diagrammatic reasoning and discussion about specific cases. It is illustrated by application to a scenario of a fatal collision between an autonomous AI-enabled ship and a traditional, crewed vessel at sea.",
    "summary": "This paper clarifies the ambiguous concept of responsibility in the context of AI systems by differentiating between causal, role, liability, and moral responsibility. It uses a three-part formulation and a graphical notation to analyze who is responsible for AI actions and under what conditions, illustrated with a case study of a maritime accident."
  },
  {
    "title": "Attributions of ethical responsibility by Artificial Intelligence practitioners",
    "abstract": "ABSTRACT Systems based on Artificial Intelligence (AI) are increasingly normalized as part of work, leisure, and governance in contemporary societies. Although ethics in AI has received significant attention, it remains unclear where the burden of responsibility lies. Through twenty-one interviews with AI practitioners in Australia, this research seeks to understand how ethical attributions figure into the professional imagination. As institutionally embedded technical experts, AI practitioners act as a connective tissue linking the range of actors that come in contact with, and have effects upon, AI products and services. Findings highlight that practitioners distribute ethical responsibility across a range of actors and factors, reserving a portion of responsibility for themselves, albeit constrained. Characterized by imbalances of decision-making power and technical expertise, practitioners position themselves as mediators between powerful bodies that set parameters for production; users who engage with products once they leave the proverbial workbench; and AI systems that evolve and develop beyond practitioner control. Distributing responsibility throughout complex sociotechnical networks, practitioners preclude simple attributions of accountability for the social effects of AI. This indicates that AI ethics are not the purview of any singular player but instead, derive from collectivities that require critical guidance and oversight at all stages of conception, production, distribution, and use.",
    "published_date": "2020-01-26",
    "citation_count": 72,
    "url": "https://www.tandfonline.com/doi/full/10.1080/1369118X.2020.1713842",
    "summary": "AI practitioners interviewed in this study distribute ethical responsibility for AI systems across multiple actors—including developers, users, and the AI itself—highlighting the complex sociotechnical networks involved and rejecting simplistic accountability frameworks. This distributed responsibility necessitates collective oversight throughout the AI lifecycle."
  },
  {
    "url": "https://arxiv.org/abs/2411.15147",
    "title": "Delegating Responsibilities to Intelligent Autonomous Systems: Challenges and Benefits",
    "published_date": "2024-11-06",
    "abstract": "As AI systems increasingly operate with autonomy and adaptability, the traditional boundaries of moral responsibility in techno-social systems are being challenged. This paper explores the evolving discourse on the delegation of responsibilities to intelligent autonomous agents and the ethical implications of such practices. Synthesizing recent developments in AI ethics, including concepts of distributed responsibility and ethical AI by design, the paper proposes a functionalist perspective as a framework. This perspective views moral responsibility not as an individual trait but as a role within a socio-technical system, distributed among human and artificial agents. As an example of 'AI ethical by design,' we present Basti and Vitiello's implementation. They suggest that AI can act as artificial moral agents by learning ethical guidelines and using Deontic Higher-Order Logic to assess decisions ethically. Motivated by the possible speed and scale beyond human supervision and ethical implications, the paper argues for 'AI ethical by design', while acknowledging the distributed, shared, and dynamic nature of responsibility. This functionalist approach offers a practical framework for navigating the complexities of AI ethics in a rapidly evolving technological landscape.",
    "summary": "This paper examines the ethical challenges of delegating responsibilities to increasingly autonomous AI systems, proposing a functionalist framework that distributes moral responsibility among human and artificial agents within a socio-technical system. It advocates for \"AI ethical by design,\" using techniques like Deontic Higher-Order Logic to ensure ethical AI decision-making."
  },
  {
    "url": "https://arxiv.org/abs/2404.14068",
    "title": "Holistic Safety and Responsibility Evaluations of Advanced AI Models",
    "published_date": "2024-04-22",
    "abstract": "Safety and responsibility evaluations of advanced AI models are a critical but developing field of research and practice. In the development of Google DeepMind's advanced AI models, we innovated on and applied a broad set of approaches to safety evaluation. In this report, we summarise and share elements of our evolving approach as well as lessons learned for a broad audience. Key lessons learned include: First, theoretical underpinnings and frameworks are invaluable to organise the breadth of risk domains, modalities, forms, metrics, and goals. Second, theory and practice of safety evaluation development each benefit from collaboration to clarify goals, methods and challenges, and facilitate the transfer of insights between different stakeholders and disciplines. Third, similar key methods, lessons, and institutions apply across the range of concerns in responsibility and safety - including established and emerging harms. For this reason it is important that a wide range of actors working on safety evaluation and safety research communities work together to develop, refine and implement novel evaluation approaches and best practices, rather than operating in silos. The report concludes with outlining the clear need to rapidly advance the science of evaluations, to integrate new evaluations into the development and governance of AI, to establish scientifically-grounded norms and standards, and to promote a robust evaluation ecosystem.",
    "citation_count": 4,
    "summary": "Google DeepMind's report details their holistic approach to AI safety and responsibility evaluations, emphasizing the importance of theoretical frameworks, interdisciplinary collaboration, and the urgent need for advancing the science and establishing robust evaluation standards for advanced AI models."
  },
  {
    "url": "https://www.alignmentforum.org/tag/copenhagen-interpretation-of-ethics",
    "title": "Copenhagen Interpretation of Ethics - AI Alignment Forum",
    "published_date": "2024-02-01",
    "summary": "The Copenhagen Interpretation of Ethics posits that any interaction with a problem, regardless of intent or outcome, results in moral responsibility for that problem. It assigns blame based on engagement, not necessarily causation."
  },
  {
    "url": "https://www.lesswrong.com/posts/mXD53GFvMCWQhcCwt/distinguishing-ways-ai-can-be-concentrated",
    "author": "Matthew Barnett",
    "title": "Distinguishing ways AI can be \"concentrated\"",
    "published_date": "2024-10-21",
    "summary": "The article argues that discussions of AI concentration need greater clarity, proposing three distinct dimensions: concentration of AI development, service provisioning, and control over AI services. These dimensions are independent and should be analyzed separately to accurately assess the risks and potential trajectories of AI development."
  },
  {
    "url": "https://www.lesswrong.com/posts/ECnLBSxw4TvpWPnae/ai-model-registries-a-regulatory-review",
    "author": "Deric Cheng, Elliot_Mckernon",
    "title": "AI Model Registries: A Regulatory Review",
    "published_date": "2024-03-22",
    "summary": "This article, part of a series reviewing the 2024 AI regulatory landscape, examines AI model registries—centralized databases tracking AI systems for governmental monitoring and future legislation. These registries, drawing parallels from pharmaceutical regulations, vary widely in requirements and accessibility across jurisdictions like the US, EU, and China."
  },
  {
    "url": "https://www.lesswrong.com/posts/8xN5KYB9xAgSSi494/against-the-open-source-closed-source-dichotomy-regulated",
    "author": "alex.herwix",
    "title": "Against the Open Source / Closed Source Dichotomy: Regulated Source as a Model for Responsible AI Development",
    "published_date": "2023-09-04",
    "summary": "The article questions the open-source versus closed-source dichotomy in AI development, arguing that both models have inherent risks and benefits. It proposes \"Regulated Source\" as a potential alternative model for responsible AI development, aiming to address concerns raised by proponents of closed-source approaches while mitigating the risks associated with open-source models."
  }
]