[
  {
    "url": "https://arxiv.org/abs/2410.20288",
    "title": "Who is Responsible? Explaining Safety Violations in Multi-Agent Cyber-Physical Systems",
    "published_date": "2024-10-10",
    "abstract": "Multi-agent cyber-physical systems are present in a variety of applications. Agent decision-making can be affected due to errors induced by uncertain, dynamic operating environments or due to incorrect actions taken by an agent. When an erroneous decision that leads to a violation of safety is identified, assigning responsibility to individual agents is a key step towards preventing future accidents. Current approaches to carrying out such investigations require human labor or high degree of familiarity with operating environments. Automated strategies to assign responsibility can achieve significant reduction in human effort and associated cognitive burden.In this paper, we develop an automated procedure to assign responsibility for safety violations to actions of any single agent in a principled manner. We ground our approach on reasoning about safety violations in road safety. When provided with an instance of a safety violation, we use counterfactual reasoning to create alternate scenarios that determine how different outcomes might have been achieved if a specific action or set of actions was replaced by another action or set of actions. We devise a metric called the degree of responsibility (DoR) for each agent. The DoR uses the Shapley value to quantify the relative contribution of each agent to the observed safety violation, thus serving as a basis to explain and justify future decisions. We devise both heuristic techniques and methods based on the structure of agent interactions to improve scalability of our solution as the number of agents increases. We consider three instances of safety violations from the National Highway Traffic Safety Administration (NHTSA). We carry out experiments using representations of the three scenarios using the CARLA urban driving simulator. Our results indicate that the DoR enhances explainability of decision-making and assigning accountability for actions of agents and their consequences.",
    "summary": "This paper presents an automated method for assigning responsibility for safety violations in multi-agent cyber-physical systems, using counterfactual reasoning and the Shapley value to quantify each agent's contribution to the violation and improve explainability. The approach is demonstrated on real-world road safety scenarios simulated in CARLA."
  },
  {
    "title": "Towards a computational model of responsibility judgments in sequential human-AI collaboration",
    "abstract": "When a human and an AI agent collaborate to complete a task and something goes wrong, who is responsible? Prior work has developed theories to describe how people assign responsibility to individuals in teams. However, there has been little work studying the cognitive processes that underlie responsibility judgments in human-AI collaborations, especially for tasks comprising a sequence of interdependent actions. In this work, we take a step towards filling this gap. Using semi-autonomous driving as a paradigm, we develop an environment that simulates stylized cases of human-AI collaboration using a generative model of agent behavior. We propose a model of responsibility that considers how unexpected an agent's action was, and what would have happened had they acted differently. We test the model's predictions empirically and find that in addition to action expectations and counterfactual considerations, participants' responsibility judgments are also affected by how much each agent actually contributed to the outcome.",
    "published_date": "2024-07-17",
    "citation_count": 2,
    "url": "https://escholarship.org/uc/item/5h1742zk",
    "summary": "This paper investigates human responsibility judgments in sequential human-AI collaborations, proposing a computational model that considers action unexpectedness, counterfactuals, and contribution to outcome, and empirically testing its predictions using a simulated semi-autonomous driving scenario."
  },
  {
    "url": "https://arxiv.org/abs/2406.10572",
    "title": "Collaborative Framework with Shared Responsibility for Relief Management in Disaster Scenarios",
    "published_date": "2024-06-15",
    "abstract": "Disasters instances have been increasing both in frequency and intensity causing the tragic loss of life and making life harder for the survivors. Disaster relief management plays a crucial role in enhancing the lifestyle of disaster victims by managing the disaster impacts. Disaster relief management is a process with many collaborative sectors where different stakeholders should operate in all major phases of the disaster management progression. In the different phases of the disaster management process, many collaborative government organisations along with nongovernment organisations, leadership, community, and media at different levels need to share the responsibility with disaster victims to achieve effective disaster relief management. Shared responsibility enhances disaster relief management effectiveness and reduces the disaster's impact on the victims. Considering the diverse roles of different stakeholders, there has been a need for a framework that can bind different stakeholders together during disaster management. this paper shows a framework with major stakeholders of disaster relief management and how different stakeholders can take part in an effective disaster relief management process. The framework also highlights how each stakeholder can contribute to relief management at different phases after a disaster. The paper also explores some of the shared responsibility collaborative practices that have been implemented around the world in response to the disaster as a disaster relief management process. In addition, the paper highlights the knowledge obtained from those disaster instances and how this knowledge can be transferred and can be helpful in disaster mitigation and preparedness for future disaster scenarios.",
    "summary": "This paper proposes a collaborative framework for effective disaster relief management, emphasizing shared responsibility among diverse stakeholders (government, NGOs, communities, etc.) across all disaster phases. The framework highlights individual stakeholder contributions and incorporates lessons learned from past disaster responses to improve future preparedness and mitigation."
  },
  {
    "url": "https://arxiv.org/abs/2411.15147",
    "title": "Delegating Responsibilities to Intelligent Autonomous Systems: Challenges and Benefits",
    "published_date": "2024-11-06",
    "abstract": "As AI systems increasingly operate with autonomy and adaptability, the traditional boundaries of moral responsibility in techno-social systems are being challenged. This paper explores the evolving discourse on the delegation of responsibilities to intelligent autonomous agents and the ethical implications of such practices. Synthesizing recent developments in AI ethics, including concepts of distributed responsibility and ethical AI by design, the paper proposes a functionalist perspective as a framework. This perspective views moral responsibility not as an individual trait but as a role within a socio-technical system, distributed among human and artificial agents. As an example of 'AI ethical by design,' we present Basti and Vitiello's implementation. They suggest that AI can act as artificial moral agents by learning ethical guidelines and using Deontic Higher-Order Logic to assess decisions ethically. Motivated by the possible speed and scale beyond human supervision and ethical implications, the paper argues for 'AI ethical by design', while acknowledging the distributed, shared, and dynamic nature of responsibility. This functionalist approach offers a practical framework for navigating the complexities of AI ethics in a rapidly evolving technological landscape.",
    "summary": "This paper examines the ethical challenges of delegating responsibilities to autonomous AI systems, arguing for a functionalist approach that distributes responsibility among humans and AI agents within a socio-technical system. It advocates for \"AI ethical by design,\" using Basti and Vitiello's work as an example, to ensure ethical decision-making in AI."
  },
  {
    "url": "https://www.alignmentforum.org/tag/copenhagen-interpretation-of-ethics",
    "title": "Copenhagen Interpretation of Ethics - AI Alignment Forum",
    "published_date": "2024-02-01",
    "summary": "The Copenhagen Interpretation of Ethics posits that any interaction with a problem, regardless of intent, constitutes culpability. This implies responsibility extends even to passive observation or indirect involvement."
  },
  {
    "url": "https://arxiv.org/abs/2305.15003",
    "title": "Feasible Action-Space Reduction as a Metric of Causal Responsibility in Multi-Agent Spatial Interactions",
    "published_date": "2023-05-24",
    "abstract": "Modelling causal responsibility in multi-agent spatial interactions is crucial for safety and efficiency of interactions of humans with autonomous agents. However, current formal metrics and models of responsibility either lack grounding in ethical and philosophical concepts of responsibility, or cannot be applied to spatial interactions. In this work we propose a metric of causal responsibility which is tailored to multi-agent spatial interactions, for instance interactions in traffic. In such interactions, a given agent can, by reducing another agent's feasible action space, influence the latter. Therefore, we propose feasible action space reduction (FeAR) as a metric of causal responsibility among agents. Specifically, we look at ex-post causal responsibility for simultaneous actions. We propose the use of Moves de Rigueur (MdR) - a consistent set of prescribed actions for agents - to model the effect of norms on responsibility allocation. We apply the metric in a grid world simulation for spatial interactions and show how the actions, contexts, and norms affect the causal responsibility ascribed to agents. Finally, we demonstrate the application of this metric in complex multi-agent interactions. We argue that the FeAR metric is a step towards an interdisciplinary framework for quantifying responsibility that is needed to ensure safety and meaningful human control in human-AI systems.",
    "summary": "This paper introduces Feasible Action Space Reduction (FeAR) as a novel metric for quantifying causal responsibility in multi-agent spatial interactions, leveraging the concept of restricting another agent's possible actions and incorporating the influence of norms on responsibility assignment. The metric is demonstrated through simulations and proposed as a step towards safer and more accountable human-AI systems."
  },
  {
    "url": "https://arxiv.org/pdf/2301.02728.pdf",
    "title": "A responsibility value for digraphs",
    "published_date": "2023-01-06",
    "abstract": "There is an increasing need to hold players responsible for negative or positive impact that take place elsewhere in a value chain or a network. For example, countries or companies are held more and more responsible for their indirect carbon emissions. We introduce a responsibility value that allocates the total impact of the value chain among the players, taking into account their direct impact and their indirect impact through the underlying graph. Moreover, we show that the responsibility value satisfies a set of natural yet important properties.",
    "summary": "This paper introduces a responsibility value for digraphs, assigning responsibility for total network impact to individual players based on both their direct and indirect contributions. This value satisfies several key desirable properties."
  },
  {
    "url": "https://arxiv.org/pdf/2308.02608.pdf",
    "title": "Unravelling Responsibility for AI",
    "published_date": "2023-08-04",
    "abstract": "It is widely acknowledged that we need to establish where responsibility lies for the outputs and impacts of AI-enabled systems. But without a clear and precise understanding of what\"responsibility\"means, deliberations about where responsibility lies will be, at best, unfocused and incomplete and, at worst, misguided. To address this concern, this paper draws upon central distinctions in philosophy and law to clarify the concept of responsibility for AI for policymakers, practitioners, researchers and students from non-philosophical and non-legal backgrounds. Taking the three-part formulation\"Actor A is responsible for Occurrence O,\"the paper unravels the concept of responsibility to clarify that there are different possibilities of who is responsible for AI, the senses in which they are responsible, and aspects of events they are responsible for. Criteria and conditions for fitting attributions of responsibility in the core senses (causal responsibility, role-responsibility, liability responsibility and moral responsibility) are articulated to promote an understanding of when responsibility attributions would be inappropriate or unjust. The analysis is presented with a graphical notation to facilitate informal diagrammatic reasoning and discussion about specific cases. It is illustrated by application to a scenario of a fatal collision between an autonomous AI-enabled ship and a traditional, crewed vessel at sea.",
    "summary": "This paper clarifies the ambiguous concept of responsibility in the context of AI systems by distinguishing between causal, role, liability, and moral responsibility. It uses a three-part framework (\"Actor A is responsible for Occurrence O\") and a graphical notation to analyze responsibility attribution, illustrated with a case study of an AI-enabled ship collision."
  }
]