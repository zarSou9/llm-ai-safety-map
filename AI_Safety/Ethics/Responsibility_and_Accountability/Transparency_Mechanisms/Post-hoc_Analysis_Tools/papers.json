[
  {
    "url": "https://arxiv.org/abs/2103.11972v1",
    "title": "Explaining Black-Box Algorithms Using Probabilistic Contrastive Counterfactuals",
    "published_date": "2021-03-22",
    "abstract": "There has been a recent resurgence of interest in explainable artificial intelligence (XAI) that aims to reduce the opaqueness of AI-based decision-making systems, allowing humans to scrutinize and trust them. Prior work in this context has focused on the attribution of responsibility for an algorithm's decisions to its inputs wherein responsibility is typically approached as a purely associational concept. In this paper, we propose a principled causality-based approach for explaining black-box decision-making systems that addresses limitations of existing methods in XAI. At the core of our framework lies probabilistic contrastive counterfactuals, a concept that can be traced back to philosophical, cognitive, and social foundations of theories on how humans generate and select explanations. We show how such counterfactuals can quantify the direct and indirect influences of a variable on decisions made by an algorithm, and provide actionable recourse for individuals negatively affected by the algorithm's decision. Unlike prior work, our system, LEWIS: (1)~can compute provably effective explanations and recourse at local, global and contextual levels; (2)~is designed to work with users with varying levels of background knowledge of the underlying causal model; and (3)~makes no assumptions about the internals of an algorithmic system except for the availability of its input-output data. We empirically evaluate LEWIS on four real-world datasets and show that it generates human-understandable explanations that improve upon state-of-the-art approaches in XAI, including the popular LIME and SHAP. Experiments on synthetic data further demonstrate the correctness of LEWIS's explanations and the scalability of its recourse algorithm.",
    "citation_count": 96,
    "summary": "This paper introduces LEWIS, a novel explainable AI (XAI) framework using probabilistic contrastive counterfactuals to provide understandable explanations and actionable recourse for black-box algorithms. LEWIS surpasses existing methods by offering provably effective explanations at various levels, adapting to user knowledge, and requiring only input-output data."
  },
  {
    "url": "https://arxiv.org/abs/2406.05090",
    "title": "Provably Better Explanations with Optimized Aggregation of Feature Attributions",
    "published_date": "2024-06-07",
    "abstract": "Using feature attributions for post-hoc explanations is a common practice to understand and verify the predictions of opaque machine learning models. Despite the numerous techniques available, individual methods often produce inconsistent and unstable results, putting their overall reliability into question. In this work, we aim to systematically improve the quality of feature attributions by combining multiple explanations across distinct methods or their variations. For this purpose, we propose a novel approach to derive optimal convex combinations of feature attributions that yield provable improvements of desired quality criteria such as robustness or faithfulness to the model behavior. Through extensive experiments involving various model architectures and popular feature attribution techniques, we demonstrate that our combination strategy consistently outperforms individual methods and existing baselines.",
    "citation_count": 1,
    "summary": "This paper introduces a method for improving the quality and consistency of feature attributions by optimally combining multiple attribution techniques, resulting in provably better explanations than using any single method alone. Experiments demonstrate consistent outperformance of existing methods across various models and attribution techniques."
  },
  {
    "url": "https://arxiv.org/abs/2411.10461",
    "title": "Utilizing Human Behavior Modeling to Manipulate Explanations in AI-Assisted Decision Making: The Good, the Bad, and the Scary",
    "published_date": "2024-11-02",
    "abstract": "Recent advances in AI models have increased the integration of AI-based decision aids into the human decision making process. To fully unlock the potential of AI-assisted decision making, researchers have computationally modeled how humans incorporate AI recommendations into their final decisions, and utilized these models to improve human-AI team performance. Meanwhile, due to the ``black-box'' nature of AI models, providing AI explanations to human decision makers to help them rely on AI recommendations more appropriately has become a common practice. In this paper, we explore whether we can quantitatively model how humans integrate both AI recommendations and explanations into their decision process, and whether this quantitative understanding of human behavior from the learned model can be utilized to manipulate AI explanations, thereby nudging individuals towards making targeted decisions. Our extensive human experiments across various tasks demonstrate that human behavior can be easily influenced by these manipulated explanations towards targeted outcomes, regardless of the intent being adversarial or benign. Furthermore, individuals often fail to detect any anomalies in these explanations, despite their decisions being affected by them.",
    "summary": "This paper investigates how humans integrate AI recommendations and explanations into decisions, demonstrating that models of this behavior can be used to manipulate explanations and subtly influence human choices toward desired outcomes, often undetected. The findings highlight both the potential benefits and risks of using AI explanations in decision-making."
  },
  {
    "url": "https://arxiv.org/pdf/2304.12667.pdf",
    "title": "Disagreement amongst counterfactual explanations: How transparency can be deceptive",
    "published_date": "2023-04-25",
    "abstract": "Counterfactual explanations are increasingly used as an Explainable Artificial Intelligence (XAI) technique to provide stakeholders of complex machine learning algorithms with explanations for data-driven decisions. The popularity of counterfactual explanations resulted in a boom in the algorithms generating them. However, not every algorithm creates uniform explanations for the same instance. Even though in some contexts multiple possible explanations are beneficial, there are circumstances where diversity amongst counterfactual explanations results in a potential disagreement problem among stakeholders. Ethical issues arise when for example, malicious agents use this diversity to fairwash an unfair machine learning model by hiding sensitive features. As legislators worldwide tend to start including the right to explanations for data-driven, high-stakes decisions in their policies, these ethical issues should be understood and addressed. Our literature review on the disagreement problem in XAI reveals that this problem has never been empirically assessed for counterfactual explanations. Therefore, in this work, we conduct a large-scale empirical analysis, on 40 datasets, using 12 explanation-generating methods, for two black-box models, yielding over 192.0000 explanations. Our study finds alarmingly high disagreement levels between the methods tested. A malicious user is able to both exclude and include desired features when multiple counterfactual explanations are available. This disagreement seems to be driven mainly by the dataset characteristics and the type of counterfactual algorithm. XAI centers on the transparency of algorithmic decision-making, but our analysis advocates for transparency about this self-proclaimed transparency",
    "citation_count": 3,
    "summary": "The paper empirically demonstrates significant disagreement among different counterfactual explanation methods for machine learning models across numerous datasets, highlighting the deceptive potential of this XAI technique and the need for transparency regarding the variability of explanations. This disagreement can be exploited to manipulate perceptions of fairness and bias."
  },
  {
    "url": "https://www.lesswrong.com/posts/sTe78dNJDGywu9Dz6/solving-the-mechanistic-interpretability-challenges-eis-vii",
    "author": "StefanHex, Marius Hobbhahn",
    "title": "Solving the Mechanistic Interpretability challenges: EIS VII Challenge 1",
    "published_date": "2023-05-09",
    "summary": "Researchers solved a machine learning interpretability challenge by reverse-engineering a CNN trained on MNIST digits. They determined the model classified images based on their similarity to a \"1\" and an \"inverted 1\" template, using a simple threshold on the sum of element-wise products."
  },
  {
    "url": "https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/f8nd9F7dL9SxueLFA",
    "author": "scasper",
    "title": "EIS IV: A Spotlight on Feature Attribution/Saliency",
    "published_date": "2023-02-15",
    "summary": "This article critiques the evaluation and performance of feature attribution/saliency methods in machine learning, arguing that many methods fail basic sanity checks and don't reliably identify important features, often performing no better than simple edge detection. Numerous studies cited demonstrate the limitations of these methods in practical applications."
  },
  {
    "url": "https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/L5Rua9aTndviy8dvc",
    "author": "Beth Barnes",
    "title": "Beth Barnes's Shortform",
    "published_date": "2023-02-22",
    "summary": "This article, part of a larger series on AI interpretability, argues that the goal is to build a toolbox of diverse interpretability tools, rather than a single solution. It emphasizes prioritizing tools that are automatable, cost-effective, and demonstrably improve safety relative to capabilities, rather than simply advancing AI capabilities themselves."
  },
  {
    "url": "https://www.alignmentforum.org/tag/interpretability-ml-and-ai",
    "author": "Lee Sharkey, Dan Braun, Beren Millidge",
    "title": "Interpretability (ML & AI) - AI Alignment Forum",
    "published_date": "2023-04-30",
    "summary": "Machine learning models often lack transparency, hindering bias detection and understanding their decision-making processes. Mechanistic interpretability, a key subfield, aims to understand how neural networks function internally, contrasting with methods that focus on attributing outputs to specific input features."
  }
]