[
  {
    "url": "https://arxiv.org/abs/2409.15014",
    "title": "Acting for the Right Reasons: Creating Reason-Sensitive Artificial Moral Agents",
    "published_date": "2024-09-23",
    "abstract": "We propose an extension of the reinforcement learning architecture that enables moral decision-making of reinforcement learning agents based on normative reasons. Central to this approach is a reason-based shield generator yielding a moral shield that binds the agent to actions that conform with recognized normative reasons so that our overall architecture restricts the agent to actions that are (internally) morally justified. In addition, we describe an algorithm that allows to iteratively improve the reason-based shield generator through case-based feedback from a moral judge.",
    "summary": "This paper presents a reinforcement learning architecture augmented with a \"reason-based shield\" that guides agents towards morally justified actions based on normative reasons. The architecture is iteratively improved through feedback from a moral judge, ensuring actions align with pre-defined moral principles."
  },
  {
    "url": "https://arxiv.org/abs/2412.12848",
    "title": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical Insights from Large Language Models",
    "published_date": "2024-12-17",
    "abstract": "With the rise and widespread use of Large Language Models (LLMs), ensuring their safety is crucial to prevent harm to humans and promote ethical behaviors. However, directly assessing value valence (i.e., support or oppose) by leveraging large-scale data training is untrustworthy and inexplainable. We assume that emulating humans to rely on social norms to make moral decisions can help LLMs understand and predict moral judgment. However, capturing human values remains a challenge, as multiple related norms might conflict in specific contexts. Consider norms that are upheld by the majority and promote the well-being of society are more likely to be accepted and widely adopted (e.g.,\"don't cheat,\"). Therefore, it is essential for LLM to identify the appropriate norms for a given scenario before making moral decisions. To this end, we introduce a novel moral judgment approach called \\textit{ClarityEthic} that leverages LLMs' reasoning ability and contrastive learning to uncover relevant social norms for human actions from different perspectives and select the most reliable one to enhance judgment accuracy. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in moral judgment tasks. Moreover, human evaluations confirm that the generated social norms provide plausible explanations that support the judgments. This suggests that modeling human moral judgment with the emulating humans moral strategy is promising for improving the ethical behaviors of LLMs.",
    "summary": "ClarityEthic is a novel approach to improving the ethical decision-making of Large Language Models (LLMs) by using contrastive learning to identify and prioritize relevant social norms from multiple perspectives, resulting in more accurate and explainable moral judgments. This method outperforms existing techniques and provides plausible explanations for its decisions."
  },
  {
    "url": "https://arxiv.org/abs/2408.12250",
    "title": "Can Artificial Intelligence Embody Moral Values?",
    "published_date": "2024-08-22",
    "abstract": "The neutrality thesis holds that technology cannot be laden with values. This long-standing view has faced critiques, but much of the argumentation against neutrality has focused on traditional, non-smart technologies like bridges and razors. In contrast, AI is a smart technology increasingly used in high-stakes domains like healthcare, finance, and policing, where its decisions can cause moral harm. In this paper, we argue that artificial intelligence, particularly artificial agents that autonomously make decisions to pursue their goals, challenge the neutrality thesis. Our central claim is that the computational models underlying artificial agents can integrate representations of moral values such as fairness, honesty and avoiding harm. We provide a conceptual framework discussing the neutrality thesis, values, and AI. Moreover, we examine two approaches to designing computational models of morality, artificial conscience and ethical prompting, and present empirical evidence from text-based game environments that artificial agents with such models exhibit more ethical behavior compared to agents without these models. The findings support that AI can embody moral values, which contradicts the claim that all technologies are necessarily value-neutral.",
    "summary": "This paper challenges the neutrality thesis, arguing that artificial agents, through computational models incorporating moral values like fairness and avoiding harm, can embody and exhibit ethical behavior, contradicting the assertion that technology is inherently value-neutral. Empirical evidence from text-based games supports this claim."
  },
  {
    "url": "https://www.lesswrong.com/tag/machine-ethics",
    "title": "Machine Ethics - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Machine ethics aims to create Artificial Moral Agents (AMAs) capable of morally sound actions, a complex challenge currently limited to simple machines despite concerns about the ethical implications of future superintelligence. Early attempts like Asimov's Three Laws of Robotics highlighted the difficulty of creating truly reliable moral machines."
  },
  {
    "url": "https://www.lesswrong.com/posts/nnGwHuJfCBxKDgsds/embedding-ethical-priors-into-ai-systems-a-bayesian-approach",
    "author": "Justausername",
    "title": "Embedding Ethical Priors into AI Systems: A Bayesian Approach",
    "published_date": "2023-08-03",
    "summary": "This paper proposes a novel framework for embedding ethical considerations into AI systems by using Bayesian priors. This approach, inspired by human moral intuitions, aims to guide AI learning and decision-making toward ethically sound judgments."
  },
  {
    "url": "https://arxiv.org/pdf/2103.00752v1.pdf",
    "title": "Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence",
    "published_date": "2021-03-01",
    "abstract": "The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.",
    "citation_count": 21,
    "summary": "This paper proposes a philosophical framework for explainable AI, bridging technical, epistemic, and normative aspects of the debate surrounding AI's societal impact by identifying pertinent explanation types, relevant values, and the importance of diverse stakeholder input for improving AI system design. This framework aims to connect the technical and ethical dimensions of AI more effectively."
  },
  {
    "title": "A Multi-Agent Approach to Combine Reasoning and Learning for an Ethical Behavior",
    "abstract": "The recent field of Machine Ethics is experiencing rapid growth to answer the societal need for Artificial Intelligence (AI) algorithms imbued with ethical considerations, such as benevolence toward human users and actors. Several approaches already exist for this purpose, mostly either by reasoning over a set of predefined ethical principles (Top-Down), or by learning new principles (Bottom-Up). While both methods have their own advantages and drawbacks, only few works have explored hybrid approaches, such as using symbolic rules to guide the learning process for instance, combining the advantages of each. This paper draws upon existing works to propose a novel hybrid method using symbolic judging agents to evaluate the ethics of learning agents' behaviors, and accordingly improve their ability to ethically behave in dynamic multi-agent environments. Multiple benefits ensue from this separation between judging and learning agents: agents can evolve (or be updated by human designers) separately, benefiting from co-construction processes; judging agents can act as accessible proxies for non-expert human stakeholders or regulators; and finally, multiple points of view (one per judging agent) can be adopted to judge the behavior of the same agent, which produces a richer feedback. Our proposed approach is applied to an energy distribution problem, in the context of a Smart Grid simulator, with continuous and multi-dimensional states and actions. The experiments and results show the ability of learning agents to correctly adapt their behaviors to comply with the judging agents' rules, including when rules evolve over time.",
    "published_date": "2021-07-21",
    "citation_count": 7,
    "url": "https://dl.acm.org/doi/10.1145/3461702.3462515",
    "summary": "This paper proposes a multi-agent system for imbuing AI with ethical behavior, combining top-down reasoning (via judging agents evaluating actions based on ethical principles) and bottom-up learning (by learning agents adapting their behavior based on this evaluation), demonstrating its effectiveness in a smart grid simulation. The separation of judging and learning agents allows for flexible system evolution and diverse ethical perspectives."
  },
  {
    "url": "https://arxiv.org/abs/2110.14419v2",
    "title": "Toward a Theory of Justice for Artificial Intelligence",
    "published_date": "2021-10-27",
    "abstract": "Abstract This essay explores the relationship between artificial intelligence and principles of distributive justice. Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of sociotechnical systems, and that the operation of these systems is increasingly shaped and influenced by AI. Consequently, egalitarian norms of justice apply to the technology when it is deployed in these contexts. These norms entail that the relevant AI systems must meet a certain standard of public justification, support citizens' rights, and promote substantively fair outcomes, something that requires particular attention to the impact they have on the worst-off members of society.",
    "citation_count": 40,
    "summary": "This essay argues that principles of distributive justice, particularly Rawls's theory, should apply to artificial intelligence systems as they increasingly shape societal structures. Therefore, AI systems must be designed and deployed to uphold citizens' rights, promote fair outcomes, and especially protect the most disadvantaged."
  },
  {
    "url": "https://arxiv.org/pdf/2110.07574v1.pdf",
    "title": "CAN MACHINES LEARN MORALITY? THE DELPHI EXPERIMENT",
    "published_date": "2021-10-14",
    "abstract": "As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it.\r\n\r\nTo explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., “helping a friend” is generally good, while “helping a friend spread fake news” is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense.\r\n\r\nYet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.",
    "citation_count": 93,
    "summary": "The Delphi experiment, using deep neural networks, explores the feasibility of teaching machines morality by training them on descriptive ethical judgments. While demonstrating some success in generalization, Delphi also reveals limitations, including biases and inconsistencies, highlighting the challenges and complexities of imbuing AI with ethical reasoning."
  },
  {
    "title": "Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence",
    "abstract": "The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.",
    "published_date": "2021-03-01",
    "citation_count": 21,
    "url": "https://dl.acm.org/doi/10.1145/3442188.3445866",
    "summary": "This paper proposes a philosophical framework for explainable AI, addressing the gap in interdisciplinary dialogue by integrating technical, epistemic, and normative perspectives through a multi-faceted approach that considers explanation types, relevant values, and stakeholder engagement. This framework aims to bridge the technical and ethical aspects of AI system design."
  }
]