[
  {
    "url": "https://arxiv.org/abs/2410.21882",
    "title": "Building Altruistic and Moral AI Agent with Brain-inspired Affective Empathy Mechanisms",
    "published_date": "2024-10-29",
    "abstract": "As AI closely interacts with human society, it is crucial to ensure that its decision-making is safe, altruistic, and aligned with human ethical and moral values. However, existing research on embedding ethical and moral considerations into AI remains insufficient, and previous external constraints based on principles and rules are inadequate to provide AI with long-term stability and generalization capabilities. In contrast, the intrinsic altruistic motivation based on empathy is more willing, spontaneous, and robust. Therefore, this paper is dedicated to autonomously driving intelligent agents to acquire morally behaviors through human-like affective empathy mechanisms. We draw inspiration from the neural mechanism of human brain's moral intuitive decision-making, and simulate the mirror neuron system to construct a brain-inspired affective empathy-driven altruistic decision-making model. Here, empathy directly impacts dopamine release to form intrinsic altruistic motivation. Based on the principle of moral utilitarianism, we design the moral reward function that integrates intrinsic empathy and extrinsic self-task goals. A comprehensive experimental scenario incorporating empathetic processes, personal objectives, and altruistic goals is developed. The proposed model enables the agent to make consistent moral decisions (prioritizing altruism) by balancing self-interest with the well-being of others. We further introduce inhibitory neurons to regulate different levels of empathy and verify the positive correlation between empathy levels and altruistic preferences, yielding conclusions consistent with findings from psychological behavioral experiments. This work provides a feasible solution for the development of ethical AI by leveraging the intrinsic human-like empathy mechanisms, and contributes to the harmonious coexistence between humans and AI.",
    "summary": "This paper proposes a brain-inspired model for building altruistic AI agents, using simulated mirror neurons and dopamine reward to create affective empathy that motivates moral decision-making, balancing self-interest with the well-being of others. Experiments demonstrate a positive correlation between empathy levels and altruistic behavior in the model."
  },
  {
    "url": "https://arxiv.org/abs/2409.15014",
    "title": "Acting for the Right Reasons: Creating Reason-Sensitive Artificial Moral Agents",
    "published_date": "2024-09-23",
    "abstract": "We propose an extension of the reinforcement learning architecture that enables moral decision-making of reinforcement learning agents based on normative reasons. Central to this approach is a reason-based shield generator yielding a moral shield that binds the agent to actions that conform with recognized normative reasons so that our overall architecture restricts the agent to actions that are (internally) morally justified. In addition, we describe an algorithm that allows to iteratively improve the reason-based shield generator through case-based feedback from a moral judge.",
    "summary": "This paper presents a reinforcement learning architecture augmented with a \"reason-based shield\" that guides agents towards morally justified actions based on normative reasons. The architecture incorporates an iterative improvement algorithm using feedback from a moral judge to refine the shield's effectiveness."
  },
  {
    "url": "https://arxiv.org/abs/2408.12250",
    "title": "Can Artificial Intelligence Embody Moral Values?",
    "published_date": "2024-08-22",
    "abstract": "The neutrality thesis holds that technology cannot be laden with values. This long-standing view has faced critiques, but much of the argumentation against neutrality has focused on traditional, non-smart technologies like bridges and razors. In contrast, AI is a smart technology increasingly used in high-stakes domains like healthcare, finance, and policing, where its decisions can cause moral harm. In this paper, we argue that artificial intelligence, particularly artificial agents that autonomously make decisions to pursue their goals, challenge the neutrality thesis. Our central claim is that the computational models underlying artificial agents can integrate representations of moral values such as fairness, honesty and avoiding harm. We provide a conceptual framework discussing the neutrality thesis, values, and AI. Moreover, we examine two approaches to designing computational models of morality, artificial conscience and ethical prompting, and present empirical evidence from text-based game environments that artificial agents with such models exhibit more ethical behavior compared to agents without these models. The findings support that AI can embody moral values, which contradicts the claim that all technologies are necessarily value-neutral.",
    "summary": "This paper challenges the neutrality thesis by arguing that AI agents, through computational models incorporating moral values like fairness and avoiding harm, can embody and exhibit ethical behavior, contradicting the assertion that technology is inherently value-neutral. Empirical evidence from text-based games supports this claim."
  },
  {
    "url": "https://www.lesswrong.com/tag/machine-ethics",
    "title": "Machine Ethics - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Machine ethics seeks to create Artificial Moral Agents (AMAs) capable of moral reasoning, a challenge highlighted by the limitations of past attempts like Asimov's Three Laws of Robotics. The field's current focus is on developing basic ethical sensitivity in AI, particularly as the potential for superintelligence necessitates proactive measures."
  },
  {
    "url": "https://www.lesswrong.com/posts/wwioAJHTeaGqBvtjd/update-on-developing-an-ethics-calculator-to-align-an-agi-to",
    "author": "Sweenesm",
    "title": "Update on Developing an Ethics Calculator to Align an AGI to",
    "published_date": "2024-03-12",
    "summary": "This article details the ongoing development of an \"ethics calculator\" for aligning Artificial General Intelligence (AGI) with ethical principles. The calculator uses a utilitarian framework that incorporates positive experiences, rights, and self-esteem, aiming for quantitative consistency across diverse scenarios."
  },
  {
    "url": "https://arxiv.org/abs/2305.01424v1",
    "title": "Uncertain Machine Ethical Decisions Using Hypothetical Retrospection",
    "published_date": "2023-05-02",
    "abstract": "We propose the use of the hypothetical retrospection argumentation procedure, developed by Sven Ove Hansson to improve existing approaches to machine ethical reasoning by accounting for probability and uncertainty from a position of Philosophy that resonates with humans. Actions are represented with a branching set of potential outcomes, each with a state, utility, and either a numeric or poetic probability estimate. Actions are chosen based on comparisons between sets of arguments favouring actions from the perspective of their branches, even those branches that led to an undesirable outcome. This use of arguments allows a variety of philosophical theories for ethical reasoning to be used, potentially in flexible combination with each other. We implement the procedure, applying consequentialist and deontological ethical theories, independently and concurrently, to an autonomous library system use case. We introduce a preliminary framework that seems to meet the varied requirements of a machine ethics system: versatility under multiple theories and a resonance with humans that enables transparency and explainability.",
    "citation_count": 2,
    "summary": "This paper introduces a machine ethical decision-making framework using hypothetical retrospection, modeling actions with probabilistic outcomes and employing argumentation to compare actions based on diverse ethical theories (e.g., consequentialist and deontological), enhancing transparency and human understanding."
  },
  {
    "url": "https://arxiv.org/pdf/2307.11119.pdf",
    "title": "From computational ethics to morality: how decision-making algorithms can help us understand the emergence of moral principles, the existence of an optimal behaviour and our ability to discover it",
    "published_date": "2023-07-20",
    "abstract": "This paper adds to the efforts of evolutionary ethics to naturalize morality by providing specific insights derived from a computational ethics view. We propose a stylized model of human decision-making, which is based on Reinforcement Learning, one of the most successful paradigms in Artificial Intelligence. After the main concepts related to Reinforcement Learning have been presented, some particularly useful parallels are drawn that can illuminate evolutionary accounts of ethics. Specifically, we investigate the existence of an optimal policy (or, as we will refer to, objective ethical principles) given the conditions of an agent. In addition, we will show how this policy is learnable by means of trial and error, supporting our hypotheses on two well-known theorems in the context of Reinforcement Learning. We conclude by discussing how the proposed framework can be enlarged to study other potentially interesting areas of human behavior from a formalizable perspective.",
    "citation_count": 1,
    "summary": "This paper uses reinforcement learning models of human decision-making to explore the evolutionary origins of morality, arguing that optimal ethical principles exist and are discoverable through trial-and-error learning. The authors leverage reinforcement learning theorems to support their hypotheses on the learnability of optimal ethical policies."
  },
  {
    "url": "https://www.lesswrong.com/posts/nnGwHuJfCBxKDgsds/embedding-ethical-priors-into-ai-systems-a-bayesian-approach",
    "author": "Justausername",
    "title": "Embedding Ethical Priors into AI Systems: A Bayesian Approach",
    "published_date": "2023-08-03",
    "summary": "This paper proposes a novel framework for building ethical AI by incorporating ethical principles as Bayesian priors into AI learning processes. This approach, inspired by human moral intuitions, aims to guide AI decision-making towards ethically sound judgments."
  },
  {
    "url": "https://arxiv.org/pdf/2206.00595v2.pdf",
    "title": "Logic-Based Ethical Planning (short paper)",
    "published_date": "2022-06-01",
    "abstract": "In this paper we propose a framework for ethical decision making in the context of planning, with intended application to robotics. We put forward a compact but highly expressive language for ethical planning that combines linear temporal logic with lexicographic preference modelling. This original combination allows us to assess plans both with respect to an agent's values and their desires, introducing the novel concept of the morality level of an agent and moving towards multigoal, multivalue planning. We initiate the study of computational complexity of planning tasks in our setting, and we discuss potential applications to robotics.",
    "summary": "This paper introduces a novel ethical planning framework for robotics that uses a combination of linear temporal logic and lexicographic preferences to evaluate plans based on both agent values and desires, defining a \"morality level\" for the agent. The framework also explores the computational complexity of these planning tasks."
  },
  {
    "title": "A Multi-Agent Approach to Combine Reasoning and Learning for an Ethical Behavior",
    "abstract": "The recent field of Machine Ethics is experiencing rapid growth to answer the societal need for Artificial Intelligence (AI) algorithms imbued with ethical considerations, such as benevolence toward human users and actors. Several approaches already exist for this purpose, mostly either by reasoning over a set of predefined ethical principles (Top-Down), or by learning new principles (Bottom-Up). While both methods have their own advantages and drawbacks, only few works have explored hybrid approaches, such as using symbolic rules to guide the learning process for instance, combining the advantages of each. This paper draws upon existing works to propose a novel hybrid method using symbolic judging agents to evaluate the ethics of learning agents' behaviors, and accordingly improve their ability to ethically behave in dynamic multi-agent environments. Multiple benefits ensue from this separation between judging and learning agents: agents can evolve (or be updated by human designers) separately, benefiting from co-construction processes; judging agents can act as accessible proxies for non-expert human stakeholders or regulators; and finally, multiple points of view (one per judging agent) can be adopted to judge the behavior of the same agent, which produces a richer feedback. Our proposed approach is applied to an energy distribution problem, in the context of a Smart Grid simulator, with continuous and multi-dimensional states and actions. The experiments and results show the ability of learning agents to correctly adapt their behaviors to comply with the judging agents' rules, including when rules evolve over time.",
    "published_date": "2021-07-21",
    "citation_count": 7,
    "url": "https://dl.acm.org/doi/10.1145/3461702.3462515",
    "summary": "This paper proposes a hybrid multi-agent system for ethical AI, combining top-down reasoning (judging agents using symbolic rules) with bottom-up learning (learning agents adapting behavior), enabling separate evolution of agents, accessible ethical oversight, and diverse perspectives on ethical behavior. Experiments in a smart grid simulation demonstrate the approach's effectiveness in adapting to evolving ethical rules."
  }
]