[
  {
    "url": "https://arxiv.org/abs/2312.01818",
    "title": "Hybrid Approaches for Moral Value Alignment in AI Agents: a Manifesto",
    "published_date": "2023-12-04",
    "abstract": "Increasing interest in ensuring the safety of next-generation Artificial Intelligence (AI) systems calls for novel approaches to embedding morality into autonomous agents. This goal differs qualitatively from traditional task-specific AI methodologies. In this paper, we provide a systematization of existing approaches to the problem of introducing morality in machines - modelled as a continuum. Our analysis suggests that popular techniques lie at the extremes of this continuum - either being fully hard-coded into top-down, explicit rules, or entirely learned in a bottom-up, implicit fashion with no direct statement of any moral principle (this includes learning from human feedback, as applied to the training and finetuning of large language models, or LLMs). Given the relative strengths and weaknesses of each type of methodology, we argue that more hybrid solutions are needed to create adaptable and robust, yet controllable and interpretable agentic systems. To that end, this paper discusses both the ethical foundations (including deontology, consequentialism and virtue ethics) and implementations of morally aligned AI systems. We present a series of case studies that rely on intrinsic rewards, moral constraints or textual instructions, applied to either pure-Reinforcement Learning or LLM-based agents. By analysing these diverse implementations under one framework, we compare their relative strengths and shortcomings in developing morally aligned AI systems. We then discuss strategies for evaluating the effectiveness of moral learning agents. Finally, we present open research questions and implications for the future of AI safety and ethics which are emerging from this hybrid framework.",
    "citation_count": 3,
    "summary": "This paper argues that achieving moral alignment in AI requires hybrid approaches, combining explicitly coded moral rules with implicitly learned behavior, to overcome limitations of purely rule-based or purely learning-based methods. The authors propose a framework for analyzing and developing such hybrid systems, drawing on ethical theories and case studies."
  },
  {
    "url": "https://arxiv.org/abs/2308.15399",
    "title": "Rethinking Machine Ethics - Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
    "published_date": "2023-08-29",
    "abstract": "Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for overgeneralizing the moral stances of a limited group of annotators and lacking explainability. This work proposes a flexible top-down framework to steer (Large) Language Models (LMs) to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore, we show the alignment between different moral theories and existing morality datasets. Our analysis exhibits the potential and flaws in existing resources (models and datasets) in developing explainable moral judgment-making systems.",
    "citation_count": 18,
    "summary": "This paper critiques bottom-up approaches to machine ethics, proposing instead a top-down framework that uses established moral theories to guide large language models in moral reasoning. The framework's effectiveness is demonstrated experimentally, alongside an analysis of existing datasets and models' strengths and weaknesses."
  },
  {
    "url": "http://arxiv.org/abs/2312.11589",
    "title": "Moral Uncertainty and the Problem of Fanaticism",
    "published_date": "2023-12-18",
    "abstract": "While there is universal agreement that agents ought to act ethically, there is no agreement as to what constitutes ethical behaviour. To address this problem, recent philosophical approaches to `moral uncertainty' propose aggregation of multiple ethical theories to guide agent behaviour. However, one of the foundational proposals for aggregation - Maximising Expected Choiceworthiness (MEC) - has been criticised as being vulnerable to fanaticism; the problem of an ethical theory dominating agent behaviour despite low credence (confidence) in said theory. Fanaticism thus undermines the `democratic' motivation for accommodating multiple ethical perspectives. The problem of fanaticism has not yet been mathematically defined. Representing moral uncertainty as an instance of social welfare aggregation, this paper contributes to the field of moral uncertainty by 1) formalising the problem of fanaticism as a property of social welfare functionals and 2) providing non-fanatical alternatives to MEC, i.e. Highest k-trimmed Mean and Highest Median.",
    "citation_count": 1,
    "summary": "This paper formalizes the problem of fanaticism—where a low-credence ethical theory disproportionately influences action—within the framework of moral uncertainty and proposes the Highest k-trimmed Mean and Highest Median as non-fanatical alternatives to the existing Maximising Expected Choiceworthiness approach."
  },
  {
    "url": "https://www.lesswrong.com/posts/nnGwHuJfCBxKDgsds/embedding-ethical-priors-into-ai-systems-a-bayesian-approach",
    "author": "Justausername",
    "title": "Embedding Ethical Priors into AI Systems: A Bayesian Approach",
    "published_date": "2023-08-03",
    "summary": "This paper proposes a framework for embedding ethical considerations into AI systems by using Bayesian priors. This approach, inspired by human moral intuition, aims to guide AI learning and decision-making with ethical principles, offering a flexible alternative to rigid rule-based systems."
  },
  {
    "url": "https://arxiv.org/pdf/2101.08193.pdf",
    "title": "Mathematical foundations of moral preferences",
    "published_date": "2021-01-20",
    "abstract": "One-shot anonymous unselfishness in economic games is commonly explained by social preferences, which assume that people care about the monetary pay-offs of others. However, during the last 10 years, research has shown that different types of unselfish behaviour, including cooperation, altruism, truth-telling, altruistic punishment and trustworthiness are in fact better explained by preferences for following one's own personal norms—internal standards about what is right or wrong in a given situation. Beyond better organizing various forms of unselfish behaviour, this moral preference hypothesis has recently also been used to increase charitable donations, simply by means of interventions that make the morality of an action salient. Here we review experimental and theoretical work dedicated to this rapidly growing field of research, and in doing so we outline mathematical foundations for moral preferences that can be used in future models to better understand selfless human actions and to adjust policies accordingly. These foundations can also be used by artificial intelligence to better navigate the complex landscape of human morality.",
    "citation_count": 203,
    "summary": "The paper reviews experimental and theoretical work demonstrating that moral preferences, or internal standards of right and wrong, better explain unselfish behavior than social preferences focused on others' payoffs. It outlines mathematical foundations for modeling moral preferences to improve understanding of selfless actions and inform policy and AI development."
  },
  {
    "url": "https://arxiv.org/abs/2006.04734",
    "title": "Reinforcement Learning Under Moral Uncertainty",
    "published_date": "2020-06-08",
    "abstract": "An ambitious goal for artificial intelligence is to create agents that behave ethically: The capacity to abide by human moral norms would greatly expand the context in which autonomous agents could be practically and safely deployed. While ethical agents could be trained through reinforcement, by rewarding correct behavior under a specific moral theory (e.g. utilitarianism), there remains widespread disagreement (both societally and among moral philosophers) about the nature of morality and what ethical theory (if any) is objectively correct. Acknowledging such disagreement, recent work in moral philosophy proposes that ethical behavior requires acting under moral uncertainty, i.e. to take into account when acting that one's credence is split across several plausible ethical theories. Inspired by such work, this paper proposes a formalism that translates such insights to the field of reinforcement learning. Demonstrating the formalism's potential, we then train agents in simple environments to act under moral uncertainty, highlighting how such uncertainty can help curb extreme behavior from commitment to single theories. The overall aim is to draw productive connections from the fields of moral philosophy and machine ethics to that of machine learning, to inspire further research by highlighting a spectrum of machine learning research questions relevant to training ethically capable reinforcement learning agents.",
    "citation_count": 27,
    "summary": "This paper introduces a reinforcement learning formalism that addresses moral uncertainty by training agents to consider multiple ethical theories simultaneously, mitigating extreme behavior resulting from adherence to a single, potentially flawed, moral framework. The work bridges moral philosophy and machine learning to encourage research on ethically capable reinforcement learning agents."
  },
  {
    "title": "An Empirical Approach to Capture Moral Uncertainty in AI",
    "abstract": "As AI Systems become increasingly autonomous they are expected to engage in complex moral decision-making processes. For the purpose of guidance of such processes theoretical and empirical solutions have been sought. In this research we integrate both theoretical and empirical lines of thought to address the matters of moral reasoning in AI Systems. We reconceptualize a metanormative framework for decision-making under moral uncertainty within the Discrete Choice Analysis domain and we operationalize it through a latent class choice model. The discrete choice analysis-based formulation of the metanormative framework is theory-rooted and practical as it captures moral uncertainty through a small set of latent classes. To illustrate our approach we conceptualize a society in which AI Systems are in charge of making policy choices. In the proof of concept two AI systems make policy choices on behalf of a society but while one of the systems uses a baseline moral certain model the other uses a moral uncertain model. It was observed that there are cases in which the AI Systems disagree about the policy to be chosen which we believe is an indication about the relevance of moral uncertainty.",
    "published_date": "2020-02-04",
    "citation_count": 7,
    "url": "https://dl.acm.org/doi/10.1145/3375627.3375805",
    "summary": "This paper presents a novel approach to modeling moral uncertainty in AI decision-making, integrating theoretical metanorms with an empirical latent class choice model from discrete choice analysis. A proof-of-concept demonstrates how incorporating moral uncertainty, compared to a certainty model, can lead to differing policy choices by AI systems."
  },
  {
    "url": "https://arxiv.org/abs/2007.08670",
    "title": "Enabling Morally Sensitive Robotic Clarification Requests",
    "published_date": "2020-07-16",
    "abstract": "The design of current natural language-oriented robot architectures enables certain architectural components to circumvent moral reasoning capabilities. One example of this is reflexive generation of clarification requests as soon as referential ambiguity is detected in a human utterance. As shown in previous research, this can lead robots to (1) miscommunicate their moral dispositions and (2) weaken human perception or application of moral norms within their current context. We present a solution to these problems by performing moral reasoning on each potential disambiguation of an ambiguous human utterance and responding accordingly, rather than immediately and naively requesting clarification. We implement our solution in the Distributed Integrated Cognition Affect and Reflection robot architecture, which, to our knowledge, is the only current robot architecture with both moral reasoning and clarification request generation capabilities. We then evaluate our method with a human subjects experiment, the results of which indicate that our approach successfully ameliorates the two identified concerns.",
    "citation_count": 11,
    "summary": "This paper addresses the problem of robots prematurely requesting clarification of ambiguous human instructions, potentially masking their moral reasoning capabilities. The proposed solution integrates moral reasoning into the clarification request process, leading to improved communication and a stronger application of moral norms in human-robot interaction."
  }
]