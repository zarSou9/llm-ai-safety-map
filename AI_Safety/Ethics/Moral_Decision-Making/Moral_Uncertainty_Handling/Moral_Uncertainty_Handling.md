### Mini Description

Development of methods for AI systems to reason about and act under moral uncertainty, including approaches to weighing competing ethical considerations and handling novel moral scenarios.

### Description

Moral uncertainty handling addresses how AI systems should reason and act when faced with uncertainty about moral principles, ethical frameworks, or the correct moral evaluation of situations. This uncertainty can arise from competing ethical theories, incomplete moral knowledge, novel scenarios without clear precedent, or fundamental disagreements about moral facts. The challenge extends beyond standard decision theory under uncertainty, as it involves meta-level reasoning about different moral frameworks themselves.

Current research approaches include developing formal frameworks for comparing and aggregating different moral theories, methods for moral uncertainty resolution through learning from human feedback, and techniques for robust decision-making under moral uncertainty. Key challenges include the incommensurability of different moral theories, the difficulty of quantifying moral value across frameworks, and ensuring that uncertainty handling mechanisms remain stable under self-improvement or novel situations.

Open questions in the field include how to appropriately weight different moral theories, how to handle fundamental moral uncertainty (uncertainty about the nature of morality itself), and how to develop uncertainty handling mechanisms that are robust to manipulation or gaming. Researchers also investigate the relationship between moral uncertainty and other types of uncertainty (such as empirical or logical uncertainty) and how these different forms of uncertainty should interact in decision-making processes.

### Order

1. Moral_Theory_Integration
2. Learning_Under_Moral_Uncertainty
3. Robust_Decision_Procedures
4. Uncertainty_Quantification
5. Meta-Uncertainty_Management
