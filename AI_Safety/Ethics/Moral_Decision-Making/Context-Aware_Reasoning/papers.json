[
  {
    "url": "http://arxiv.org/abs/2312.17479",
    "title": "Culturally-Attuned Moral Machines: Implicit Learning of Human Value Systems by AI through Inverse Reinforcement Learning",
    "published_date": "2023-12-29",
    "abstract": "Constructing a universal moral code for artificial intelligence (AI) is difficult or even impossible, given that different human cultures have different definitions of morality and different societal norms. We therefore argue that the value system of an AI should be culturally attuned: just as a child raised in a particular culture learns the specific values and norms of that culture, we propose that an AI agent operating in a particular human community should acquire that community's moral, ethical, and cultural codes. How AI systems might acquire such codes from human observation and interaction has remained an open question. Here, we propose using inverse reinforcement learning (IRL) as a method for AI agents to acquire a culturally-attuned value system implicitly. We test our approach using an experimental paradigm in which AI agents use IRL to learn different reward functions, which govern the agents' moral values, by observing the behavior of different cultural groups in an online virtual world requiring real-time decision making. We show that an AI agent learning from the average behavior of a particular cultural group can acquire altruistic characteristics reflective of that group's behavior, and this learned value system can generalize to new scenarios requiring altruistic judgments. Our results provide, to our knowledge, the first demonstration that AI agents could potentially be endowed with the ability to continually learn their values and norms from observing and interacting with humans, thereby becoming attuned to the culture they are operating in.",
    "citation_count": 1,
    "summary": "This paper proposes using inverse reinforcement learning to enable AI agents to implicitly learn culturally-attuned moral values by observing and interacting with humans in a virtual world, demonstrating that AI can acquire and generalize altruistic behaviors reflective of specific cultural groups."
  },
  {
    "url": "https://arxiv.org/abs/2409.15014",
    "title": "Acting for the Right Reasons: Creating Reason-Sensitive Artificial Moral Agents",
    "published_date": "2024-09-23",
    "abstract": "We propose an extension of the reinforcement learning architecture that enables moral decision-making of reinforcement learning agents based on normative reasons. Central to this approach is a reason-based shield generator yielding a moral shield that binds the agent to actions that conform with recognized normative reasons so that our overall architecture restricts the agent to actions that are (internally) morally justified. In addition, we describe an algorithm that allows to iteratively improve the reason-based shield generator through case-based feedback from a moral judge.",
    "summary": "This paper presents a reinforcement learning architecture augmented with a \"reason-based shield\" that guides agents towards morally justified actions based on normative reasons. The architecture incorporates iterative improvement via feedback from a moral judge to refine the shield's effectiveness."
  },
  {
    "url": "https://arxiv.org/abs/2412.12848",
    "title": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical Insights from Large Language Models",
    "published_date": "2024-12-17",
    "abstract": "With the rise and widespread use of Large Language Models (LLMs), ensuring their safety is crucial to prevent harm to humans and promote ethical behaviors. However, directly assessing value valence (i.e., support or oppose) by leveraging large-scale data training is untrustworthy and inexplainable. We assume that emulating humans to rely on social norms to make moral decisions can help LLMs understand and predict moral judgment. However, capturing human values remains a challenge, as multiple related norms might conflict in specific contexts. Consider norms that are upheld by the majority and promote the well-being of society are more likely to be accepted and widely adopted (e.g.,\"don't cheat,\"). Therefore, it is essential for LLM to identify the appropriate norms for a given scenario before making moral decisions. To this end, we introduce a novel moral judgment approach called \\textit{ClarityEthic} that leverages LLMs' reasoning ability and contrastive learning to uncover relevant social norms for human actions from different perspectives and select the most reliable one to enhance judgment accuracy. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in moral judgment tasks. Moreover, human evaluations confirm that the generated social norms provide plausible explanations that support the judgments. This suggests that modeling human moral judgment with the emulating humans moral strategy is promising for improving the ethical behaviors of LLMs.",
    "summary": "ClarityEthic is a novel approach to improving the ethical decision-making of Large Language Models (LLMs) by using contrastive learning to identify and prioritize relevant social norms from various perspectives, resulting in more accurate and explainable moral judgments. This method outperforms existing techniques and provides plausible explanations for its decisions."
  },
  {
    "url": "https://arxiv.org/abs/2305.01424v1",
    "title": "Uncertain Machine Ethical Decisions Using Hypothetical Retrospection",
    "published_date": "2023-05-02",
    "abstract": "We propose the use of the hypothetical retrospection argumentation procedure, developed by Sven Ove Hansson to improve existing approaches to machine ethical reasoning by accounting for probability and uncertainty from a position of Philosophy that resonates with humans. Actions are represented with a branching set of potential outcomes, each with a state, utility, and either a numeric or poetic probability estimate. Actions are chosen based on comparisons between sets of arguments favouring actions from the perspective of their branches, even those branches that led to an undesirable outcome. This use of arguments allows a variety of philosophical theories for ethical reasoning to be used, potentially in flexible combination with each other. We implement the procedure, applying consequentialist and deontological ethical theories, independently and concurrently, to an autonomous library system use case. We introduce a preliminary framework that seems to meet the varied requirements of a machine ethics system: versatility under multiple theories and a resonance with humans that enables transparency and explainability.",
    "citation_count": 2,
    "summary": "This paper introduces a novel machine ethical decision-making framework using hypothetical retrospection, which incorporates probabilistic reasoning and diverse ethical theories (consequentialist and deontological) to select actions based on argumentative comparisons of potential outcomes. This approach aims for both versatility and human-resonant explainability."
  },
  {
    "url": "https://arxiv.org/pdf/2302.08759.pdf",
    "title": "Value Engineering for Autonomous Agents",
    "published_date": "2023-02-17",
    "abstract": "Machine Ethics (ME) is concerned with the design of Artificial Moral Agents (AMAs), i.e. autonomous agents capable of reasoning and behaving according to moral values. Previous approaches have treated values as labels associated with some actions or states of the world, rather than as integral components of agent reasoning. It is also common to disregard that a value-guided agent operates alongside other value-guided agents in an environment governed by norms, thus omitting the social dimension of AMAs. In this blue sky paper, we propose a new AMA paradigm grounded in moral and social psychology, where values are instilled into agents as context-dependent goals. These goals intricately connect values at individual levels to norms at a collective level by evaluating the outcomes most incentivized by the norms in place. We argue that this type of normative reasoning, where agents are endowed with an understanding of norms' moral implications, leads to value-awareness in autonomous agents. Additionally, this capability paves the way for agents to align the norms enforced in their societies with respect to the human values instilled in them, by complementing the value-based reasoning on norms with agreement mechanisms to help agents collectively agree on the best set of norms that suit their human values. Overall, our agent model goes beyond the treatment of values as inert labels by connecting them to normative reasoning and to the social functionalities needed to integrate value-aware agents into our modern hybrid human-computer societies.",
    "citation_count": 3,
    "summary": "This paper proposes a novel framework for designing Artificial Moral Agents (AMAs) that integrates values as context-dependent goals, connecting individual values to societal norms through normative reasoning and agreement mechanisms to achieve value-alignment within human-computer societies. The approach moves beyond treating values as simple labels, fostering value-awareness and enabling AMAs to adapt norms to better reflect human values."
  },
  {
    "url": "https://www.lesswrong.com/posts/nnGwHuJfCBxKDgsds/embedding-ethical-priors-into-ai-systems-a-bayesian-approach",
    "author": "Justausername",
    "title": "Embedding Ethical Priors into AI Systems: A BayesianÂ Approach",
    "published_date": "2023-08-03",
    "summary": "This paper proposes a framework for embedding ethical considerations into AI systems by using Bayesian priors. This approach, inspired by human moral intuition, aims to guide AI learning and decision-making toward ethically sound judgments."
  },
  {
    "url": "https://www.lesswrong.com/posts/BCynDEwguEiogicAo/reflection-of-hierarchical-relationship-via-nuanced?commentId=LxbpnsnaqWD3xEwZc",
    "author": "Kyoung-cheol Kim",
    "title": "Reflection of Hierarchical Relationship via Nuanced Conditioning of Game Theory Approach for AI Development and Utilization - LessWrong",
    "published_date": "2023-02-07",
    "summary": "The article explores applying game theory to AI development within organizational structures, highlighting both its benefits in multi-agent systems and limitations regarding bureaucratic decision-making. It argues that even with advanced AI, human-AI collaboration within a bureaucratic framework remains crucial for efficient problem-solving due to limitations in AI's capabilities and the inherent complexity of large-scale tasks."
  },
  {
    "url": "https://arxiv.org/pdf/2204.07612v1.pdf",
    "title": "Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in Artificial Intelligence",
    "published_date": "2022-04-15",
    "abstract": "In this meta-ethnography,we explore three different angles of Ethical AI design and implementation in a top-down/bottom-up framework, including the philosophical ethical viewpoint, the technical perspective, and framing through a political lens. We will discuss the values and drawbacks of individual and hybrid approaches within this framework. Examples of approaches include ethics either being determined by corporations and governments (coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technical-ities of how AI is developed within a moral construct, in consideration of its developers and users, with expected and unexpected consequences and long-term impact. This investigation includes real-world case studies, philosophical debate, and theoretical future thought experimentation based on historical fact, current world circumstances, and possible ensuing realities.",
    "summary": "This meta-ethnography analyzes top-down, bottom-up, and hybrid approaches to integrating ethics into AI, examining philosophical, technical, and political perspectives through case studies and theoretical explorations. The study evaluates the strengths and weaknesses of each approach considering developer, user, and societal impacts."
  },
  {
    "title": "Contexts and Context-awareness Revisited from an Intelligent Environments Perspective",
    "abstract": "ABSTRACT Context is a useful concept somehow unconsciously used by humans in daily life problem-solving. Recently, several subareas of computer science have been increasingly trying to rely on this concept to design systems with practical use in certain predefined circumstances. The perception is that imbuing in the system certain context-awareness qualities can support intelligent decision-making in specific practical situations. Despite a significant number of implemented systems that aim at providing context-awareness, there is a lack of commonly accepted and used methodologies and tools. At the root of this, is the lack of agreement on a set of good principles or standards, which can act as a guide to the scientific community and the developers interested in this class of systems. There have been some extensive surveys on the use of context, still there is no theoretical corpus emerging that we can use to discuss the essential concepts making up the fabric of contexts and its use by system developers. Here we attempted such enterprise at a level, which is more formal than popular surveys, in a way that is not implementation dependent and in a way that highlights key concepts of relevance to developers. We reassessed first the basic concepts identifying the need to more prominently consider system beneficiaries' satisfaction. We then transfer explicitly these values to a more formal outline of the basic componentgs and the operations which emerge as relevant. We identify and highlight the tasks of context activation, comparison, influence, construction, and interaction. We hint at how these may work in practice and explained these through examples. We show how the theory is flexible enough by generalizing it to multiusers so that optimization of global preferences and expectations is used to drive system development and system behavior.",
    "published_date": "2022-03-02",
    "citation_count": 13,
    "url": "https://www.tandfonline.com/doi/pdf/10.1080/08839514.2021.2008644?needAccess=true",
    "summary": "This paper argues for a more formalized and universally applicable understanding of context-awareness in intelligent environments, addressing the current lack of standardized methodologies by proposing a framework that emphasizes user satisfaction and outlines key operational tasks (activation, comparison, influence, construction, and interaction). The framework is further generalized to handle multi-user systems and optimize global preferences."
  },
  {
    "url": "https://www.alignmentforum.org/tag/situational-awareness-1",
    "author": "Ajeya Cotra",
    "title": "Situational Awareness - AI Alignment Forum",
    "published_date": "2022-07-18",
    "summary": "Ajeya Cotra defines situational awareness as a multifaceted skill encompassing self-awareness, understanding external influences, recognizing power dynamics, and predicting the consequences of actions. Alternatively, from a machine learning standpoint, it's viewed as advanced out-of-context meta-learning applied to contextually relevant information."
  }
]