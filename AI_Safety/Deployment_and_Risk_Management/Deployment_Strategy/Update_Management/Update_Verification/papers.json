[
  {
    "url": "https://arxiv.org/abs/2409.09174",
    "title": "Incorporation of Verifier Functionality in the Software for Operations and Network Attack Results Review and the Autonomous Penetration Testing System",
    "published_date": "2024-09-13",
    "abstract": "The software for operations and network attack results review (SONARR) and the autonomous penetration testing system (APTS) use facts and common properties in digital twin networks to represent real-world entities. However, in some cases fact values will change regularly, making it difficult for objects in SONARR and APTS to consistently and accurately represent their real-world counterparts. This paper proposes and evaluates the addition of verifiers, which check real-world conditions and update network facts, to SONARR. This inclusion allows SONARR to retrieve fact values from its executing environment and update its network, providing a consistent method of ensuring that the operations and, therefore, the results align with the real-world systems being assessed. Verifiers allow arbitrary scripts and dynamic arguments to be added to normal SONARR operations. This provides a layer of flexibility and consistency that results in more reliable output from the software.",
    "summary": "This paper proposes integrating \"verifiers\" into the Software for Operations and Network Attack Results Review (SONARR) system to dynamically update its digital twin network representation with real-world data, ensuring consistent and accurate results during penetration testing. This improves reliability by allowing SONARR to retrieve and update fact values directly from its execution environment."
  },
  {
    "url": "https://arxiv.org/abs/2406.17455",
    "title": "Smart Casual Verification of the Confidential Consortium Framework",
    "published_date": "2024-06-25",
    "abstract": "The Confidential Consortium Framework (CCF) is an open-source platform for developing trustworthy and reliable cloud applications. CCF powers Microsoft's Azure Confidential Ledger service and as such it is vital to build confidence in the correctness of CCF's design and implementation. This paper reports our experiences applying smart casual verification to validate the correctness of CCF's novel distributed protocols, focusing on its unique distributed consensus protocol and its custom client consistency model. We use the term smart casual verification to describe our hybrid approach, which combines the rigor of formal specification and model checking with the pragmatism of automated testing, in our case binding the formal specification in TLA+ to the C++ implementation. While traditional formal methods approaches require substantial buy-in and are often one-off efforts by domain experts, we have integrated our smart casual verification approach into CCF's CI pipeline, allowing contributors to continuously validate CCF as it evolves. We describe the challenges we faced in applying smart casual verification to a complex existing codebase and how we overcame them to find six subtle bugs in the design and implementation before they could impact production",
    "citation_count": 1,
    "summary": "This paper details a \"smart casual\" verification approach, combining formal specification (TLA+) and automated testing, used to validate the Confidential Consortium Framework (CCF)'s distributed protocols, revealing six bugs and integrating the process into CCF's CI pipeline for continuous validation."
  },
  {
    "url": "http://arxiv.org/abs/2401.09906",
    "title": "BUMP: A Benchmark of Reproducible Breaking Dependency Updates",
    "published_date": "2024-01-18",
    "abstract": "Third-party dependency updates can cause a build to fail if the new dependency version introduces a change that is incompatible with the usage: this is called a breaking dependency update. Research on breaking dependency updates is active, with works on characterization, understanding, automatic repair of breaking updates, and other software engineering aspects. All such research projects require a benchmark of breaking updates that has the following properties: 1) it contains real-world breaking updates; 2) the breaking updates can be executed; 3) the benchmark provides stable scientific artifacts of breaking updates over time, a property we call “reproducibility”. To the best of our knowledge, such a benchmark is missing. To address this problem, we present BUMP, a new benchmark that contains reproducible breaking dependency updates in the context of Java projects built with the Maven build system. BUMP contains 571 breaking dependency updates collected from 153 Java projects. BUMP ensures long-term reproducibility of dependency updates on different platforms, guaranteeing consistent build failures. We categorize the different causes of build breakage in BUMP, providing novel insights for future work on breaking update engineering. To our knowledge, BUMP is the first of its kind, providing hundreds of real-world breaking updates that have all been made reproducible.",
    "citation_count": 2,
    "summary": "BUMP is a new benchmark containing 571 reproducible, real-world breaking dependency updates in Java Maven projects, addressing the lack of such a resource for research on dependency update issues and enabling consistent study of build failures. It provides categorized causes of breakage for improved understanding and future research."
  },
  {
    "url": "https://arxiv.org/abs/2310.13328",
    "title": "One-Phase Batch Update on Sparse Merkle Trees for Rollups",
    "published_date": "2023-10-20",
    "abstract": "A sparse Merkle tree is a Merkle tree with fixed height and indexed leaves given by a map from indices to leaf values. It allows for both efficient membership and non-membership proofs. It has been widely used as an authenticated data structure in various applications, such as layer-2 rollups for blockchains. zkSync Lite, a popular Ethereum layer-2 rollup solution, uses a sparse Merkle tree to represent the state of the layer-2 blockchain. The account information is recorded in the leaves of the tree. In this paper, we study the sparse Merkle tree algorithms presented in zkSync Lite, and propose an efficient batch update algorithm to calculate a new root hash given a list of account (leaf) operations. Using the construction in zkSync Lite as a benchmark, our algorithm 1) improves the account update time from $\\mathcal{O}(\\log n)$ to $\\mathcal{O}(1)$ and 2) reduces the batch update cost by half using a one-pass traversal. Empirical analysis of real-world block data shows that our algorithm outperforms the benchmark by at most 14%.",
    "summary": "This paper proposes a novel one-phase batch update algorithm for sparse Merkle trees, improving the account update time from O(log n) to O(1) and halving the batch update cost compared to the zkSync Lite implementation through a single-pass traversal. Empirical results demonstrate a performance improvement of up to 14%."
  },
  {
    "url": "https://arxiv.org/abs/2210.04179",
    "title": "Oze: Decentralized Graph-based Concurrency Control for Real-world Long Transactions on BoM Benchmark",
    "published_date": "2022-10-09",
    "abstract": "In this paper, we propose Oze, a new concurrency control protocol that handles heterogeneous workloads which include long-running update transactions. Oze explores a large scheduling space using a fully precise multi-version serialization graph to reduce false positives. Oze manages the graph in a decentralized manner to exploit many cores in modern servers. We also propose a new OLTP benchmark, BoMB (Bill of Materials Benchmark), based on a use case in an actual manufacturing company. BoMB consists of one long-running update transaction and five short transactions that conflict with each other. Experiments using BoMB show that Oze keeps the abort rate of the long-running update transaction at zero while reaching up to 1.7 Mtpm for short transactions with near linear scalability, whereas state-of-the-art protocols cannot commit the long transaction or experience performance degradation in short transaction throughput.",
    "citation_count": 5,
    "summary": "Oze is a novel decentralized concurrency control protocol that uses a multi-version serialization graph to efficiently manage long and short transactions, achieving zero aborts for long transactions and high throughput for short transactions on the new BoMB benchmark. Unlike state-of-the-art methods, Oze demonstrates near-linear scalability without sacrificing performance on either transaction type."
  },
  {
    "url": "https://arxiv.org/pdf/2103.04828v2.pdf",
    "title": "A coordination-free, convergent, and safe replicated tree",
    "published_date": "2021-03-08",
    "abstract": "The tree is an essential data structure in many applications. In a distributed application, such as a distributed file system, the tree is replicated.To improve performance and availability, different clients should be able to update their replicas concurrently and without coordination. Such concurrent updates converge if the effects commute, but nonetheless, concurrent moves can lead to incorrect states and even data loss. Such a severe issue cannot be ignored; ultimately, only one of the conflicting moves may be allowed to take effect. However, as it is rare, a solution should be lightweight. Previous approaches would require preventative cross-replica coordination, or totally order move operations after-the-fact, requiring roll-back and compensation operations. In this paper, we present a novel replicated tree that supports coordination-free concurrent atomic moves, and provably maintains the tree invariant. Our analysis identifies cases where concurrent moves are inherently safe, and we devise a lightweight, coordination-free, rollback-free algorithm for the remaining cases, such that a maximal safe subset of moves takes effect. We present a detailed analysis of the concurrency issues with trees, justifying our replicated tree data structure. We provide mechanized proof that the data structure is convergent and maintains the tree invariant. Finally, we compare the response time and availability of our design against the literature.",
    "citation_count": 4,
    "summary": "This paper introduces a novel replicated tree data structure that allows for coordination-free concurrent updates, ensuring convergence and maintaining tree invariants without rollbacks. It achieves this through a lightweight algorithm that identifies and safely handles conflicting updates."
  },
  {
    "url": "https://arxiv.org/abs/2106.01154",
    "title": "Controlled Update of Software Components using Concurrent Exection of Patched and Unpatched Versions",
    "published_date": "2021-06-02",
    "abstract": "Software patching is a common method of removing vulnerabilities in software components to make IT systems more secure. However, there are many cases where software patching is not possible due to the critical nature of the application, especially when the vendor providing the application guarantees correct operation only in a specific configuration. In this paper, we propose a method to solve this problem. The idea is to run unpatched and patched application instances concurrently, with the unpatched one having complete control and the output of the patched one being used only for comparison, to watch for differences that are consequences of introduced bugs. To test this idea, we developed a system that allows us to run web applications in parallel and tested three web applications. The experiments have shown that the idea is promising for web applications from the technical side. Furthermore, we discuss the potential limitations of this system and the idea in general, how long two instances should run in order to be able to claim with some probability that the patched version has not introduced any new bugs, other potential use cases of the proposed system where two application instances run concurrently, and finally the potential uses of this system with different types of applications, such as SCADA systems.",
    "summary": "This paper proposes a method for safely updating software components by concurrently running patched and unpatched versions, using the unpatched version's output as the ground truth to detect bugs introduced by the patch. Experiments with web applications demonstrate the technical feasibility of this approach."
  },
  {
    "url": "https://arxiv.org/abs/2101.06542v3",
    "title": "ConE: A Concurrent Edit Detection Tool for Large-scale Software Development",
    "published_date": "2021-01-16",
    "abstract": "Modern, complex software systems are being continuously extended and adjusted. The developers responsible for this may come from different teams or organizations, and may be distributed over the world. This may make it difficult to keep track of what other developers are doing, which may result in multiple developers concurrently editing the same code areas. This, in turn, may lead to hard-to-merge changes or even merge conflicts, logical bugs that are difficult to detect, duplication of work, and wasted developer productivity. To address this, we explore the extent of this problem in the pull-request-based software development model. We study half a year of changes made to six large repositories in Microsoft in which at least 1,000 pull requests are created each month. We find that files concurrently edited in different pull requests are more likely to introduce bugs. Motivated by these findings, we design, implement, and deploy a service named Concurrent Edit Detector (ConE) that proactively detects pull requests containing concurrent edits, to help mitigate the problems caused by them. ConE has been designed to scale, and to minimize false alarms while still flagging relevant concurrently edited files. Key concepts of ConE include the detection of the Extent of Overlap between pull requests, and the identification of Rarely Concurrently Edited Files. To evaluate ConE, we report on its operational deployment on 234 repositories inside Microsoft. ConE assessed 26,000 pull requests and made 775 recommendations about conflicting changes, which were rated as useful in over 70% (554) of the cases. From interviews with 48 users, we learned that they believed ConE would save time in conflict resolution and avoiding duplicate work, and that over 90% intend to keep using the service on a daily basis.",
    "citation_count": 4,
    "summary": "ConE is a scalable service that detects concurrent edits in large-scale software development, proactively identifying potentially problematic pull requests to reduce merge conflicts, bugs, and wasted developer effort; its deployment at Microsoft showed high user satisfaction and significant utility in improving code quality and developer productivity."
  }
]