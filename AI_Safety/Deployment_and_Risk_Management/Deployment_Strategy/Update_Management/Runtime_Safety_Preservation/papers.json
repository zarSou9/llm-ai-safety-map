[
  {
    "url": "https://arxiv.org/abs/2409.10235",
    "title": "Maintaining Distributed Data Structures in Dynamic Peer-to-Peer Networks",
    "published_date": "2024-09-16",
    "abstract": "We study robust and efficient distributed algorithms for building and maintaining distributed data structures in dynamic Peer-to-Peer (P2P) networks. P2P networks are characterized by a high level of dynamicity with abrupt heavy node \\emph{churn} (nodes that join and leave the network continuously over time). We present a novel algorithm that builds and maintains with high probability a skip list for $poly(n)$ rounds despite $\\mathcal{O}(n/\\log n)$ churn \\emph{per round} ($n$ is the stable network size). We assume that the churn is controlled by an oblivious adversary (that has complete knowledge and control of what nodes join and leave and at what time and has unlimited computational power, but is oblivious to the random choices made by the algorithm). Moreover, the maintenance overhead is proportional to the churn rate. Furthermore, the algorithm is scalable in that the messages are small (i.e., at most $polylog(n)$ bits) and every node sends and receives at most $polylog(n)$ messages per round. Our algorithm crucially relies on novel distributed and parallel algorithms to merge two $n$-elements skip lists and delete a large subset of items, both in $\\mathcal{O}(\\log n)$ rounds with high probability. These procedures may be of independent interest due to their elegance and potential applicability in other contexts in distributed data structures. To the best of our knowledge, our work provides the first-known fully-distributed data structure that provably works under highly dynamic settings (i.e., high churn rate). Furthermore, they are localized (i.e., do not require any global topological knowledge). Finally, we believe that our framework can be generalized to other distributed and dynamic data structures including graphs, potentially leading to stable distributed computation despite heavy churn.",
    "citation_count": 1,
    "summary": "This paper presents a novel, fully distributed algorithm for maintaining a skip list data structure in dynamic peer-to-peer networks with high churn, achieving robustness and efficiency despite an oblivious adversary controlling node joins and leaves. The algorithm's scalability and localized nature, along with its efficient merge and delete procedures, offer a significant advance in handling highly dynamic distributed data structures."
  },
  {
    "url": "https://arxiv.org/abs/2409.16258",
    "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
    "published_date": "2024-09-24",
    "abstract": "Memory disaggregation is an emerging data center architecture that improves resource utilization and scalability. Replication is key to ensure the fault tolerance of applications, but replicating shared data in disaggregated memory is hard. We propose SWARM (Swift WAit-free Replication in disaggregated Memory), the first replication scheme for in-disaggregated-memory shared objects to provide (1) single-roundtrip reads and writes in the common case, (2) strong consistency (linearizability), and (3) strong liveness (wait-freedom). SWARM makes two independent contributions. The first is Safe-Guess, a novel wait-free replication protocol with single-roundtrip operations. The second is In-n-Out, a novel technique to provide conditional atomic update and atomic retrieval of large buffers in disaggregated memory in one roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly consistent and highly available disaggregated key-value store. We evaluate SWARM-KV and find that it has marginal latency overhead compared to an unreplicated key-value store, and that it offers much lower latency and better availability than FUSEE, a state-of-the-art replicated disaggregated key-value store.",
    "summary": "SWARM is a novel wait-free replication scheme for shared data in disaggregated memory architectures, achieving single-roundtrip reads/writes, strong consistency, and strong liveness with minimal latency overhead compared to unreplicated systems. It utilizes Safe-Guess and In-n-Out techniques to enable efficient atomic operations on large buffers."
  },
  {
    "url": "https://arxiv.org/abs/2410.18124",
    "title": "Optimal Checkpoint Interval with Availability as an Objective Function",
    "published_date": "2024-10-15",
    "abstract": "We present a simplified derivation of the optimal checkpoint interval in Young_1974 [1]. The optimal checkpoint interval derivation in [1] is based on minimizing the total lost time as an objective-function. Lost time is a function of checkpoint interval, checkpoint save time, and average failure time. This simplified derivation yields lost-time-optimal that is identical to the one derived in [1]. For large scale-out super-computer or datacenter systems, what is important is the selection of optimal checkpoint interval that maximizes availability. We show that availability-optimal checkpoint interval is different from the one derived in [1]. However, availability-optimal checkpoint interval is asymptotically same as lost-time-optimal checkpoint interval for certain conditions on checkpoint save and recovery time. We show that these optimal checkpoint intervals hold in situations where the error detection latency is significantly smaller than any selected checkpoint interval. However, in cases where the error detection latency is very large then the optimal checkpoint interval is greater than or equal to the error detection latency.",
    "summary": "This paper re-derives the optimal checkpoint interval, first minimizing lost time then maximizing availability, showing they differ but converge under specific conditions of checkpoint and recovery times. The optimal interval also depends on error detection latency, exceeding it when latency is significant."
  },
  {
    "url": "https://arxiv.org/abs/2408.08702",
    "title": "Vertical Atomic Broadcast and Passive Replication (Extended Version)",
    "published_date": "2024-08-16",
    "abstract": "Atomic broadcast is a reliable communication abstraction ensuring that all processes deliver the same set of messages in a common global order. It is a fundamental building block for implementing fault-tolerant services using either active (aka state-machine) or passive (aka primary-backup) replication. We consider the problem of implementing reconfigurable atomic broadcast, which further allows users to dynamically alter the set of participating processes, e.g., in response to failures or changes in the load. We give a complete safety and liveness specification of this communication abstraction and propose a new protocol implementing it, called Vertical Atomic Broadcast, which uses an auxiliary service to facilitate reconfiguration. In contrast to prior proposals, our protocol significantly reduces system downtime when reconfiguring from a functional configuration by allowing it to continue processing messages while agreement on the next configuration is in progress. Furthermore, we show that this advantage can be maintained even when our protocol is modified to support a stronger variant of atomic broadcast required for passive replication.",
    "summary": "Vertical Atomic Broadcast is a new protocol for reconfigurable atomic broadcast that minimizes downtime during reconfiguration by processing messages concurrently with configuration agreement, unlike previous methods. This protocol is adaptable to support the stricter requirements of passive replication."
  },
  {
    "url": "https://www.lesswrong.com/tag/knuths-up-arrow-notation",
    "title": "Knuth's Up-Arrow Notation - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Knuth's up-arrow notation provides a concise way to represent extremely large numbers, such as 3^^^3, which despite being easily describable requires vastly more space to write out in base 10 than exists atoms in the observable universe."
  },
  {
    "url": "https://arxiv.org/abs/2309.04873",
    "title": "From Reversible Computation to Checkpoint-Based Rollback Recovery for Message-Passing Concurrent Programs",
    "published_date": "2023-09-09",
    "abstract": "The reliability of concurrent and distributed systems often depends on some well-known techniques for fault tolerance. One such technique is based on checkpointing and rollback recovery. Checkpointing involves processes to take snapshots of their current states regularly, so that a rollback recovery strategy is able to bring the system back to a previous consistent state whenever a failure occurs. In this paper, we consider a message-passing concurrent programming language and propose a novel rollback recovery strategy that is based on some explicit checkpointing operators and the use of a (partially) reversible semantics for rolling back the system.",
    "citation_count": 3,
    "summary": "This paper presents a novel rollback recovery strategy for message-passing concurrent programs, combining explicit checkpointing with a partially reversible semantics to restore a consistent system state after a failure. The approach improves fault tolerance in concurrent and distributed systems."
  },
  {
    "url": "https://arxiv.org/abs/2210.04179",
    "title": "Oze: Decentralized Graph-based Concurrency Control for Real-world Long Transactions on BoM Benchmark",
    "published_date": "2022-10-09",
    "abstract": "In this paper, we propose Oze, a new concurrency control protocol that handles heterogeneous workloads which include long-running update transactions. Oze explores a large scheduling space using a fully precise multi-version serialization graph to reduce false positives. Oze manages the graph in a decentralized manner to exploit many cores in modern servers. We also propose a new OLTP benchmark, BoMB (Bill of Materials Benchmark), based on a use case in an actual manufacturing company. BoMB consists of one long-running update transaction and five short transactions that conflict with each other. Experiments using BoMB show that Oze keeps the abort rate of the long-running update transaction at zero while reaching up to 1.7 Mtpm for short transactions with near linear scalability, whereas state-of-the-art protocols cannot commit the long transaction or experience performance degradation in short transaction throughput.",
    "citation_count": 5,
    "summary": "Oze is a novel decentralized concurrency control protocol using a multi-version serialization graph to efficiently manage heterogeneous workloads, including long transactions, achieving zero abort rates for long transactions and high throughput for short transactions in the new BoMB benchmark. Unlike state-of-the-art protocols, Oze demonstrates near-linear scalability without sacrificing long transaction committability."
  },
  {
    "url": "https://arxiv.org/abs/2209.00813",
    "title": "CASU: Compromise Avoidance via Secure Update for Low-end Embedded Systems",
    "published_date": "2022-09-02",
    "abstract": "Guaranteeing runtime integrity of embedded system software is an open problem. Trade-offs between security and other priorities (e.g., cost or performance) are inherent, and resolving them is both challenging and important. The proliferation of runtime attacks that introduce malicious code (e.g., by injection) into embedded devices has prompted a range of mitigation techniques. One popular approach is Remote Attestation (ℛA), whereby a trusted entity (verifier) checks the current software state of an untrusted remote device (prover). RA yields a timely authenticated snapshot of prover state that verifier uses to decide whether an attack occurred.Current RA schemes require verifier to explicitly initiate ℛA, based on some unclear criteria. Thus, in case of prover's compromise, verifier only learns about it late, upon the next ℛA instance. While sufficient for compromise detection, some applications would benefit from a more proactive, prevention-based approach. To this end, we construct CASU: Compromise Avoidance via Secure Updates. CASU is an inexpensive hardware/software co-design enforcing: (i) runtime software immutability, thus precluding any illegal software modification, and (ii) authenticated updates as the sole means of modifying software. In CASU, a successful ℛA instance serves as a proof of successful update, and continuous subsequent software integrity is implicit, due to the runtime immutability guarantee. This obviates the need for ℛA in between software updates and leads to unobtrusive integrity assurance with guarantees akin to those of prior ℛA techniques, with better overall performance.",
    "citation_count": 5,
    "summary": "CASU is a hardware/software system for low-end embedded devices that prevents runtime software modification, ensuring integrity through authenticated updates and eliminating the need for continuous remote attestation. This approach offers improved performance and proactive compromise avoidance compared to traditional remote attestation methods."
  }
]