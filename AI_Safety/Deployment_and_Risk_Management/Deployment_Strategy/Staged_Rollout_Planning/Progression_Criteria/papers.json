[
  {
    "url": "https://arxiv.org/pdf/1905.10493v1.pdf",
    "title": "Safely and Quickly Deploying New Features with a Staged Rollout Framework Using Sequential Test and Adaptive Experimental Design",
    "published_date": "2018-07-01",
    "abstract": "During the rapid development cycle for Internet products (websites and mobile apps), new features are developed and rolled out to users constantly. Features with code defects or design flaws can cause outages and significant degradation of user experience. The traditional method of code review and change management can be time-consuming and error-prone. In order to make the feature rollout process safe and fast, this paper proposes a methodology for rolling out features in an automated way using an adaptive experimental design. Under this framework, a feature is gradually ramped up from a small proportion of users to a larger population based on real-time evaluation of the performance of important metrics. If there are any regression detected during the ramp-up step, the ramp-up process stops and the feature developer is alerted. There are two main algorithm components powering this framework: 1) a continuous monitoring algorithm - using a variant of the sequential probability ratio test (SPRT) to monitor the feature performance metrics and alert feature developers when a metric degradation is detected, 2) an automated ramp-up algorithm - deciding when and how to ramp up to the next stage with larger sample size. This paper presents one monitoring algorithm and three ramping up algorithms including time-based, power-based, and risk-based (a Bayesian approach) schedules. These algorithms are evaluated and compared on both simulated data and real data. There are three benefits provided by this framework for feature rollout: 1) for defective features, it can detect the regression early and reduce negative effect, 2) for healthy features, it rolls out the feature quickly, 3) it reduces the need for manual intervention via the automation of the feature rollout process.",
    "citation_count": 6,
    "summary": "This paper presents a staged rollout framework for quickly and safely deploying new features, using sequential testing and adaptive experimental design to continuously monitor performance metrics and automatically adjust rollout speed based on real-time evaluation, minimizing negative impact from faulty features while accelerating deployment of successful ones."
  },
  {
    "url": "http://arxiv.org/abs/2401.11567",
    "title": "Deterministic Multi-stage Constellation Reconfiguration Using Integer Linear Programing and Sequential Decision-Making Methods",
    "published_date": "2024-01-21",
    "abstract": "This paper addresses the problem of reconfiguring Earth observation satellite constellation systems through multiple stages. The Multistage Constellation Reconfiguration Problem (MCRP) aims to maximize the total observation rewards obtained by covering a set of targets of interest through the active manipulation of the orbits and relative phasing of constituent satellites. This paper considers deterministic problem settings in which the targets of interest are known a priori. We propose a novel integer linear programming formulation for MCRP, capable of obtaining provably optimal solutions. To overcome computational intractability due to the combinatorial explosion in solving large-scale instances, we introduce two computationally efficient sequential decision-making methods based on the principles of a myopic policy and a rolling horizon procedure. The computational experiments demonstrate that the devised sequential decision-making approaches yield high-quality solutions with improved computational efficiency over the baseline MCRP. Finally, a case study using Hurricane Harvey data showcases the advantages of multistage constellation reconfiguration over single-stage and no-reconfiguration scenarios.",
    "summary": "This paper presents an integer linear programming formulation for optimally reconfiguring Earth observation satellite constellations over multiple stages to maximize target observation rewards, and introduces two computationally efficient sequential decision-making methods to handle large-scale problems."
  },
  {
    "url": "http://arxiv.org/abs/2401.09678",
    "title": "Integrating Graceful Degradation and Recovery Through Requirement-Driven Adaptation",
    "published_date": "2024-01-18",
    "abstract": "Cyber-physical systems (CPS) are subject to environmental uncertainties such as adverse operating conditions, malicious attacks, and hardware degradation. These uncertainties may lead to failures that put the system in a sub-optimal or unsafe state. Systems that are resilient to such uncertainties rely on two types of operations: (1) graceful degradation, for ensuring that the system maintains an acceptable level of safety during unexpected environmental conditions and (2) recovery, to facilitate the resumption of normal system functions. Typically, mechanisms for degradation and recovery are developed independently from each other, and later integrated into a system, requiring the designer to develop an additional, ad-hoc logic for activating and coordinating between the two operations. In this paper, we propose a self-adaptation approach for improving system resiliency through automated triggering and coordination of graceful degradation and recovery. The key idea behind our approach is to treat degradation and recovery as requirement-driven adaptation tasks: Degradation can be thought of as temporarily weakening original (i.e., ideal) system requirements to be achieved by the system, and recovery as strengthening the weakened requirements when the environment returns within an expected operating boundary. Furthermore, by treating weakening and strengthening as dual operations, we argue that a single requirement-based adaptation method is sufficient to enable coordination between degradation and recovery. Given system requirements specified in signal temporal logic (STL), we propose a run-time adaptation framework that performs degradation and recovery in response to environmental changes. We describe a prototype implementation of our framework and demonstrate the feasibility of the proposed approach using a case study in unmanned underwater vehicles.",
    "summary": "This paper presents a self-adaptation approach for improving cyber-physical system resilience by integrating graceful degradation and recovery as requirement-driven adaptation tasks, using signal temporal logic to automate triggering and coordination based on environmental changes. A prototype implementation and case study demonstrate the feasibility of this unified approach."
  },
  {
    "url": "https://arxiv.org/abs/2402.00015",
    "title": "Maintaining User Trust Through Multistage Uncertainty Aware Inference",
    "published_date": "2023-12-28",
    "abstract": "This paper describes and evaluates a multistage approach to AI deployment. Each stage involves a more accurate method of inference, yet engaging each comes with an increasing cost. In outlining the architecture, we present a method for quantifying model uncertainty that facilitates confident deferral decisions. The architecture is currently under active deployment to thousands of cotton farmers across India. The broader idea however is applicable to a growing sector of AI deployments in challenging low resources settings.",
    "summary": "This paper introduces a multistage AI deployment strategy that balances inference accuracy and cost by using uncertainty quantification to defer to more accurate, but expensive, methods when necessary. This approach is being used in India to help cotton farmers and is applicable to other resource-constrained AI deployments."
  },
  {
    "url": "https://arxiv.org/abs/2309.17241",
    "title": "Measuring the Robustness of Predictive Probability for Early Stopping in Experimental Design",
    "published_date": "2023-09-29",
    "abstract": "Physical experiments in the national security domain are often expensive and time-consuming. Test engineers must certify the compatibility of aircraft and their weapon systems before they can be deployed in the field, but the testing required is time consuming, expensive, and resource limited. Adopting Bayesian adaptive designs are a promising way to borrow from the successes seen in the clinical trials domain. The use of predictive probability (PP) to stop testing early and make faster decisions is particularly appealing given the aforementioned constraints. Given the high-consequence nature of the tests performed in the national security space, a strong understanding of new methods is required before being deployed. Although PP has been thoroughly studied for binary data, there is less work with continuous data, which often in reliability studies interested in certifying the specification limits of components. A simulation study evaluating the robustness of this approach indicate early stopping based on PP is reasonably robust to minor assumption violations, especially when only a few interim analyses are conducted. A post-hoc analysis exploring whether release requirements of a weapon system from an aircraft are within specification with desired reliability resulted in stopping the experiment early and saving 33% of the experimental runs.",
    "summary": "This paper investigates the robustness of using predictive probability for early stopping in expensive, continuous-data experiments, finding it reasonably robust to minor assumption violations, particularly with few interim analyses, and demonstrating cost savings in a national security application."
  },
  {
    "url": "https://arxiv.org/abs/2204.02189",
    "title": "Automating Staged Rollout with Reinforcement Learning",
    "published_date": "2022-04-01",
    "abstract": "Staged rollout is a strategy of incrementally releasing software updates to portions of the user population in order to accelerate defect discovery without incurring catastrophic outcomes such as system wide outages. Some past studies have examined how to quantify and automate staged rollout, but stop short of simultaneously considering multiple product or process metrics explicitly. This paper demonstrates the potential to automate staged rollout with multi-objective reinforcement learning in order to dynamically balance stakeholder needs such as time to deliver new features and downtime incurred by failures due to latent defects. CCS CONCEPTS • Software and its engineering → Software testing and debugging.",
    "citation_count": 1,
    "summary": "This paper proposes automating staged software rollouts using multi-objective reinforcement learning, enabling dynamic optimization of conflicting goals like faster feature delivery and minimizing downtime caused by undiscovered defects. It improves upon previous work by explicitly considering multiple product and process metrics simultaneously."
  },
  {
    "title": "Impact of decision horizon on post-prognostics maintenance and missions scheduling: a railways case study",
    "abstract": "ABSTRACT In this paper, we propose a study of the decision horizon duration for rolling stock mission assignment and maintenance planning in a prognostics and health management (PHM) context. The aim is to determine the best decision horizon duration that allows the construction of a suitable schedule that assigns railway vehicles to missions and integrates required maintenance operations according to the current and future health of the vehicles. A genetic algorithm is used to minimize the overall cost of the joint schedule as a function of the decision horizon. The results are compared to three proposed heuristics to study the influence of the resolution method on the decision horizon duration. The best decision horizon duration is given for each used method for an illustration case.",
    "published_date": "2021-06-22",
    "citation_count": 8,
    "url": "https://www.tandfonline.com/doi/full/10.1080/23248378.2021.1940329",
    "summary": "This paper investigates the optimal decision horizon length for scheduling railway vehicle missions and maintenance, using a genetic algorithm to minimize overall costs considering prognostics and health management data. The study compares the algorithm's performance to three heuristic methods, identifying optimal horizon lengths for each."
  },
  {
    "url": "https://arxiv.org/pdf/2101.10430.pdf",
    "title": "Test and Evaluation Framework for Multi-Agent Systems of Autonomous Intelligent Agents",
    "published_date": "2021-01-25",
    "abstract": "Test and evaluation is a necessary process for ensuring that engineered systems perform as intended under a variety of conditions, both expected and unexpected. In this work, we consider the unique challenges of developing a unifying test and evaluation framework for complex ensembles of cyber-physical systems with embedded artificial intelligence. We propose a framework that incorporates test and evaluation throughout not only the development life cycle, but continues into operation as the system learns and adapts in a noisy, changing, and contended environment. The framework accounts for the challenges of testing the integration of diverse systems at various hierarchical scales of composition while respecting that testing time and resources are limited. A generic use case is provided for illustrative purposes. Research directions emerging as a result of exploring the use case via the framework are suggested.",
    "citation_count": 6,
    "summary": "This paper proposes a unifying test and evaluation framework for complex multi-agent systems, addressing the challenges of testing integrated cyber-physical systems with embedded AI throughout their lifecycle, even as they learn and adapt in dynamic environments. The framework considers resource constraints and hierarchical system composition, illustrated with a generic use case and suggestions for future research."
  }
]