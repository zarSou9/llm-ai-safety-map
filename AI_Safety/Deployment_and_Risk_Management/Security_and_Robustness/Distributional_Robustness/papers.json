[
  {
    "url": "https://arxiv.org/pdf/2212.09962.pdf",
    "title": "Distributional Robustness Bounds Generalization Errors",
    "published_date": "2022-12-20",
    "abstract": "Bayesian methods, distributionally robust optimization methods, and regularization methods are three pillars of trustworthy machine learning combating distributional uncertainty, e.g., the uncertainty of an empirical distribution compared to the true underlying distribution. This paper investigates the connections among the three frameworks and, in particular, explores why these frameworks tend to have smaller generalization errors. Specifically, first, we suggest a quantitative definition for\"distributional robustness\", propose the concept of\"robustness measure\", and formalize several philosophical concepts in distributionally robust optimization. Second, we show that Bayesian methods are distributionally robust in the probably approximately correct (PAC) sense; in addition, by constructing a Dirichlet-process-like prior in Bayesian nonparametrics, it can be proven that any regularized empirical risk minimization method is equivalent to a Bayesian method. Third, we show that generalization errors of machine learning models can be characterized using the distributional uncertainty of the nominal distribution and the robustness measures of these machine learning models, which is a new perspective to bound generalization errors, and therefore, explain the reason why distributionally robust machine learning models, Bayesian models, and regularization models tend to have smaller generalization errors in a unified manner.",
    "citation_count": 4,
    "summary": "This paper establishes connections between Bayesian methods, distributionally robust optimization, and regularization, showing that their superior generalization performance stems from their inherent distributional robustness. It quantifies distributional robustness, proving that Bayesian and regularized methods are distributionally robust and using this robustness to bound generalization error."
  },
  {
    "url": "https://arxiv.org/pdf/2104.13326v2.pdf",
    "title": "Fast Distributionally Robust Learning with Variance Reduced Min-Max Optimization",
    "published_date": "2021-04-27",
    "abstract": "Distributionally robust supervised learning (DRSL) is emerging as a key paradigm for building reliable machine learning systems for real-world applications -- reflecting the need for classifiers and predictive models that are robust to the distribution shifts that arise from phenomena such as selection bias or nonstationarity. Existing algorithms for solving Wasserstein DRSL -- one of the most popular DRSL frameworks based around robustness to perturbations in the Wasserstein distance -- have serious limitations that limit their use in large-scale problems -- in particular they involve solving complex subproblems and they fail to make use of stochastic gradients. We revisit Wasserstein DRSL through the lens of min-max optimization and derive scalable and efficiently implementable stochastic extra-gradient algorithms which provably achieve faster convergence rates than existing approaches. We demonstrate their effectiveness on synthetic and real data when compared to existing DRSL approaches. Key to our results is the use of variance reduction and random reshuffling to accelerate stochastic min-max optimization, the analysis of which may be of independent interest.",
    "citation_count": 22,
    "summary": "This paper introduces novel stochastic extra-gradient algorithms for Wasserstein distributionally robust supervised learning, achieving faster convergence than existing methods by leveraging variance reduction and random reshuffling in a min-max optimization framework. These algorithms address scalability limitations of previous approaches, demonstrated effectively on various datasets."
  },
  {
    "url": "https://arxiv.org/pdf/2007.03724v1.pdf",
    "title": "Learning while Respecting Privacy and Robustness to Distributional Uncertainties and Adversarial Data",
    "published_date": "2020-07-07",
    "abstract": "Data used to train machine learning models can be adversarial--maliciously constructed by adversaries to fool the model. Challenge also arises by privacy, confidentiality, or due to legal constraints when data are geographically gathered and stored across multiple learners, some of which may hold even an \"anonymized\" or unreliable dataset. In this context, the distributionally robust optimization framework is considered for training a parametric model, both in centralized and federated learning settings. The objective is to endow the trained model with robustness against adversarially manipulated input data, or, distributional uncertainties, such as mismatches between training and testing data distributions, or among datasets stored at different workers. To this aim, the data distribution is assumed unknown, and lies within a Wasserstein ball centered around the empirical data distribution. This robust learning task entails an infinite-dimensional optimization problem, which is challenging. Leveraging a strong duality result, a surrogate is obtained, for which three stochastic primal-dual algorithms are developed: i) stochastic proximal gradient descent with an $\\epsilon$-accurate oracle, which invokes an oracle to solve the convex sub-problems; ii) stochastic proximal gradient descent-ascent, which approximates the solution of the convex sub-problems via a single gradient ascent step; and, iii) a distributionally robust federated learning algorithm, which solves the sub-problems locally at different workers where data are stored. Compared to the empirical risk minimization and federated learning methods, the proposed algorithms offer robustness with little computation overhead. Numerical tests using image datasets showcase the merits of the proposed algorithms under several existing adversarial attacks and distributional uncertainties.",
    "citation_count": 4,
    "summary": "This paper proposes distributionally robust optimization methods for training machine learning models that are robust to adversarial attacks and distributional uncertainties, both in centralized and federated learning settings, by leveraging a Wasserstein ball and developing efficient stochastic primal-dual algorithms. These algorithms demonstrate improved robustness compared to standard methods with minimal computational overhead."
  },
  {
    "url": "https://arxiv.org/abs/2309.02211",
    "title": "Distributionally Robust Machine Learning with Multi-source Data",
    "published_date": "2023-09-05",
    "abstract": "Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the convergence rate. Our proposal can be seen as a distributionally robust federated learning approach that is computationally efficient and easy to implement using arbitrary machine learning base algorithms, satisfies some privacy constraints, and has a nice interpretation of different sources' importance for predicting a given target covariate distribution. We demonstrate the performance of our proposed group distributionally robust method on simulated and real data with random forests and neural networks as base-learning algorithms.",
    "citation_count": 7,
    "summary": "This paper proposes a group distributionally robust prediction model that improves prediction accuracy under distribution shifts by using multi-source data and optimizing an adversarial reward based on explained variance. The model, a weighted average of source population models, is easily implemented with various machine learning algorithms and offers computational efficiency and privacy benefits."
  },
  {
    "url": "https://arxiv.org/pdf/2309.08825.pdf",
    "title": "Distributionally Robust Post-hoc Classifiers under Prior Shifts",
    "published_date": "2023-09-16",
    "abstract": "The generalization ability of machine learning models degrades significantly when the test distribution shifts away from the training distribution. We investigate the problem of training models that are robust to shifts caused by changes in the distribution of class-priors or group-priors. The presence of skewed training priors can often lead to the models overfitting to spurious features. Unlike existing methods, which optimize for either the worst or the average performance over classes or groups, our work is motivated by the need for finer control over the robustness properties of the model. We present an extremely lightweight post-hoc approach that performs scaling adjustments to predictions from a pre-trained model, with the goal of minimizing a distributionally robust loss around a chosen target distribution. These adjustments are computed by solving a constrained optimization problem on a validation set and applied to the model during test time. Our constrained optimization objective is inspired by a natural notion of robustness to controlled distribution shifts. Our method comes with provable guarantees and empirically makes a strong case for distributional robust post-hoc classifiers. An empirical implementation is available at https://github.com/weijiaheng/Drops.",
    "citation_count": 15,
    "summary": "This paper proposes a lightweight post-hoc method to improve the robustness of classifiers against prior shifts by scaling predictions from a pre-trained model, minimizing a distributionally robust loss around a target distribution via constrained optimization on a validation set. This approach offers finer control over robustness than existing methods and comes with theoretical guarantees."
  },
  {
    "url": "https://www.alignmentforum.org/s/dT7CKGXwq9vt76CeX/p/nM99oLhRzrmLWozoM",
    "author": "Rohin Shah",
    "title": "[AN #134]: Underspecification as a cause of fragility to distribution shift",
    "published_date": "2021-01-21",
    "summary": "This Alignment Newsletter discusses two papers: one explaining the fragility of machine learning models to distributional shifts due to \"underspecification,\" where many models fit training data equally well but vary wildly in out-of-distribution performance; and another rigorously testing whether model transparency improves prediction accuracy and understanding."
  },
  {
    "url": "https://arxiv.org/abs/2412.14080",
    "title": "On the Robustness of Distributed Machine Learning against Transfer Attacks",
    "published_date": "2024-12-18",
    "abstract": "Although distributed machine learning (distributed ML) is gaining considerable attention in the community, prior works have independently looked at instances of distributed ML in either the training or the inference phase. No prior work has examined the combined robustness stemming from distributing both the learning and the inference process. In this work, we explore, for the first time, the robustness of distributed ML models that are fully heterogeneous in training data, architecture, scheduler, optimizer, and other model parameters. Supported by theory and extensive experimental validation using CIFAR10 and FashionMNIST, we show that such properly distributed ML instantiations achieve across-the-board improvements in accuracy-robustness tradeoffs against state-of-the-art transfer-based attacks that could otherwise not be realized by current ensemble or federated learning instantiations. For instance, our experiments on CIFAR10 show that for the Common Weakness attack, one of the most powerful state-of-the-art transfer-based attacks, our method improves robust accuracy by up to 40%, with a minimal impact on clean task accuracy.",
    "summary": "This paper investigates the robustness of fully heterogeneous distributed machine learning (both training and inference) against transfer attacks, demonstrating through theoretical analysis and experiments that this approach significantly improves accuracy-robustness tradeoffs compared to existing ensemble or federated learning methods. Specifically, it achieves up to 40% improvement in robust accuracy against state-of-the-art attacks on CIFAR10 with minimal clean accuracy loss."
  },
  {
    "url": "https://arxiv.org/pdf/2307.14364.pdf",
    "title": "Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis",
    "published_date": "2023-07-25",
    "abstract": "Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity is also analyzed. Extensive empirical studies on real-world datasets demonstrate that the proposed method can not only achieve fast convergence, and remain robust against data heterogeneity as well as malicious attacks, but also tradeoff robustness with performance.",
    "citation_count": 1,
    "summary": "This paper introduces ASPIRE, an asynchronous distributed algorithm for federated distributionally robust optimization (FDRO) with non-convex objectives, using a novel constrained D-norm uncertainty set to handle data heterogeneity and malicious attacks while achieving a balance between robustness and performance. Theoretical convergence and complexity analysis are provided."
  },
  {
    "url": "https://arxiv.org/pdf/2210.07588.pdf",
    "title": "Distributed Distributionally Robust Optimization with Non-Convex Objectives",
    "published_date": "2022-10-14",
    "abstract": "Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the distributed distributionally robust optimization (DDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity is also analyzed. Extensive empirical studies on real-world datasets demonstrate that the proposed method can not only achieve fast convergence, and remain robust against data heterogeneity as well as malicious attacks, but also tradeoff robustness with performance.",
    "citation_count": 10,
    "summary": "This paper introduces ASPIRE, an asynchronous distributed algorithm for solving non-convex distributionally robust optimization problems, addressing challenges of asynchronous updates, prior distribution utilization, and robustness control. The algorithm uses a novel constrained D-norm uncertainty set and is proven to converge with analyzed iteration complexity, exhibiting robustness and performance trade-offs in empirical studies."
  },
  {
    "url": "https://www.lesswrong.com/posts/dhbLE8BqRvhBtsXhS/mlsn-3-neurips-safety-paper-roundup",
    "author": "Dan H",
    "title": "[MLSN #3]: NeurIPS Safety Paper Roundup",
    "published_date": "2022-03-08",
    "summary": "The ML Safety Newsletter's third issue summarizes recent research on improving machine learning model robustness, including findings that Vision Transformers offer greater distribution shift robustness but not inherently superior adversarial robustness to CNNs, and a new fractal-based data augmentation method (PixMix) that enhances various reliability metrics without tradeoffs. The newsletter also covers advancements in anomaly detection and the detection/creation of \"Trojan\" models with hidden vulnerabilities."
  }
]