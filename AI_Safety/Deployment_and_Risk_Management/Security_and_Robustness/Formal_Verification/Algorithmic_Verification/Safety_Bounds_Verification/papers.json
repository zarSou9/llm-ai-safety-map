[
  {
    "url": "https://www.lesswrong.com/s/3ni2P2GZzBvNebWYZ",
    "author": "markov, Charbel-RaphaÃ«l",
    "title": "AI Safety 101 - LessWrong",
    "published_date": "2023-10-20"
  },
  {
    "url": "https://www.lesswrong.com/posts/jiXMZHGmEf7qPrKPc/systems-that-cannot-be-unsafe-cannot-be-safe",
    "author": "Davidmanheim",
    "title": "Systems that cannot be unsafe cannot be safe",
    "published_date": "2023-05-02"
  },
  {
    "title": "Towards combining deep learning, verification, and scenario-based programming",
    "abstract": "Deep learning (DL) [4] is dramatically changing the world of software. The rapid improvement in deep neural network (DNN) technology now enables engineers to train models that achieve superhuman results, often surpassing algorithms that have been carefully hand-crafted by domain experts [19, 20]. There is even an intensifying trend of incorporating DNNs in safety-critical systems, e.g. as controllers for autonomous vehicles and drones [1, 12].",
    "published_date": "2021-05-18",
    "citation_count": 6,
    "url": "https://dl.acm.org/doi/10.1145/3459086.3459631"
  },
  {
    "title": "Verifiable autonomy under perceptual limitations",
    "abstract": "A recent set of algorithms in the intersection of formal methods, convex optimization and machine learning offers orders-of-magnitude improvement in the scalability of verification and synthesis in partially observable Markov decision processes possibly with uncertain transition probabilities.",
    "published_date": "2021-05-18",
    "url": "https://dl.acm.org/doi/10.1145/3459086.3459635"
  },
  {
    "title": "Verifiably safe exploration for end-to-end reinforcement learning",
    "abstract": "Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We characterize safety constraints in terms of a refinement relation on Markov decision processes - rather than directly constraining the reinforcement learning algorithm so that it only takes safe actions, we instead refine the environment so that only safe actions are defined in the environment's transition structure. This has pragmatic system design benefits and, more importantly, provides a clean conceptual setting in which we are able to prove important safety and efficiency properties. These allow us to transform the constrained optimization problem of acting safely in the original environment into an unconstrained optimization in a refined environment.",
    "published_date": "2020-07-02",
    "citation_count": 42,
    "url": "https://dl.acm.org/doi/10.1145/3447928.3456653"
  },
  {
    "title": "Safety and Robustness for Deep Learning with Provable Guarantees",
    "abstract": "Computing systems are becoming ever more complex, with decisions increasingly often based on deep learning components. A wide variety of applications are being developed, many of them safety-critical, such as self-driving cars and medical diagnosis. Since deep learning is unstable with respect to adversarial perturbations, there is a need for rigorous software development methodologies that encompass machine learning components. This lecture will describe progress with developing automated verification and testing techniques for deep neural networks to ensure safety and robustness of their decisions with respect to bounded input perturbations. The techniques exploit Lipschitz continuity of the networks and aim to approximate, for a given set of inputs, the reachable set of network outputs in terms of lower and upper bounds, in anytime manner, with provable guarantees. We develop novel algorithms based on feature-guided search, games, global optimisation and Bayesian methods, and evaluate them on state-of-the-art networks. The lecture will conclude with an overview of the challenges in this field.",
    "published_date": "2020-09-01",
    "citation_count": 4,
    "url": "https://dl.acm.org/doi/10.1145/3324884.3418901"
  }
]