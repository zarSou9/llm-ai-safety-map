[
  {
    "url": "https://arxiv.org/pdf/2104.10076.pdf",
    "title": "MixDefense: A Defense-in-Depth Framework for Adversarial Example Detection Based on Statistical and Semantic Analysis",
    "published_date": "2021-04-20",
    "abstract": "Machine learning with deep neural networks (DNNs) has become one of the foundation techniques in many safety-critical systems, such as autonomous vehicles and medical diagnosis systems. DNN-based systems, however, are known to be vulnerable to adversarial examples (AEs) that are maliciously perturbed variants of legitimate inputs. While there has been a vast body of research to defend against AE attacks in the literature, the performances of existing defense techniques are still far from satisfactory, especially for adaptive attacks, wherein attackers are knowledgeable about the defense mechanisms and craft AEs accordingly. In this work, we propose a multilayer defense-in-depth framework for AE detection, namely MixDefense. For the first layer, we focus on those AEs with large perturbations. We propose to leverage the `noise' features extracted from the inputs to discover the statistical difference between natural images and tampered ones for AE detection. For AEs with small perturbations, the inference result of such inputs would largely deviate from their semantic information. Consequently, we propose a novel learning-based solution to model such contradictions for AE detection. Both layers are resilient to adaptive attacks because there do not exist gradient propagation paths for AE generation. Experimental results with various AE attack methods on image classification datasets show that the proposed MixDefense solution outperforms the existing AE detection techniques by a considerable margin.",
    "citation_count": 1,
    "summary": "MixDefense is a multi-layered adversarial example detection framework that combines statistical analysis of image noise (for large perturbations) and semantic analysis of prediction discrepancies (for small perturbations) to achieve robust defense against adaptive attacks. Experimental results demonstrate its superior performance compared to existing techniques."
  },
  {
    "url": "https://arxiv.org/pdf/2105.08620.pdf",
    "title": "Adversarial Examples Detection With Bayesian Neural Network",
    "published_date": "2021-05-18",
    "abstract": "In this paper, we propose a new framework to detect adversarial examples motivated by the observations that random components can improve the smoothness of predictors and make it easier to simulate the output distribution of a deep neural network. With these observations, we propose a novel Bayesian adversarial example detector, short for BATer, to improve the performance of adversarial example detection. Specifically, we study the distributional difference of hidden layer output between natural and adversarial examples, and propose to use the randomness of the Bayesian neural network to simulate hidden layer output distribution and leverage the distribution dispersion to detect adversarial examples. The advantage of a Bayesian neural network is that the output is stochastic while a deep neural network without random components does not have such characteristics. Empirical results on several benchmark datasets against popular attacks show that the proposed BATer outperforms the state-of-the-art detectors in adversarial example detection.",
    "citation_count": 4,
    "summary": "BATer, a novel Bayesian adversarial example detector, leverages the stochastic output of Bayesian neural networks to model the distributional differences in hidden layer outputs between natural and adversarial examples, achieving state-of-the-art performance in detection. This method exploits the inherent randomness of Bayesian networks to better simulate and distinguish these distributions."
  },
  {
    "url": "https://arxiv.org/pdf/2102.11586v1.pdf",
    "title": "Adversarial Examples Detection Beyond Image Space",
    "published_date": "2021-02-23",
    "abstract": "Deep neural networks have been proved that they are vulnerable to adversarial examples, which are generated by adding human-imperceptible perturbations to images. To defend these adversarial examples, various detection based methods have been proposed. However, most of them perform poorly on detecting adversarial examples with extremely slight perturbations. By exploring these adversarial examples, we find that there exists compliance between perturbations and prediction confidence, which guides us to detect few-perturbation attacks from the aspect of prediction confidence. To detect both few-perturbation attacks and large-perturbation attacks, we propose a method beyond image space by a two-stream architecture, in which the image stream focuses on the pixel artifacts and the gradient stream copes with the confidence artifacts. The experimental results show that the proposed method outperforms the existing methods under oblivious attacks and is verified effective to defend omniscient attacks as well.",
    "citation_count": 8,
    "summary": "This paper introduces a novel adversarial example detection method using a two-stream architecture that analyzes both image pixel artifacts and prediction confidence, effectively detecting both large and small perturbations, outperforming existing methods in both oblivious and omniscient attack scenarios."
  },
  {
    "title": "Real-time detectors for digital and physical adversarial inputs to perception systems",
    "abstract": "Deep neural network (DNN) models have proven to be vulnerable to adversarial digital and physical attacks. In this paper, we propose a novel attack- and dataset-agnostic and real-time detector for both types of adversarial inputs to DNN-based perception systems. In particular, the proposed detector relies on the observation that adversarial images are sensitive to certain label-invariant transformations. Specifically, to determine if an image has been adversarially manipulated, the proposed detector checks if the output of the target classifier on a given input image changes significantly after feeding it a transformed version of the image under investigation. Moreover, we show that the proposed detector is computationally-light both at runtime and design-time which makes it suitable for real-time applications that may also involve large-scale image domains. To highlight this, we demonstrate the efficiency of the proposed detector on ImageNet, a task that is computationally challenging for the majority of relevant defenses, and on physically attacked traffic signs that may be encountered in real-time autonomy applications. Finally, we propose the first adversarial dataset, called AdvNet that includes both clean and physical traffic sign images. Our extensive comparative experiments on the MNIST, CIFAR10, ImageNet, and AdvNet datasets show that VisionGuard outperforms existing defenses in terms of scalability and detection performance. We have also evaluated the proposed detector on field test data obtained on a moving vehicle equipped with a perception-based DNN being under attack.",
    "published_date": "2020-02-23",
    "citation_count": 10,
    "url": "https://dl.acm.org/doi/10.1145/3450267.3450535",
    "summary": "This paper introduces VisionGuard, a real-time, computationally efficient detector for both digital and physical adversarial attacks on deep neural networks, which works by assessing the classifier's output sensitivity to label-invariant transformations of the input image. Its effectiveness is demonstrated across various datasets, including a newly created physical adversarial traffic sign dataset (AdvNet), and in real-world field tests."
  },
  {
    "title": "Towards Certifiable Adversarial Sample Detection",
    "abstract": "Convolutional Neural Networks (CNNs) are deployed in more and more classification systems, but adversarial samples can be maliciously crafted to trick them, and are becoming a real threat. There have been various proposals to improve CNNs' adversarial robustness but these all suffer performance penalties or have other limitations. In this paper, we offer a new approach in the form of a certifiable adversarial detection scheme, the Certifiable Taboo Trap (CTT). This system, in theory, can provide certifiable guarantees of detectability of a range of adversarial inputs for certain l-∞ sizes. We develop and evaluate several versions of CTT with different defense capabilities, training overheads and certifiability on adversarial samples. In practice, against adversaries with various l-p norms, CTT outperforms existing defense methods that focus purely on improving network robustness. We show that CTT has small false positive rates on clean test data, minimal compute overheads when deployed, and can support complex security policies.",
    "published_date": "2020-02-20",
    "citation_count": 13,
    "url": "https://dl.acm.org/doi/10.1145/3411508.3421381",
    "summary": "The Certifiable Taboo Trap (CTT) offers a novel approach to adversarial sample detection, providing theoretical certifiability for detecting a range of adversarial inputs within specific l-∞ bounds and practically outperforming existing defense methods with low false positives and minimal computational overhead."
  },
  {
    "title": "A Statistical Defense Approach for Detecting Adversarial Examples",
    "abstract": "Adversarial examples are maliciously modified inputs created to fool Machine Learning algorithms (ML). The existence of such inputs presents a major issue to the expansion of ML-based solutions. Many researchers have already contributed to the topic, providing both cutting edge-attack techniques and various defense strategies. This work focuses on the development of a system capable of detecting adversarial samples by exploiting statistical information from the training-set. Our detector computes several distorted replicas of the test input, then collects the classifier's prediction vectors to build a meaningful signature for the detection task. Then, the signature is projected onto a class-specific statistic vector to infer the input's nature. The class predicted for the original input is used to select the class-statistic vector. We show that our method reliably detects malicious inputs, outperforming state-of-the-art approaches in various settings, while being complementary to other defense solutions.",
    "published_date": "2019-08-26",
    "citation_count": 4,
    "url": "https://dl.acm.org/doi/10.1145/3415048.3416103",
    "summary": "This paper presents a novel adversarial example detection system that leverages statistical information from the training set by analyzing the classifier's prediction vectors of distorted test inputs. This method outperforms existing approaches by projecting a signature of these vectors onto class-specific statistics to infer the input's nature."
  },
  {
    "title": "MetaAdvDet: Towards Robust Detection of Evolving Adversarial Attacks",
    "abstract": "Deep neural networks (DNNs) are vulnerable to the adversarial attack which is maliciously implemented by adding human-imperceptible perturbation to images and thus leads to incorrect prediction. Existing studies have proposed various methods to detect the new adversarial attacks. However, new attack methods keep evolving constantly and yield new adversarial examples to bypass the existing detectors. It needs to collect tens of thousands samples to train detectors, while the new attacks evolve much more frequently than the high-cost data collection. Thus, this situation leads the newly evolved attack samples to remain in small scales. To solve such few-shot problem with the evolving attacks, we propose a meta-learning based robust detection method to detect new adversarial attacks with limited examples. Specifically, the learning consists of a double-network framework: a task-dedicated network and a master network which alternatively learn the detection capability for either seen attack or a new attack. To validate the effectiveness of our approach, we construct the benchmarks with few-shot-fashion protocols based on three conventional datasets, i.e. CIFAR-10, MNIST and Fashion-MNIST. Comprehensive experiments are conducted on them to verify the superiority of our approach with respect to the traditional adversarial attack detection methods. The implementation code is available online.",
    "published_date": "2019-08-06",
    "citation_count": 17,
    "url": "https://dl.acm.org/doi/10.1145/3343031.3350887",
    "summary": "MetaAdvDet addresses the challenge of detecting evolving adversarial attacks on deep neural networks by using meta-learning to train a robust detector with limited examples of new attacks, improving detection capabilities for both known and unseen attack types. This approach leverages a double-network framework for efficient adaptation to novel adversarial examples."
  },
  {
    "url": "https://arxiv.org/abs/1705.07263v1",
    "title": "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods",
    "published_date": "2017-05-20",
    "abstract": "Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.",
    "citation_count": 1786,
    "summary": "The paper demonstrates that ten existing adversarial example detection methods are easily bypassed using novel loss functions, revealing that detecting adversarial examples is significantly more challenging than previously thought. This challenges the assumed intrinsic properties of adversarial examples."
  }
]