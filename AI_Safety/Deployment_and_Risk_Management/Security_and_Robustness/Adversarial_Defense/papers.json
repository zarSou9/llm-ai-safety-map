[
  {
    "url": "https://arxiv.org/abs/1706.06083v4",
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "published_date": "2017-06-19",
    "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
    "citation_count": 11084,
    "summary": "This paper investigates adversarial robustness in deep neural networks using robust optimization, developing methods for training networks significantly more resistant to a wide range of attacks and offering a concrete security guarantee against first-order adversaries. The approach provides a unifying framework for prior work and offers improved resistance to adversarial examples."
  },
  {
    "url": "https://arxiv.org/auth/show-endorsers/1803.00404",
    "title": "Deep Defense: Training DNNs with Improved Adversarial Robustness",
    "published_date": "2018-02-01",
    "abstract": "Despite the efficacy on a variety of computer vision tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks, limiting their applications in security-critical systems. Recent works have shown the possibility of generating imperceptibly perturbed image inputs (a.k.a., adversarial examples) to fool well-trained DNN classifiers into making arbitrary predictions. To address this problem, we propose a training recipe named \"deep defense\". Our core idea is to integrate an adversarial perturbation-based regularizer into the classification objective, such that the obtained models learn to resist potential attacks, directly and precisely. The whole optimization problem is solved just like training a recursive network. Experimental results demonstrate that our method outperforms training with adversarial/Parseval regularizations by large margins on various datasets (including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code and models for reproducing our results are available at this https URL",
    "citation_count": 103,
    "summary": "Deep Defense enhances the adversarial robustness of deep neural networks (DNNs) by incorporating an adversarial perturbation-based regularizer into the training objective, achieving superior performance compared to existing adversarial training methods across multiple datasets and architectures. This approach directly and precisely improves the model's resistance to adversarial attacks."
  },
  {
    "title": "Interpolated Adversarial Training: Achieving Robust Neural Networks Without Sacrificing Too Much Accuracy",
    "abstract": "Adversarial robustness has become a central goal in deep learning, both in theory and in practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how achieving adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error (when there is no adversary) from 4.43% to 12.32%, whereas with our Interpolated adversarial training we retain adversarial robustness while achieving a standard test error of only 6.45%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1% to just 45.5%.",
    "published_date": "2019-06-16",
    "citation_count": 93,
    "url": "https://dl.acm.org/doi/10.1145/3338501.3357369",
    "summary": "Interpolated adversarial training improves the adversarial robustness of neural networks without significantly sacrificing accuracy on clean data, unlike standard adversarial training which substantially increases standard test error. This method achieves this by incorporating interpolation-based training techniques within the adversarial training framework."
  },
  {
    "url": "https://arxiv.org/abs/2310.02480",
    "title": "Splitting the Difference on Adversarial Training",
    "published_date": "2023-10-03",
    "abstract": "The existence of adversarial examples points to a basic weakness of deep neural networks. One of the most effective defenses against such examples, adversarial training, entails training models with some degree of robustness, usually at the expense of a degraded natural accuracy. Most adversarial training methods aim to learn a model that finds, for each class, a common decision boundary encompassing both the clean and perturbed examples. In this work, we take a fundamentally different approach by treating the perturbed examples of each class as a separate class to be learned, effectively splitting each class into two classes:\"clean\"and\"adversarial.\"This split doubles the number of classes to be learned, but at the same time considerably simplifies the decision boundaries. We provide a theoretical plausibility argument that sheds some light on the conditions under which our approach can be expected to be beneficial. Likewise, we empirically demonstrate that our method learns robust models while attaining optimal or near-optimal natural accuracy, e.g., on CIFAR-10 we obtain near-optimal natural accuracy of $95.01\\%$ alongside significant robustness across multiple tasks. The ability to achieve such near-optimal natural accuracy, while maintaining a significant level of robustness, makes our method applicable to real-world applications where natural accuracy is at a premium. As a whole, our main contribution is a general method that confers a significant level of robustness upon classifiers with only minor or negligible degradation of their natural accuracy.",
    "citation_count": 2,
    "summary": "This paper proposes a novel adversarial training method that splits each class into \"clean\" and \"adversarial\" subclasses, simplifying decision boundaries and improving robustness. This approach achieves near-optimal natural accuracy while maintaining significant robustness against adversarial examples, unlike traditional methods which often trade off accuracy for robustness."
  },
  {
    "url": "https://arxiv.org/abs/2302.10980v2",
    "title": "MultiRobustBench: Benchmarking Robustness Against Multiple Attacks",
    "published_date": "2023-02-21",
    "abstract": "The bulk of existing research in defending against adversarial examples focuses on defending against a single (typically bounded Lp-norm) attack, but for a practical setting, machine learning (ML) models should be robust to a wide variety of attacks. In this paper, we present the first unified framework for considering multiple attacks against ML models. Our framework is able to model different levels of learner's knowledge about the test-time adversary, allowing us to model robustness against unforeseen attacks and robustness against unions of attacks. Using our framework, we present the first leaderboard, MultiRobustBench, for benchmarking multiattack evaluation which captures performance across attack types and attack strengths. We evaluate the performance of 16 defended models for robustness against a set of 9 different attack types, including Lp-based threat models, spatial transformations, and color changes, at 20 different attack strengths (180 attacks total). Additionally, we analyze the state of current defenses against multiple attacks. Our analysis shows that while existing defenses have made progress in terms of average robustness across the set of attacks used, robustness against the worst-case attack is still a big open problem as all existing models perform worse than random guessing.",
    "citation_count": 7,
    "summary": "MultiRobustBench introduces a unified framework and leaderboard for evaluating machine learning model robustness against multiple adversarial attacks, revealing that while average robustness has improved, performance against the worst-case attack remains significantly deficient."
  },
  {
    "url": "https://www.lesswrong.com/posts/timk6zHDTFdrHYLmu/adversarial-robustness-could-help-prevent-catastrophic",
    "author": "aogara",
    "title": "Adversarial Robustness Could Help Prevent Catastrophic Misuse",
    "published_date": "2023-12-11",
    "summary": "The article argues that adversarial robustness in AI models is crucial to mitigating catastrophic misuse, such as AI-assisted bioterrorism, because current models, despite safety training, remain vulnerable to adversarial attacks that circumvent their safety protocols. While acknowledging the difficulty of achieving robust defenses, the author advocates for increased research and funding in this area as a key component of AI safety."
  },
  {
    "url": "https://arxiv.org/pdf/2103.07633v1.pdf",
    "title": "Attack as defense: characterizing adversarial examples using robustness",
    "published_date": "2021-03-13",
    "abstract": "As a new programming paradigm, deep learning has expanded its application to many real-world problems. At the same time, deep learning based software are found to be vulnerable to adversarial attacks. Though various defense mechanisms have been proposed to improve robustness of deep learning software, many of them are ineffective against adaptive attacks. In this work, we propose a novel characterization to distinguish adversarial examples from benign ones based on the observation that adversarial examples are significantly less robust than benign ones. As existing robustness measurement does not scale to large networks, we propose a novel defense framework, named attack as defense (A2D), to detect adversarial examples by effectively evaluating an example's robustness. A2D uses the cost of attacking an input for robustness evaluation and identifies those less robust examples as adversarial since less robust examples are easier to attack. Extensive experiment results on MNIST, CIFAR10 and ImageNet show that A2D is more effective than recent promising approaches. We also evaluate our defense against potential adaptive attacks and show that A2D is effective in defending carefully designed adaptive attacks, e.g., the attack success rate drops to 0% on CIFAR10.",
    "citation_count": 29,
    "summary": "The paper introduces \"attack as defense\" (A2D), a novel adversarial example detection framework that leverages the lower robustness of adversarial examples compared to benign ones. A2D efficiently assesses robustness by measuring the cost of attacking an input, effectively identifying and defending against adversarial examples, even adaptive ones."
  },
  {
    "title": "Attack as defense: characterizing adversarial examples using robustness",
    "abstract": "As a new programming paradigm, deep learning has expanded its application to many real-world problems. At the same time, deep learning based software are found to be vulnerable to adversarial attacks. Though various defense mechanisms have been proposed to improve robustness of deep learning software, many of them are ineffective against adaptive attacks. In this work, we propose a novel characterization to distinguish adversarial examples from benign ones based on the observation that adversarial examples are significantly less robust than benign ones. As existing robustness measurement does not scale to large networks, we propose a novel defense framework, named attack as defense (A2D), to detect adversarial examples by effectively evaluating an example's robustness. A2D uses the cost of attacking an input for robustness evaluation and identifies those less robust examples as adversarial since less robust examples are easier to attack. Extensive experiment results on MNIST, CIFAR10 and ImageNet show that A2D is more effective than recent promising approaches. We also evaluate our defense against potential adaptive attacks and show that A2D is effective in defending carefully designed adaptive attacks, e.g., the attack success rate drops to 0% on CIFAR10.",
    "published_date": "2021-03-13",
    "citation_count": 29,
    "url": "https://dl.acm.org/doi/10.1145/3460319.3464822",
    "summary": "The paper introduces \"Attack as Defense\" (A2D), a novel adversarial example detection framework that leverages the observation that adversarial examples are less robust than benign ones. A2D efficiently evaluates robustness by measuring the cost of attacking an input, effectively identifying adversarial examples and demonstrating superior performance against adaptive attacks compared to existing defenses."
  },
  {
    "url": "https://arxiv.org/pdf/2002.08439v1.pdf",
    "title": "AdvMS: A Multi-Source Multi-Cost Defense Against Adversarial Attacks",
    "published_date": "2020-02-19",
    "abstract": "Designing effective defense against adversarial attacks is a crucial topic as deep neural networks have been proliferated rapidly in many security-critical domains such as malware detection and self-driving cars. Conventional defense methods, although shown to be promising, are largely limited by their single-source single-cost nature: The robustness promotion tends to plateau when the defenses are made increasingly stronger while the cost tends to amplify. In this paper, we study principles of designing multi-source and multi-cost schemes where defense performance is boosted from multiple defending components. Based on this motivation, we propose a multi-source and multi-cost defense scheme, Adversarially Trained Model Switching (AdvMS), that inherits advantages from two leading schemes: adversarial training and random model switching. We show that the multi-source nature of AdvMS mitigates the performance plateauing issue and the multi-cost nature enables improving robustness at a flexible and adjustable combination of costs over different factors which can better suit specific restrictions and needs in practice.",
    "citation_count": 3,
    "summary": "AdvMS is a novel defense against adversarial attacks on deep neural networks that combines adversarial training and random model switching, mitigating performance plateaus by leveraging multiple defense sources and offering flexible cost control across various factors. This multi-source, multi-cost approach improves robustness while adapting to practical constraints."
  },
  {
    "url": "https://arxiv.org/abs/2010.09670v1",
    "title": "RobustBench: a standardized adversarial robustness benchmark",
    "published_date": "2020-10-19",
    "abstract": "Evaluation of adversarial robustness is often error-prone leading to overestimation of the true robustness of models. While adaptive attacks designed for a particular defense are a way out of this, there are only approximate guidelines on how to perform them. Moreover, adaptive evaluations are highly customized for particular models, which makes it difficult to compare different defenses. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. This requires to impose some restrictions on the admitted models to rule out defenses that only make gradient-based attacks ineffective without improving actual robustness. We evaluate robustness of models for our benchmark with AutoAttack, an ensemble of white- and black-box attacks which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. Our leaderboard, hosted at this http URL, aims at reflecting the current state of the art on a set of well-defined tasks in $\\ell_\\infty$- and $\\ell_2$-threat models with possible extensions in the future. Additionally, we open-source the library this http URL that provides unified access to state-of-the-art robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze general trends in $\\ell_p$-robustness and its impact on other tasks such as robustness to various distribution shifts and out-of-distribution detection.",
    "citation_count": 609,
    "summary": "RobustBench is a standardized benchmark for evaluating adversarial robustness that uses AutoAttack, a strong ensemble of attacks, to provide more accurate and comparable results across different defense methods, thus addressing the limitations of previous, often overoptimistic, evaluations. The benchmark includes a leaderboard and open-source library to facilitate research and application of robust models."
  }
]