[
  {
    "url": "https://www.lesswrong.com/posts/timk6zHDTFdrHYLmu/adversarial-robustness-could-help-prevent-catastrophic",
    "author": "aogara",
    "title": "Adversarial Robustness Could Help Prevent Catastrophic Misuse",
    "published_date": "2023-12-11"
  },
  {
    "title": "Interpolated Adversarial Training: Achieving Robust Neural Networks Without Sacrificing Too Much Accuracy",
    "abstract": "Adversarial robustness has become a central goal in deep learning, both in theory and in practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how achieving adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error (when there is no adversary) from 4.43% to 12.32%, whereas with our Interpolated adversarial training we retain adversarial robustness while achieving a standard test error of only 6.45%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1% to just 45.5%.",
    "published_date": "2019-06-16",
    "citation_count": 93,
    "url": "https://dl.acm.org/doi/10.1145/3338501.3357369"
  },
  {
    "title": "On Improving the Effectiveness of Adversarial Training",
    "abstract": "Machine learning models, including neural networks, are vulnerable to adversarial examples, which are adversarial inputs generated from legitimate examples by applying small perturbations to fool machine learning models to misclassify. Algorithms that are used to generate adversarial examples are called adversarial example generation methods. As the state-of-the-art defense approach, adversarial training improves the robustness of machine learning models by augmenting the training data with adversarial examples. However, adversarial training is far from being perfect yet, and a deeper understanding of it is always needed for further improving its effectiveness. In this paper, we propose to investigate two research questions. The first question is: whether Method-Based Ensemble Adversarial Training (MBEAT) could be beneficial, i.e., whether leveraging the adversarial examples generated by multiple methods could help increase the effectiveness of adversarial training. The second question is: whether Round Gap Of Adversarial Training (RGOAT) could exist, i.e., whether a neural network model adversarially trained in one round would not be robust against the adversarial examples further generated from this model itself. We design an adversarial training experimental framework to answer these two research questions. We find that MBEAT is indeed beneficial, indicating that it has some important value in practice. We also find that RGOAT indeed exists, indicating that adversarial training should be an iterative process.",
    "published_date": "2019-03-13",
    "citation_count": 6,
    "url": "https://dl.acm.org/doi/10.1145/3309182.3309190"
  },
  {
    "title": "Spartan Networks: Self-Feature-Squeezing Networks for Increased Robustness in Adversarial Settings",
    "abstract": "Deep Learning Models are vulnerable to adversarial inputs, samples modified in order to maximize error of the system. We hereby introduce Spartan Networks, Deep Learning models that are inherently more resistant to adverarial examples, without doing any input preprocessing out of the network or adversarial training. These networks have an adversarial layer within the network designed to starve the network of information, using a new activation function to discard data. This layer trains the neural network to filter-out usually-irrelevant parts of its input. These models thus have a slightly lower precision, but report a higher robustness under attack than unprotected models.",
    "published_date": "2018-10-08",
    "citation_count": 2,
    "url": "https://dl.acm.org/doi/10.1145/3243734.3278486"
  },
  {
    "title": "Making machine learning robust against adversarial inputs",
    "abstract": "Such inputs distort how machine-learning-based systems are able to function in the world as it is.",
    "published_date": "2018-06-25",
    "citation_count": 330,
    "url": "https://dl.acm.org/doi/10.1145/3134599"
  }
]