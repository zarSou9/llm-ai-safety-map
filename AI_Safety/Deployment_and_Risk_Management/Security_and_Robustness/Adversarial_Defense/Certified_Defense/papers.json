[
  {
    "url": "https://arxiv.org/abs/1902.02918v1",
    "title": "Certified Adversarial Robustness via Randomized Smoothing",
    "published_date": "2019-02-08",
    "abstract": "We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at this http URL.",
    "citation_count": 1872,
    "summary": "Randomized smoothing transforms classifiers robust to Gaussian noise into certifiably robust classifiers against $\\ell_2$-norm adversarial perturbations, achieving state-of-the-art certified accuracy on ImageNet and outperforming other methods on smaller datasets. This improved robustness guarantee is based on a tighter theoretical bound for Gaussian noise smoothing."
  },
  {
    "url": "https://arxiv.org/abs/2002.10733v1",
    "title": "(De)Randomized Smoothing for Certifiable Defense against Patch Attacks",
    "published_date": "2020-02-25",
    "abstract": "Patch adversarial attacks on images, in which the attacker can distort pixels within a region of bounded size, are an important threat model since they provide a quantitative model for physical adversarial attacks. In this paper, we introduce a certifiable defense against patch attacks that guarantees for a given image and patch attack size, no patch adversarial examples exist. Our method is related to the broad class of randomized smoothing robustness schemes which provide high-confidence probabilistic robustness certificates. By exploiting the fact that patch attacks are more constrained than general sparse attacks, we derive meaningfully large robustness certificates. Additionally, the algorithm we propose is de-randomized, providing deterministic certificates. To the best of our knowledge, there exists only one prior method for certifiable defense against patch attacks, which relies on interval bound propagation. While this sole existing method performs well on MNIST, it has several limitations: it requires computationally expensive training, does not scale to ImageNet, and performs poorly on CIFAR-10. In contrast, our proposed method effectively addresses all of these issues: our classifier can be trained quickly, achieves high clean and certified robust accuracy on CIFAR-10, and provides certificates at the ImageNet scale. For example, for a 5*5 patch attack on CIFAR-10, our method achieves up to around 57.8% certified accuracy (with a classifier around 83.9% clean accuracy), compared to at most 30.3% certified accuracy for the existing method (with a classifier with around 47.8% clean accuracy), effectively establishing a new state-of-the-art. Code is available at this https URL.",
    "citation_count": 142,
    "summary": "This paper presents a novel, derandomized smoothing technique for certifiably defending against patch adversarial attacks, achieving significantly higher certified accuracy than existing methods on CIFAR-10 and scaling effectively to ImageNet. The method provides deterministic robustness guarantees against attacks within a bounded region."
  },
  {
    "url": "https://arxiv.org/pdf/2001.02378.pdf",
    "title": "MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius",
    "published_date": "2020-01-08",
    "abstract": "Adversarial training is one of the most popular ways to learn robust models but is usually attack-dependent and time costly. In this paper, we propose the MACER algorithm, which learns robust models without using adversarial training but performs better than all existing provable l2-defenses. Recent work shows that randomized smoothing can be used to provide certified l2 radius to smoothed classifiers, and our algorithm trains provably robust smoothed classifiers via MAximizing the CErtified Radius (MACER). The attack-free characteristic makes MACER faster to train and easier to optimize. In our experiments, we show that our method can be applied to modern deep neural networks on a wide range of datasets, including Cifar-10, ImageNet, MNIST, and SVHN. For all tasks, MACER spends less training time than state-of-the-art adversarial training algorithms, and the learned models achieve larger average certified radius.",
    "citation_count": 168,
    "summary": "MACER is a novel robust training method that maximizes the certified l2 radius of smoothed classifiers without adversarial training, resulting in faster training and superior performance compared to existing provable defenses. This attack-free approach achieves larger average certified radii on various datasets than state-of-the-art adversarial training methods."
  },
  {
    "url": "https://arxiv.org/abs/2304.10446",
    "title": "Certified Adversarial Robustness Within Multiple Perturbation Bounds",
    "published_date": "2023-04-20",
    "abstract": "Randomized smoothing (RS) is a well known certified defense against adversarial attacks, which creates a smoothed classifier by predicting the most likely class under random noise perturbations of inputs during inference. While initial work focused on robustness to ℓ2 norm perturbations using noise sampled from a Gaussian distribution, subsequent works have shown that different noise distributions can result in robustness to other ℓp norm bounds as well. In general, a specific noise distribution is optimal for defending against a given ℓp norm based attack. In this work, we aim to improve the certified adversarial robustness against multiple perturbation bounds simultaneously. Towards this, we firstly present a novel certification scheme, that effectively combines the certificates obtained using different noise distributions to obtain optimal results against multiple perturbation bounds. We further propose a novel training noise distribution along with a regularized training scheme to improve the certification within both ℓ1 and ℓ2 perturbation norms simultaneously. Contrary to prior works, we compare the certified robustness of different training algorithms across the same natural (clean) accuracy, rather than across fixed noise levels used for training and certification. We also empirically invalidate the argument that training and certifying the classifier with the same amount of noise gives the best results. The proposed approach achieves improvements on the ACR (Average Certified Radius) metric across both ℓ1 and ℓ2 perturbation bounds. Code available at https://github.com/valiisc/NU-Certified-Robustness",
    "citation_count": 3,
    "summary": "This paper introduces a novel certification scheme that improves certified adversarial robustness against multiple (ℓ1 and ℓ2) perturbation bounds simultaneously by combining certificates from different noise distributions and using a novel regularized training approach. The authors demonstrate improved average certified radius (ACR) across both bounds while controlling for clean accuracy, challenging the assumption that matching training and certification noise levels is optimal."
  },
  {
    "url": "https://arxiv.org/pdf/2309.00879.pdf",
    "title": "Towards Certified Probabilistic Robustness with High Accuracy",
    "published_date": "2023-09-02",
    "abstract": "Adversarial examples pose a security threat to many critical systems built on neural networks (such as face recognition systems, and self-driving cars). While many methods have been proposed to build robust models, how to build certifiably robust yet accurate neural network models remains an open problem. For example, adversarial training improves empirical robustness, but they do not provide certification of the model's robustness. On the other hand, certified training provides certified robustness but at the cost of a significant accuracy drop. In this work, we propose a novel approach that aims to achieve both high accuracy and certified probabilistic robustness. Our method has two parts, i.e., a probabilistic robust training method with an additional goal of minimizing variance in terms of divergence and a runtime inference method for certified probabilistic robustness of the prediction. The latter enables efficient certification of the model's probabilistic robustness at runtime with statistical guarantees. This is supported by our training objective, which minimizes the variance of the model's predictions in a given vicinity, derived from a general definition of model robustness. Our approach works for a variety of perturbations and is reasonably efficient. Our experiments on multiple models trained on different datasets demonstrate that our approach significantly outperforms existing approaches in terms of both certification rate and accuracy.",
    "summary": "This paper introduces a novel method for training certifiably robust neural networks that achieves high accuracy by minimizing prediction variance through a probabilistic robust training approach and a runtime inference method for certified probabilistic robustness. This dual approach significantly improves both certification rate and accuracy compared to existing methods."
  },
  {
    "url": "https://arxiv.org/pdf/2206.10550.pdf",
    "title": "(Certified!!) Adversarial Robustness for Free!",
    "published_date": "2022-06-21",
    "abstract": "In this paper we show how to achieve state-of-the-art certified adversarial robustness to 2-norm bounded perturbations by relying exclusively on off-the-shelf pretrained models. To do so, we instantiate the denoised smoothing approach of Salman et al. 2020 by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This allows us to certify 71% accuracy on ImageNet under adversarial perturbations constrained to be within an 2-norm of 0.5, an improvement of 14 percentage points over the prior certified SoTA using any approach, or an improvement of 30 percentage points over denoised smoothing. We obtain these results using only pretrained diffusion models and image classifiers, without requiring any fine tuning or retraining of model parameters.",
    "citation_count": 128,
    "summary": "This paper achieves state-of-the-art certified adversarial robustness on ImageNet by applying denoised smoothing with pretrained denoising diffusion probabilistic models and classifiers, requiring no model retraining. This yields a 71% certified accuracy under ℓ₂-norm bounded perturbations of 0.5, significantly surpassing previous methods."
  },
  {
    "url": "https://arxiv.org/pdf/2011.14085.pdf",
    "title": "Deterministic Certification to Adversarial Attacks via Bernstein Polynomial Approximation",
    "published_date": "2020-11-28",
    "abstract": "Randomized smoothing has established state-of-the-art provable robustness against $\\ell_2$ norm adversarial attacks with high probability. However, the introduced Gaussian data augmentation causes a severe decrease in natural accuracy. We come up with a question, \"Is it possible to construct a smoothed classifier without randomization while maintaining natural accuracy?\". We find the answer is definitely yes. We study how to transform any classifier into a certified robust classifier based on a popular and elegant mathematical tool, Bernstein polynomial. Our method provides a deterministic algorithm for decision boundary smoothing. We also introduce a distinctive approach of norm-independent certified robustness via numerical solutions of nonlinear systems of equations. Theoretical analyses and experimental results indicate that our method is promising for classifier smoothing and robustness certification.",
    "citation_count": 1,
    "summary": "This paper introduces a deterministic method for certifying robustness against adversarial attacks using Bernstein polynomial approximation, avoiding the accuracy loss associated with randomized smoothing techniques. The approach offers both a deterministic algorithm for smoothing decision boundaries and a norm-independent certification method."
  },
  {
    "url": "https://arxiv.org/abs/1801.09344v1",
    "title": "Certified Defenses against Adversarial Examples",
    "published_date": "2018-01-29",
    "abstract": "While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \\epsilon = 0.1 can cause more than 35% test error.",
    "citation_count": 945,
    "summary": "This paper presents a certified defense against adversarial examples in single-hidden-layer neural networks, using a semidefinite relaxation to generate certificates bounding the maximum error under attack, and incorporating this certificate into network training for enhanced robustness. This approach achieves a certified 35% maximum test error on MNIST under 0.1 pixel perturbations."
  }
]