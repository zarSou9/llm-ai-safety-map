[
  {
    "url": "https://arxiv.org/abs/2406.08221",
    "title": "FAIL: Analyzing Software Failures from the News Using LLMs",
    "published_date": "2024-06-12",
    "abstract": "Software failures inform engineering work, standards, regulations. For example, the Log4J vulnerability brought government and industry attention to evaluating and securing software supply chains. Retrospective failure analysis is thus a valuable line of software engineering research. Accessing private engineering records is difficult, so such analyses tend to use information reported by the news media. However, prior works in this direction have relied on manual analysis. That has limited the scale of their analyses. The community lacks automated support to enable such analyses to consider a wide range of news sources and incidents.To fill this gap, we propose the Failure Analysis Investigation with LLMs (FAIL) system. FAIL is a novel LLM-based pipeline that collects, analyzes, and summarizes software failures as reported in the news. FAIL groups articles that describe the same incidents. It then analyzes incidents using existing taxonomies for postmortems, faults, and system characteristics. To tune and evaluate FAIL, we followed the methods of prior works by manually analyzing 31 software failures. FAIL achieved an F1 score of 90% for collecting news about software failures, a V-measure of 0.98 for merging articles reporting on the same incident, and extracted 90% of the facts about failures. We then applied FAIL to a total of 137,427 news articles from 11 providers published between 2010 and 2022. FAIL identified and analyzed 2,457 distinct failures reported across 4,184 articles. Our findings include: (1) current generation of large language models are capable of identifying news articles that describe failures, and analyzing them according to structured taxonomies; (2) high recurrences of similar failures within organizations and across organizations; and (3) severity of the consequences of software failures have increased over the past decade. The full FAIL database is available so that researchers, engineers, and policymakers can learn from a diversity of software failures.CCS CONCEPTS• Software and its engineering → Software defect analysis.",
    "citation_count": 2,
    "summary": "FAIL is a novel LLM-based system that automatically analyzes news articles to identify, categorize, and summarize software failures, enabling large-scale retrospective analysis of incidents and revealing trends in failure types and severity over time. The system achieved high accuracy in identifying and analyzing failures, producing a publicly available database of 2,457 distinct software failures."
  },
  {
    "url": "https://arxiv.org/pdf/2210.08667.pdf",
    "title": "From Function to Failure",
    "published_date": "2022-10-17",
    "abstract": "Failure Mode Reasoning (FMR) is a method for formal analysis of system-related faults. The method was originally developed for identifying failure modes of safety-critical systems based on an analysis of their programs. In this paper, we generalize the method and present a mathematical framework for its use in model-based system and safety analyses. We explain the concepts, formalize the method, formulate models for example systems, and discuss the practical application of the method.",
    "summary": "This paper generalizes Failure Mode Reasoning (FMR), a method for analyzing system faults, providing a mathematical framework applicable to model-based system and safety analyses. The authors formalize the method, illustrate it with examples, and discuss its practical applications."
  },
  {
    "url": "https://arxiv.org/pdf/2209.02930.pdf",
    "title": "Reflections on software failure analysis",
    "published_date": "2022-09-07",
    "abstract": "Failure studies are important in revealing the root causes, behaviors, and life cycle of defects in software systems. These studies either focus on understanding the characteristics of defects in specific classes of systems, or the characteristics of a specific type of defect in the systems it manifests in. Failure studies have influenced various software engineering research directions, especially in the area of software evolution, defect detection, and program repair. In this paper, we reflect on the conduct of failure studies in software engineering. We reviewed a sample of 52 failure study papers. We identified several recurring problems in these studies, some of which hinder the ability of software engineering community to trust or replicate the results. Based on our findings, we suggest future research directions, including identifying and analyzing failure causal chains, standardizing the conduct of failure studies, and tool support for faster defect analysis.",
    "citation_count": 11,
    "summary": "This paper analyzes 52 software failure studies, identifying recurring methodological flaws that hinder reproducibility and trust in their findings, and proposes improvements including standardized methods, causal chain analysis, and dedicated tool support."
  },
  {
    "url": "https://arxiv.org/abs/2206.11321",
    "title": "An Application of a Modified Beta Factor Method for the Analysis of Software Common Cause Failures",
    "published_date": "2022-06-22",
    "abstract": ": This paper presents an approach for modeling software common cause failures (CCFs) within digital instrumentation and control (I&C) systems. CCFs consist of a concurrent failure between two or more components due to a shared failure cause and coupling mechanism. This work emphasizes the importance of identifying software-centric attributes related to the coupling mechanisms necessary for simultaneous failures of redundant software components. The groups of components that share coupling mechanisms are called common cause component groups (CCCGs). Most CCF models rely on operational data as the basis for establishing CCCG parameters and predicting CCFs. This work is motivated by two primary concerns: (1) a lack of operational and CCF data for estimating software CCF model parameters; and (2) the need to model single components as part of multiple CCCGs simultaneously. A hybrid approach was developed to account for these concerns by leveraging existing techniques: a modified beta factor model allows single components to be placed within multiple CCCGs, while a second technique provides software-specific model parameters for each CCCG. This hybrid approach provides a means to overcome the limitations of conventional methods while offering support for design decisions under the limited data scenario.",
    "summary": "This paper proposes a modified beta factor model for analyzing software common cause failures in digital instrumentation and control systems, addressing the limitations of existing models by incorporating software-specific attributes and allowing components to belong to multiple common cause component groups. This hybrid approach enables CCF analysis even with limited operational data."
  },
  {
    "url": "https://arxiv.org/pdf/2210.17497.pdf",
    "title": "Leveraging Pre-trained Models for Failure Analysis Triplets Generation",
    "published_date": "2022-10-31",
    "abstract": "Pre-trained Language Models recently gained traction in the Natural Language Processing (NLP) domain for text summarization, generation and question-answering tasks. This stems from the innovation introduced in Transformer models and their overwhelming performance compared with Recurrent Neural Network Models (Long Short Term Memory (LSTM)). In this paper, we leverage the attention mechanism of pre-trained causal language models such as Transformer model for the downstream task of generating Failure Analysis Triplets (FATs) - a sequence of steps for analyzing defected components in the semiconductor industry. We compare different transformer models for this generative task and observe that Generative Pre-trained Transformer 2 (GPT2) outperformed other transformer model for the failure analysis triplet generation (FATG) task. In particular, we observe that GPT2 (trained on 1.5B parameters) outperforms pre-trained BERT, BART and GPT3 by a large margin on ROUGE. Furthermore, we introduce Levenshstein Sequential Evaluation metric (LESE) for better evaluation of the structured FAT data and show that it compares exactly with human judgment than existing metrics.",
    "citation_count": 1,
    "summary": "This paper explores using pre-trained transformer models, specifically GPT-2, to generate Failure Analysis Triplets (FATs) for semiconductor defect analysis, demonstrating superior performance compared to BERT, BART, and GPT-3 using ROUGE and a novel Levenshtein Sequential Evaluation metric (LESE). GPT-2's attention mechanism effectively generates the structured FAT sequences."
  },
  {
    "url": "https://arxiv.org/pdf/2212.08755.pdf",
    "title": "Implicit Actions and Non-blocking Failure Recovery with MPI",
    "published_date": "2022-11-01",
    "abstract": "Scientific applications have long embraced the MPI as the environment of choice to execute on large distributed systems. The User-Level Failure Mitigation (ULFM) specification extends the MPI standard to address resilience and enable MPI applications to restore their communication capability after a failure. This works builds upon the wide body of experience gained in the field to eliminate a gap between current practice and the ideal, more asynchronous, recovery model in which the fault tolerance activities of multiple components can be carried out simultaneously and overlap. This work proposes to: (1) provide the required consistency in fault reporting to applications (i.e., enable an application to assess the success of a computational phase without incurring an unacceptable performance hit); (2) bring forward the building blocks that permit the effective scoping of fault recovery in an application, so that independent components in an application can recover without interfering with each other, and separate groups of processes in the application can recover independently or in unison; and (3) overlap recovery activities necessary to restore the consistency of the system (e.g., eviction of faulty processes from the communication group) with application recovery activities (e.g., dataset restoration from checkpoints).",
    "citation_count": 2,
    "summary": "This paper proposes extending the MPI standard with User-Level Failure Mitigation (ULFM) to improve resilience in scientific applications by enabling non-blocking, overlapping failure recovery among independent application components. This aims to minimize performance overhead while maintaining consistency in fault reporting and recovery scoping."
  },
  {
    "url": "https://arxiv.org/abs/2101.05444",
    "title": "Application of Failure Modes and Effects Analysis in the Engineering Design Process",
    "published_date": "2021-01-14",
    "abstract": "Failure modes and effects analysis (FMEA) is one of the most practical design tools implemented in the product design to analyze the possible failures and to improve the design. The use of FMEA is diversified, and different approaches are proposed by various organizations and researchers from one application to another. The question is how to use the features of FMEA along with the design process. This research focuses on different types of FMEA in the design process, which is considered as the mapping between customer requirements, design components, and product functions. These three elements of design are the foundation of the integration model proposed in this research. The objective of this research is to understand an integrated approach of FMEA in the design process. Significantly, an integration framework is developed to integrate the design process and FMEA. Then, a step-by-step FMEA-facilitated design process is proposed to apply FMEA along with the design process.",
    "citation_count": 3,
    "summary": "This paper explores the integration of Failure Modes and Effects Analysis (FMEA) into the engineering design process, proposing an integrated framework and a step-by-step methodology to proactively identify and mitigate potential product failures by mapping customer requirements, design components, and product functions."
  },
  {
    "url": "https://arxiv.org/pdf/2109.08546v1.pdf",
    "title": "Catastrophic failure and cumulative damage models involving two types of extended exponential distributions",
    "published_date": "2021-09-17",
    "abstract": "The present study supposes a single unit and investigates cumulative damage and catastrophic failure models for the unit, in situations where the interarrival times between the shocks, and the magnitudes of the shocks, involve two different stochastic processes. In order to consider two essentially different stochastic processes, integer gamma and Weibull distributions are treated as distributions with two parameters and extensions of exponential distributions. With respect to the cumulative damage models, under the assumption that the interarrival times between shocks follow exponential distributions, the case in which the magnitudes of the shocks follow integer gamma distributions is analyzed. With respect to the catastrophic failure models, the respective cases in which the interarrival times between shocks follow integer gamma and Weibull distributions are discussed. Finally, the study provides some characteristic values for reliability in such models.",
    "summary": "This paper analyzes cumulative damage and catastrophic failure models for a single unit, using extended exponential distributions (integer gamma and Weibull) to model inter-arrival times and shock magnitudes. The analysis explores reliability characteristics under different distributional assumptions for these two processes."
  }
]