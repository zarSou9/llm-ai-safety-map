[
  {
    "url": "https://arxiv.org/abs/2410.04334",
    "title": "AI Assistants for Incident Lifecycle in a Microservice Environment: A Systematic Literature Review",
    "published_date": "2024-10-06",
    "abstract": "Incidents in microservice environments can be costly and challenging to recover from due to their complexity and distributed nature. Recent advancements in artificial intelligence (AI) offer promising solutions for improving incident management. This paper systematically reviews primary studies on AI assistants designed to support different phases of the incident lifecycle. It highlights successful applications of AI, identifies gaps in current research, and suggests future opportunities for enhancing incident management through AI. By examining these studies, the paper aims to provide insights into the effectiveness of AI tools and their potential to address ongoing challenges in incident recovery.",
    "summary": "This systematic literature review examines the application of AI assistants in managing incidents within microservice environments, highlighting successful uses, research gaps, and future directions for improving incident lifecycle management through AI. The review aims to assess the effectiveness of AI tools in mitigating the challenges of incident recovery in complex, distributed systems."
  },
  {
    "url": "https://arxiv.org/abs/2403.04123",
    "title": "Exploring LLM-based Agents for Root Cause Analysis",
    "published_date": "2024-03-07",
    "abstract": "The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team's specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical evaluation of a ReAct agent equipped with retrieval tools, on an out-of-distribution dataset of production incidents collected at Microsoft. Results show that ReAct performs competitively with strong retrieval and reasoning baselines, but with highly increased factual accuracy. We then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements. Lastly, we conduct a case study with a team at Microsoft to equip the ReAct agent with tools that give it access to external diagnostic services that are used by the team for manual RCA. Our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice.",
    "citation_count": 8,
    "summary": "This paper investigates using Large Language Model (LLM) agents, specifically a ReAct agent with retrieval capabilities, for automated root cause analysis (RCA) of cloud software incidents. The authors demonstrate improved factual accuracy compared to baselines through empirical evaluation on a Microsoft production incident dataset, and explore the practical considerations of integrating such agents into real-world incident management workflows."
  },
  {
    "url": "https://arxiv.org/abs/2404.03662",
    "title": "X-lifecycle Learning for Cloud Incident Management using LLMs",
    "published_date": "2024-02-15",
    "abstract": "Incident management for large cloud services is a complex and tedious process and requires significant amount of manual efforts from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root causing and mitigating of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) created opportunities to automatically generate contextual recommendations to the OCEs assisting them to quickly identify and mitigate critical issues. However, existing research typically takes a silo-ed view for solving a certain task in incident management by leveraging data from a single stage of SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying ontology of service monitors used for automatically detecting incidents. By leveraging 353 incident and 260 monitor dataset from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over State-of-The-Art methods.",
    "citation_count": 9,
    "summary": "This paper proposes \"X-lifecycle Learning,\" a method using LLMs to improve cloud incident management by incorporating data from multiple software development lifecycle stages. Results on a Microsoft dataset show that this approach outperforms state-of-the-art methods for root cause recommendation and incident detection ontology identification."
  },
  {
    "url": "https://arxiv.org/abs/2409.16425",
    "title": "Lessons for Editors of AI Incidents from the AI Incident Database",
    "published_date": "2024-09-24",
    "abstract": "As artificial intelligence (AI) systems become increasingly deployed across the world, they are also increasingly implicated in AI incidents - harm events to individuals and society. As a result, industry, civil society, and governments worldwide are developing best practices and regulations for monitoring and analyzing AI incidents. The AI Incident Database (AIID) is a project that catalogs AI incidents and supports further research by providing a platform to classify incidents for different operational and research-oriented goals. This study reviews the AIID's dataset of 750+ AI incidents and two independent taxonomies applied to these incidents to identify common challenges to indexing and analyzing AI incidents. We find that certain patterns of AI incidents present structural ambiguities that challenge incident databasing and explore how epistemic uncertainty in AI incident reporting is unavoidable. We therefore report mitigations to make incident processes more robust to uncertainty related to cause, extent of harm, severity, or technical details of implicated systems. With these findings, we discuss how to develop future AI incident reporting practices.",
    "citation_count": 1,
    "summary": "Analyzing over 750 AI incidents in the AI Incident Database reveals structural ambiguities hindering consistent categorization and highlights the inherent uncertainty in AI incident reporting, leading to recommendations for improving future incident reporting practices."
  },
  {
    "url": "http://arxiv.org/abs/2401.13810",
    "title": "Automated Root Causing of Cloud Incidents using In-Context Learning with GPT-4",
    "published_date": "2024-01-24",
    "abstract": "Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis process for cloud services, requiring on-call engineers to identify the primary issues and implement corrective actions to prevent future recurrences. Improving the incident RCA process is vital for minimizing service downtime, customer impact and manual toil. Recent advances in artificial intelligence have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which have proven effective in tackling various AIOps problems, ranging from code authoring to incident management. Nonetheless, the GPT-4 model's immense size presents challenges when trying to fine-tune it on user data because of the significant GPU resource demand and the necessity for continuous model fine-tuning with the emergence of new data. To address the high cost of fine-tuning LLM, we propose an in-context learning approach for automated root causing, which eliminates the need for fine-tuning. We conduct extensive study over 100,000 production incidents, comparing several large language models using multiple metrics. The results reveal that our in-context learning approach outperforms the previous fine-tuned large language models such as GPT-3 by an average of 24.8\\% across all metrics, with an impressive 49.7\\% improvement over the zero-shot model. Moreover, human evaluation involving actual incident owners demonstrates its superiority over the fine-tuned model, achieving a 43.5\\% improvement in correctness and an 8.7\\% enhancement in readability. The impressive results demonstrate the viability of utilizing a vanilla GPT model for the RCA task, thereby avoiding the high computational and maintenance costs associated with a fine-tuned model.",
    "citation_count": 17,
    "summary": "This paper presents an in-context learning approach for automated root cause analysis of cloud incidents using GPT-4, eliminating the need for costly model fine-tuning while achieving a 24.8% average improvement in accuracy compared to fine-tuned GPT-3 models and superior performance in human evaluations."
  },
  {
    "url": "https://www.lesswrong.com/posts/gZBgmDFqqyw3Lghok/ai-regulatory-landscape-review-incident-reporting",
    "author": "Deric Cheng, Elliot_Mckernon",
    "title": "AI Incident Reporting: A Regulatory Review",
    "published_date": "2024-03-11",
    "summary": "This article begins a series analyzing the evolving global AI regulatory landscape, focusing on the US, EU, and China. The first installment examines AI incident reporting, exploring its potential benefits, various approaches being developed, and existing examples from other sectors like aviation and workplace safety."
  },
  {
    "url": "https://arxiv.org/pdf/2301.03797.pdf",
    "title": "Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models",
    "published_date": "2023-01-10",
    "abstract": "Incident management for cloud services is a complex process involving several steps and has a huge impact on both service health and developer productivity. On-call engineers require significant amount of domain knowledge and manual effort for root causing and mitigation of production incidents. Recent advances in artificial intelligence has resulted in state-of-the-art large language models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a variety of problems ranging from question answering to text summarization. In this work, we do the first large-scale study to evaluate the effectiveness of these models for helping engineers root cause and mitigate production incidents. We do a rigorous study at Microsoft, on more than 40,000 incidents and compare several large language models in zero-shot, fine-tuned and multi-task setting using semantic and lexical metrics. Lastly, our human evaluation with actual incident owners show the efficacy and future potential of using artificial intelligence for resolving cloud incidents.",
    "citation_count": 41,
    "summary": "This paper presents a large-scale study evaluating the effectiveness of large language models (LLMs) like GPT-3 in assisting with root cause analysis and mitigation of cloud incidents at Microsoft, demonstrating their potential to improve incident management. The study compares various LLM configurations and includes human evaluation of their efficacy on over 40,000 incidents."
  },
  {
    "url": "https://arxiv.org/pdf/2305.15778.pdf",
    "title": "Automatic Root Cause Analysis via Large Language Models for Cloud Incidents",
    "published_date": "2023-05-25",
    "abstract": "Ensuring the reliability and availability of cloud services necessitates efficient root cause analysis (RCA) for cloud incidents. Traditional RCA methods, which rely on manual investigations of data sources such as logs and traces, are often laborious, error-prone, and challenging for on-call engineers. In this paper, we introduce RCACopilot, an innovative on-call system empowered by the large language model for automating RCA of cloud incidents. RCACopilot matches incoming incidents to corresponding incident handlers based on their alert types, aggregates the critical runtime diagnostic information, predicts the incident's root cause category, and provides an explanatory narrative. We evaluate RCACopilot using a real-world dataset consisting of a year's worth of incidents from Microsoft. Our evaluation demonstrates that RCACopilot achieves RCA accuracy up to 0.766. Furthermore, the diagnostic information collection component of RCACopilot has been successfully in use at Microsoft for over four years.",
    "citation_count": 54,
    "summary": "RCACopilot, a large language model-powered system, automates root cause analysis of cloud incidents by aggregating diagnostic information, predicting root cause categories, and generating explanatory narratives, achieving up to 76.6% accuracy in real-world Microsoft data. A key component has been successfully deployed at Microsoft for over four years."
  }
]