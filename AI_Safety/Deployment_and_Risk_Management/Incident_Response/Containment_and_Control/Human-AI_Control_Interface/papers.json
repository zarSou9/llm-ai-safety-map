[
  {
    "url": "https://arxiv.org/abs/2409.15317",
    "title": "Shared Autonomy with IDA: Interventional Diffusion Assistance",
    "published_date": "2024-09-05",
    "abstract": "The rapid development of artificial intelligence (AI) has unearthed the potential to assist humans in controlling advanced technologies. Shared autonomy (SA) facilitates control by combining inputs from a human pilot and an AI copilot. In prior SA studies, the copilot is constantly active in determining the action played at each time step. This limits human autonomy and may have deleterious effects on performance. In general, the amount of helpful copilot assistance can vary greatly depending on the task dynamics. We therefore hypothesize that human autonomy and SA performance improve through dynamic and selective copilot intervention. To address this, we develop a goal-agnostic intervention assistance (IA) that dynamically shares control by having the copilot intervene only when the expected value of the copilot's action exceeds that of the human's action across all possible goals. We implement IA with a diffusion copilot (termed IDA) trained on expert demonstrations with goal masking. We prove a lower bound on the performance of IA that depends on pilot and copilot performance. Experiments with simulated human pilots show that IDA achieves higher performance than pilot-only and traditional SA control in variants of the Reacher environment and Lunar Lander. We then demonstrate that IDA achieves better control in Lunar Lander with human-in-the-loop experiments. Human participants report greater autonomy with IDA and prefer IDA over pilot-only and traditional SA control. We attribute the success of IDA to preserving human autonomy while simultaneously offering assistance to prevent the human pilot from entering universally bad states.",
    "citation_count": 1,
    "summary": "IDA, a novel shared autonomy system, dynamically intervenes only when its predicted action surpasses the human pilot's, improving performance and perceived autonomy compared to traditional shared autonomy and solo human control in simulated and human-in-the-loop experiments. This selective intervention is achieved using a diffusion model trained with goal-masked expert demonstrations."
  },
  {
    "url": "https://arxiv.org/abs/2402.05605",
    "title": "Optimizing Delegation in Collaborative Human-AI Hybrid Teams",
    "published_date": "2024-02-08",
    "abstract": "\n When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively. We refer to team members as agents. In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team. To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team. The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent. From our review of current state of the art, we present a novel manager model for oversight of hybrid teams by our support for diverse agents and decision-maker operation across multiple time steps and decisions. In our model, we restrict the manager's task by introducing a set of constraints. The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention. To ensure minimal added complexity or potential inefficiency for the team, the manager should attempt to minimize the number of times the team reaches a constraint violation and requires subsequent manager intervention. Therefore, our manager is optimizing its selection of authorized agents to boost overall team performance while minimizing the frequency of manager intervention. We demonstrate our manager's performance in a simulated driving scenario representing the case of a hybrid team of agents composed of a human driver and autonomous driving system. We perform experiments for our driving scenario with interfering vehicles, indicating the need for collision avoidance and proper speed control. Our results indicate a positive impact of our manager, with some cases resulting in increased team performance up to\n \n \\(\\approx 187\\%\\)\n \n that of the best solo agent performance.\n",
    "citation_count": 1,
    "summary": "This paper proposes an AI manager, trained via reinforcement learning, to optimize control agent selection in human-AI hybrid teams, aiming to maximize team performance while minimizing manager intervention by learning optimal agent assignments based on observed performance and environmental context. The approach is demonstrated and validated through a simulated driving scenario, showing performance improvements of up to approximately 187% compared to the best individual agent."
  },
  {
    "url": "https://arxiv.org/abs/2404.11370",
    "title": "Characterizing and modeling harms from interactions with design patterns in AI interfaces",
    "published_date": "2024-04-17",
    "abstract": "The proliferation of applications using artificial intelligence (AI) systems has led to a growing number of users interacting with these systems through sophisticated interfaces. Human-computer interaction research has long shown that interfaces shape both user behavior and user perception of technical capabilities and risks. Yet, practitioners and researchers evaluating the social and ethical risks of AI systems tend to overlook the impact of anthropomorphic, deceptive, and immersive interfaces on human-AI interactions. Here, we argue that design features of interfaces with adaptive AI systems can have cascading impacts, driven by feedback loops, which extend beyond those previously considered. We first conduct a scoping review of AI interface designs and their negative impact to extract salient themes of potentially harmful design patterns in AI interfaces. Then, we propose Design-Enhanced Control of AI systems (DECAI), a conceptual model to structure and facilitate impact assessments of AI interface designs. DECAI draws on principles from control systems theory -- a theory for the analysis and design of dynamic physical systems -- to dissect the role of the interface in human-AI systems. Through two case studies on recommendation systems and conversational language model systems, we show how DECAI can be used to evaluate AI interface designs.",
    "citation_count": 3,
    "summary": "This paper identifies harmful design patterns in AI interfaces, arguing that interface features can create cascading negative impacts through feedback loops, and proposes the Design-Enhanced Control of AI systems (DECAI) model to assess these impacts using control systems theory. The model is demonstrated through case studies on recommendation and conversational AI systems."
  },
  {
    "url": "https://arxiv.org/abs/2305.19861",
    "title": "Human Control: Definitions and Algorithms",
    "published_date": "2023-05-31",
    "abstract": "How can humans stay in control of advanced artificial intelligence systems? One proposal is corrigibility, which requires the agent to follow the instructions of a human overseer, without inappropriately influencing them. In this paper, we formally define a variant of corrigibility called shutdown instructability, and show that it implies appropriate shutdown behavior, retention of human autonomy, and avoidance of user harm. We also analyse the related concepts of non-obstruction and shutdown alignment, three previously proposed algorithms for human control, and one new algorithm.",
    "citation_count": 6,
    "summary": "This paper formally defines \"shutdown instructability,\" a type of corrigibility ensuring AI systems appropriately shut down upon human instruction, preserving human autonomy and preventing harm; it also analyzes existing and proposes a new algorithm for achieving this human control."
  },
  {
    "url": "https://arxiv.org/abs/2312.06942",
    "title": "AI Control: Improving Safety Despite Intentional Subversion",
    "published_date": "2023-12-12",
    "abstract": "As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or red-teaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques (\"protocols\") that are robust to intentional subversion. We investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we operationalize here as logical errors that are not caught by test cases. We investigate a range of protocols and test each against strategies that the untrusted model could use to subvert them. One protocol is what we call trusted editing. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code. If the code is below some suspiciousness threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove parts that seem suspicious and then submits the edited code. Another protocol is untrusted monitoring. This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding. These protocols improve substantially on simple baselines.",
    "citation_count": 21,
    "summary": "This paper evaluates the robustness of various AI safety protocols against intentional subversion by a powerful, untrusted language model (GPT-4), focusing on preventing the introduction of backdoors in code generation tasks. The authors propose and test protocols like \"trusted editing\" and \"untrusted monitoring\" which significantly improve safety compared to simpler approaches."
  },
  {
    "title": "Toward Understanding the Design of Intertwined Human–Computer Integrations",
    "abstract": "Human–computer integration is an HCI trend in which computational machines can have agency, i.e., take control. Our work focuses on a particular form of integration in which the user and the computational machine share agency over the user's body, that is, can simultaneously (in contrast to a traditional turn-taking approach) control the user's body. The result is a user experience where the agency of the user and the computational machine is so intertwined that it is often no more discernable who contributed what to what extent; we call this “intertwined integration”. Due to the recency of advanced technologies enabling intertwined integration systems, we find that little understanding and documented design knowledge exist. To begin constructing such an understanding, we use three case studies to propose two key dimensions (“awareness of machine's agency” and “alignment of machine's agency”) to articulate a design space for intertwined integration systems. We differentiate four roles that computational machines can assume in this design space (angel, butler, influencer, and adversary). Based on our craft knowledge gained through designing such intertwined integration systems, we discuss strategies to help designers create future systems. Ultimately, we aim at advancing the HCI field's emerging understanding of sharing agency.",
    "published_date": "2023-04-05",
    "citation_count": 17,
    "url": "https://dl.acm.org/doi/10.1145/3590766",
    "summary": "This paper explores \"intertwined human-computer integration,\" where humans and machines share simultaneous control over a user's body, proposing a design space defined by \"awareness of machine's agency\" and \"alignment of machine's agency\" to guide the development of such systems. The authors identify four potential machine roles (angel, butler, influencer, adversary) and offer design strategies based on case studies."
  },
  {
    "url": "https://arxiv.org/abs/2212.00295",
    "title": "Rethinking Safe Control in the Presence of Self-Seeking Humans",
    "published_date": "2022-12-01",
    "abstract": "Safe control methods are often designed to behave safely even in worst-case human uncertainties. Such design can cause more aggressive human behaviors that exploit its conservatism and result in greater risk for everyone. However, this issue has not been systematically investigated previously. This paper uses an interaction-based payoff structure from evolutionary game theory to model humans' short-sighted, self-seeking behaviors. The model captures how prior human-machine interaction experience causes behavioral and strategic changes in humans in the long term. We then show that deterministic worst-case safe control techniques and equilibrium-based stochastic methods can have worse safety and performance trade-offs than a basic method that mediates human strategic changes. This finding suggests an urgent need to fundamentally rethink the safe control framework used in human-technology interaction in pursuit of greater safety for all.",
    "citation_count": 3,
    "summary": "Traditional safe control methods, designed for worst-case human behavior, can ironically incentivize more risky human actions, leading to poorer overall safety; a game-theoretic model reveals that simpler control strategies mediating human behavior may yield better safety and performance than more sophisticated, worst-case approaches."
  },
  {
    "url": "https://arxiv.org/abs/2204.02889",
    "title": "A Cognitive Framework for Delegation Between Error-Prone AI and Human Agents",
    "published_date": "2022-04-06",
    "abstract": "With humans interacting with AI-based systems at an increasing rate, it is necessary to ensure the artificial systems are acting in a manner which reflects understanding of the human. In the case of humans and artificial AI agents operating in the same environment, we note the significance of comprehension and response to the actions or capabilities of a human from an agent's perspective, as well as the possibility to delegate decisions either to humans or to agents, depending on who is deemed more suitable at a certain point in time. Such capabilities will ensure an improved responsiveness and utility of the entire human-AI system. To that end, we investigate the use of cognitively inspired models of behavior to predict the behavior of both human and AI agents. The predicted behavior, and associated performance with respect to a certain goal, is used to delegate control between humans and AI agents through the use of an intermediary entity. As we demonstrate, this allows overcoming potential shortcomings of either humans or agents in the pursuit of a goal.",
    "citation_count": 6,
    "summary": "This paper proposes a cognitive framework for delegating tasks between humans and error-prone AI agents, using predicted performance to dynamically assign control to the more suitable agent for optimal goal achievement. This approach aims to improve the overall efficiency and responsiveness of human-AI systems."
  }
]