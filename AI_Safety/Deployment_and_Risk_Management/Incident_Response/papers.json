[
  {
    "url": "https://arxiv.org/pdf/2303.03070.pdf",
    "title": "Intrusion Response Systems: Past, Present and Future",
    "published_date": "2023-03-06",
    "abstract": "The rapid expansion of the Internet of Things and the emergence of edge computing-based applications has led to a new wave of cyber-attacks, with intensity and complexity that has never been seen before. Historically most research has focused on Intrusion Detection Systems (IDS), however due to the volume and speed of this new generation of cyber-attacks it is no longer sufficient to solely detect attacks and leave the response to security analysts. Consequently, research into Intrusion Response Systems (IRS) is accelerating rapidly. As such, new intrusion response approaches, methods and systems have been investigated, prototyped, and deployed. This paper is intended to provide a comprehensive review of the state of the art of IRSs. Specifically, a taxonomy to characterize the lifecycle of IRSs ranging from response selection to response deployment and response implementation is presented. A 10-phase structure to organize the core technical constituents of IRSs is also presented. Following this, an extensive review and analysis of the literature on IRSs published during the past decade is provided, and further classifies them into corresponding phases based on the proposed taxonomy and phase structure. This study provides a new way of classifying IRS research, thus offering in-depth insights into the latest discoveries and findings. In addition, through critical analysis and comparison, expert views, guidance and best practices on intrusion response approaches, system development and standardization are presented, upon which future research challenges and directions are postulated.",
    "citation_count": 2,
    "summary": "This paper reviews the current state of Intrusion Response Systems (IRS), driven by the increasing sophistication of cyberattacks, offering a novel taxonomy and 10-phase structure to classify and analyze existing IRS research and guide future development. It examines past research, presents expert insights, and proposes future research directions for improved intrusion response capabilities."
  },
  {
    "url": "https://arxiv.org/pdf/2309.07230.pdf",
    "title": "ESRO: Experience Assisted Service Reliability against Outages",
    "published_date": "2023-09-11",
    "abstract": "Modern cloud services are prone to failures due to their complex architecture, making diagnosis a critical process. Site Reliability Engineers (SREs) spend hours leveraging multiple sources of data, including the alerts, error logs, and domain expertise through past experiences to locate the root cause(s). These experiences are documented as natural language text in outage reports for previous outages. However, utilizing the raw yet rich semi-structured information in the reports systematically is time-consuming. Structured information, on the other hand, such as alerts that are often used during fault diagnosis, is voluminous and requires expert knowledge to discern. Several strategies have been proposed to use each source of data separately for root cause analysis. In this work, we build a diagnostic service called ESRO that recommends root causes and remediation for failures by utilizing structured as well as semi-structured sources of data systematically. ESRO constructs a causal graph using alerts and a knowledge graph using outage reports, and merges them in a novel way to form a unified graph during training. A retrieval based mechanism is then used to search the unified graph and rank the likely root causes and remediation techniques based on the alerts fired during an outage at inference time. Not only the individual alerts, but their respective importance in predicting an outage group is taken into account during recommendation. We evaluated our model on several cloud service outages of a large SaaS enterprise over the course of ~2 years, and obtained an average improvement of 27% in rouge scores after comparing the likely root causes against the ground truth over state-of-the-art baselines. We further establish the effectiveness of ESRO through qualitative analysis on multiple real outage examples.",
    "citation_count": 2,
    "summary": "ESRO is a diagnostic service that improves root cause analysis of cloud service outages by integrating structured alert data with semi-structured information from past outage reports into a unified graph; this approach yields a 27% average improvement in identifying root causes compared to existing methods."
  },
  {
    "url": "https://www.lesswrong.com/posts/g5XLHKyApAFXi3fso/president-biden-issues-executive-order-on-safe-secure-and",
    "author": "Tristan Williams",
    "title": "President Biden Issues Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence",
    "published_date": "2023-10-30",
    "summary": "The U.S. government issued a sweeping executive order addressing AI risks, mandating safety testing transparency for high-risk AI systems and establishing standards to mitigate threats to national security, public health, and safety, while also aiming to prevent AI-driven discrimination and fraud."
  },
  {
    "url": "https://arxiv.org/abs/2210.08006",
    "title": "Failure Analysis of Big Cloud Service Providers Prior to and During Covid-19 Period",
    "published_date": "2022-10-15",
    "abstract": "Cloud services are important for societal function such as healthcare, commerce, entertainment and education. Cloud can provide a variety of features such as increased collaboration and inexpensive computing. Failures are unavoidable in cloud services due to the large size and complexity, resulting in decreased reliability and efficiency. For example, due to bugs, many high-severity failures have been occurring in cloud infrastructure of popular providers, causing outages of several hours and the unrecoverable loss of user data. There are prior studies about cloud failure analyses are limited and use sources such as news articles. However, a detailed cloud failure focused study is required that provides analyses for cloud failure data gathered directly from the vendors. Furthermore, the Covid-19 cloud failures should be studied as cloud services played a major role throughout the Covid-19 period, as individuals relied on cloud services for activities such as working from home. A program can be made for this task. As a result, we will be able to better understand and mitigate cloud failures to reduce the effect of cloud failures.",
    "citation_count": 2,
    "summary": "This paper argues for a comprehensive analysis of cloud service provider failures, particularly those occurring during the COVID-19 pandemic, advocating for a shift from news-based analyses to direct vendor data for improved understanding and mitigation of future outages."
  },
  {
    "url": "https://www.lesswrong.com/posts/nEjuvyaG8HAGtbSWY/bugout-bags-for-disasters",
    "author": "tragedyofthecomments",
    "title": "Bugout Bags for Disasters",
    "published_date": "2022-03-08",
    "summary": "This article guides readers on assembling a bugout bag, emphasizing customization based on transportation (car vs. walking). It covers essential supplies like water purification, food, shelter, and first aid, advising lighter packing for walking and prioritizing comfort and morale."
  },
  {
    "url": "https://arxiv.org/pdf/2102.05870v1.pdf",
    "title": "Talking After Lights Out: An Ad Hoc Network for Electric Grid Recovery",
    "published_date": "2021-02-11",
    "abstract": "When the electrical grid in a region suffers a major outage, e.g., after a catastrophic cyber attack, a “black start” may be required, where the grid is slowly restarted, carefully and incrementally adding generating capacity and demand. To ensure safe and effective black start, the grid control center has to be able to communicate with field personnel and with supervisory control and data acquisition (SCADA) systems. Voice and text communication are particularly critical. As part of the Defense Advanced Research Projects Agency (DARPA) Rapid Attack Detection, Isolation, and Characterization Systems (RADICS) program, we designed, tested and evaluated a self-configuring mesh network prototype called the Phoenix Secure Emergency Network (PhoenixSEN). PhoenixSEN provides a secure drop-in replacement for grid's primary communication networks during black start recovery. The network combines existing and new technologies, can work with a variety of link-layer protocols, emphasizes manageability and auto-configuration, and provides services and applications for coordination of people and devices including voice, text, and SCADA communication. We discuss the architecture of PhoenixSEN and evaluate a prototype on realistic grid infrastructure through a series of DARPA-led exercises.",
    "summary": "PhoenixSEN, a self-configuring mesh network, provides secure communication for electric grid recovery during black start events, enabling voice, text, and SCADA communication even when primary networks are down. Its design, incorporating existing and new technologies, was tested and evaluated in realistic scenarios during DARPA exercises."
  },
  {
    "url": "https://arxiv.org/pdf/2108.12179.pdf",
    "title": "Graph-based Incident Aggregation for Large-Scale Online Service Systems",
    "published_date": "2021-08-27",
    "abstract": "As online service systems continue to grow in terms of complexity and volume, how service incidents are managed will significantly impact company revenue and user trust. Due to the cascading effect, cloud failures often come with an overwhelming number of incidents from dependent services and devices. To pursue efficient incident management, related incidents should be quickly aggregated to narrow down the problem scope. To this end, in this paper, we propose GRLIA, an incident aggregation framework based on graph representation learning over the cascading graph of cloud failures. A representation vector is learned for each unique type of incident in an unsupervised and unified manner, which is able to simultaneously encode the topological and temporal correlations among incidents. Thus, it can be easily employed for online incident aggregation. In particular, to learn the correlations more accurately, we try to recover the complete scope of failures' cascading impact by leveraging fine-grained system monitoring data, i.e., Key Performance Indicators (KPIs). The proposed framework is evaluated with real-world incident data collected from a large-scale online service system of Huawei Cloud. The experimental results demonstrate that GRLIA is effective and outperforms existing methods. Furthermore, our framework has been successfully deployed in industrial practice.",
    "citation_count": 16,
    "summary": "GRLIA, a novel graph-based incident aggregation framework, leverages representation learning on a cascading failure graph to efficiently group related incidents in large-scale online service systems, improving incident management by leveraging both topological and temporal correlations from fine-grained KPI data. Its effectiveness is validated through real-world deployment and outperformance of existing methods."
  },
  {
    "title": "Incident Response Support System for Multi-Located Network by Correlation Analysis of Individual Events",
    "abstract": "Recent targeted attacks have affected large networks, including those made up of multiple locations. Since branch offices are less secure than the headquarters (HQ), it is difficult to completely prevent malware from entering the network through these attacks. Therefore, it is important to quickly detect and respond to these attacks to prevent them from spreading to other locations. However, under the current management system, managers at each site must respond to incidents without sharing information from other sites, and the manager at the HQ cannot obtain enough information to estimate the relationship between the incidents. We propose a management system that refers to all of incident information archived by the organization, generates recommendations on the basis of the similarity between the ongoing incident and archived ones, and notifies each administrator to respond to the incident that occurred in his/her network regardless of resolved or unresolved incidents. The proposed system also notifies the administrators at HQ of the entirety of the attack on the basis of the relationship among recent and ongoing incidents.",
    "published_date": "2021-03-17",
    "citation_count": 1,
    "url": "https://dl.acm.org/doi/10.1145/3459955.3460592",
    "summary": "This paper proposes an incident response system for multi-location networks that correlates individual events across sites to improve detection and response to targeted attacks. The system analyzes similarities between current and past incidents to provide recommendations to local and HQ administrators, facilitating faster and more informed responses."
  },
  {
    "url": "https://arxiv.org/pdf/2108.00344.pdf",
    "title": "Groot: An Event-graph-based Approach for Root Cause Analysis in Industrial Settings",
    "published_date": "2021-08-01",
    "abstract": "For large-scale distributed systems, it is crucial to efficiently diagnose the root causes of incidents to maintain high system availability. The recent development of microservice architecture brings three major challenges (i.e., complexities of operation, system scale, and monitoring) to root cause analysis (RCA) in industrial settings. To tackle these challenges, in this paper, we present Groot, an event-graph-based approach for RCA. Groot constructs a real-time causality graph based on events that summarize various types of metrics, logs, and activities in the system under analysis. Moreover, to incorporate domain knowledge from site reliability engineering (SRE) engineers, Groot can be customized with user-defined events and domain-specific rules. Currently, Groot supports RCA among 5,000 real production services and is actively used by the SRE teams in eBay, a global e-commerce system serving more than 159 million active buyers per year. Over 15 months, we collect a data set containing labeled root causes of 952 real production incidents for evaluation. The evaluation results show that Groot is able to achieve 95% top-3 accuracy and 78% top-1 accuracy. To share our experience in deploying and adopting RCA in industrial settings, we conduct a survey to show that users of Groot find it helpful and easy to use. We also share the lessons learned from deploying and adopting Groot to solve RCA problems in production environments.",
    "citation_count": 54,
    "summary": "Groot is an event-graph-based root cause analysis (RCA) system designed for large-scale distributed systems, achieving high accuracy (95% top-3, 78% top-1) in identifying root causes of incidents at eBay by leveraging real-time causality graphs and incorporating domain expertise. Its effectiveness and ease of use have been validated through both quantitative evaluation and user surveys."
  },
  {
    "url": "https://arxiv.org/pdf/2106.14508v1.pdf",
    "title": "Towards anomaly detection in smart grids by combining Complex Events Processing and SNMP objects",
    "published_date": "2021-06-28",
    "abstract": "This paper describes the architecture and the fundamental methodology of an anomaly detector, which by continuously monitoring Simple Network Management Protocol data and by processing it as complex-events, is able to timely recognize patterns of faults and relevant cyber-attacks. This solution has been applied in the context of smart grids, and in particular as part of a security and resilience component of the Information and Communication Technologies (ICT) Gateway, a middleware-based architecture that correlates and fuses measurement data from different sources (e.g., Inverters, Smart Meters) to provide control coordination and to enable grid observability applications. The detector has been evaluated through experiments, where we selected some representative anomalies that can occur on the ICT side of the energy distribution infrastructure: non-malicious faults (indicated by patterns in the system resources usage), as well as effects of typical cyber-attacks directed to the smart grid infrastructure. The results show that the detection is promisingly fast and efficient.",
    "summary": "This paper presents an anomaly detection system for smart grids that uses Complex Event Processing of SNMP data to identify both non-malicious faults and cyber-attacks affecting the ICT infrastructure. Experimental results demonstrate the system's fast and efficient anomaly detection capabilities."
  },
  {
    "url": "https://arxiv.org/pdf/2103.06381v1.pdf",
    "title": "Fuzzy Logic-based Robust Failure Handling Mechanism for Fog Computing",
    "published_date": "2021-03-10",
    "abstract": "Fog computing is an emerging computing paradigm which is mainly suitable for time-sensitive and real-time Internet of Things (IoT) applications. Academia and industries are focusing on the exploration of various aspects of Fog computing for market adoption. The key idea of the Fog computing paradigm is to use idle computation resources of various handheld, mobile, stationery and network devices around us, to serve the application requests in the Fog-IoT environment. The devices in the Fog environment are autonomous and not exclusively dedicated to Fog application processing. Due to that, the probability of device failure in the Fog environment is high compared with other distributed computing paradigms. Solving failure issues in Fog is crucial because successful application execution can only be ensured if failure can be handled carefully. To handle failure, there are several techniques available in the literature, such as checkpointing and task migration, each of which works well in cloud based enterprise applications that mostly deals with static or transactional data. These failure handling methods are not applicable to highly dynamic Fog environment. In contrast, this work focuses on solving the problem of managing application failure in the Fog environment by proposing a composite solution (combining fuzzy logic-based task checkpointing and task migration techniques with task replication) for failure handling and generating a robust schedule. We evaluated the proposed methods using real failure traces in terms of application execution time, delay and cost. Average delay and total processing time improved by 56% and 48% respectively, on an average for the proposed solution, compared with the existing failure handling approaches.",
    "citation_count": 2,
    "summary": "This paper proposes a fuzzy logic-based failure handling mechanism for fog computing that combines task checkpointing, task migration, and task replication to create robust application schedules. The proposed solution, evaluated using real failure traces, significantly improved application execution time and delay compared to existing methods."
  },
  {
    "url": "https://www.lesswrong.com/posts/XvJWeFw78zmzc5iTg",
    "author": "jefftk",
    "title": "T-Mobile: Spurious Account Takeover Warning",
    "published_date": "2021-10-23",
    "summary": "The author received a concerning notification about a potential phone number takeover, but T-Mobile customer support confirmed it was a false alarm triggered by an automated system migrating their account to a new billing system. The author expresses disappointment in T-Mobile's handling of the situation."
  },
  {
    "title": "Event Management and Monitoring Framework for HPC Environments using ServiceNow and Prometheus",
    "abstract": "The challenge of monitoring and event response management of a high performance computing facility grows significantly as the facilities employs and orchestrates more complex and heterogeneous systems and infrastructure. As the computational components encompassing the HPC facility system increases, the computational staff experiences rise in alert fatigue due to the false alarms and noise related to the similar events generated by monitoring tools. The National Energy Research Scientific Computing Center (NERSC) at the Lawrence Berkeley National Laboratory (LBNL) has begun to address the issues of duplication of alerts and alert remediation. However, more automation and integration is needed for collecting, aggregating, correlating, analyzing, managing and visualizing the scale of events that will be generated by the emergent hybrid computing infrastructures. In this paper, we present an event management and monitoring framework that addresses the operational needs of the future pre-exascale systems at the Lawrence Berkeley National Laboratory's National Energy Research Scientific Computing Center (NERSC). The framework integrates the Operations Monitoring and Notification Infrastructure (OMNI) at NERSC with the Prometheus, Grafana and ServiceNow platforms to help identify, diagnose, and resolve incidents in real-time, as well as conduct more thorough post-incident reviews enabled by the intuitive dashboards that provides a single pane of glass console for an efficient operations management and real-time proactive monitoring.",
    "published_date": "2020-11-02",
    "citation_count": 11,
    "url": "https://dl.acm.org/doi/10.1145/3415958.3433046",
    "summary": "NERSC's new event management framework integrates Prometheus, Grafana, and ServiceNow with their existing OMNI system to improve monitoring and incident response in their HPC environment, mitigating alert fatigue and enabling efficient real-time management and post-incident analysis. This addresses the challenges of managing increasingly complex and heterogeneous pre-exascale systems."
  },
  {
    "title": "Towards intelligent incident management: why we need it and how we make it",
    "abstract": "The management of cloud service incidents (unplanned interruptions or outages of a service/product) greatly affects customer satisfaction and business revenue. After years of efforts, cloud enterprises are able to solve most incidents automatically and timely. However, in practice, we still observe critical service incidents that occurred in an unexpected manner and orchestrated diagnosis workflow failed to mitigate them. In order to accelerate the understanding of unprecedented incidents and provide actionable recommendations, modern incident management system employs the strategy of AIOps (Artificial Intelligence for IT Operations). In this paper, to provide a broad view of industrial incident management and understand the modern incident management system, we conduct a comprehensive empirical study spanning over two years of incident management practices at Microsoft. Particularly, we identify two critical challenges (namely, incomplete service/resource dependencies and imprecise resource health assessment) and investigate the underlying reasons from the perspective of cloud system design and operations. We also present IcM BRAIN, our AIOps framework towards intelligent incident management, and show its practical benefits conveyed to the cloud services of Microsoft.",
    "published_date": "2020-11-07",
    "citation_count": 71,
    "url": "https://dl.acm.org/doi/10.1145/3368089.3417055",
    "summary": "This paper examines challenges in cloud service incident management at Microsoft, highlighting incomplete dependencies and imprecise health assessments as critical issues hindering automated resolution. It introduces IcM BRAIN, an AIOps framework designed to improve intelligent incident management and address these challenges."
  },
  {
    "title": "Efficient incident identification from multi-dimensional issue reports via meta-heuristic search",
    "abstract": "In large-scale cloud systems, unplanned service interruptions and outages may cause severe degradation of service availability. Such incidents can occur in a bursty manner, which will deteriorate user satisfaction. Identifying incidents rapidly and accurately is critical to the operation and maintenance of a cloud system. In industrial practice, incidents are typically detected through analyzing the issue reports, which are generated over time by monitoring cloud services. Identifying incidents in a large number of issue reports is quite challenging. An issue report is typically multi-dimensional: it has many categorical attributes. It is difficult to identify a specific attribute combination that indicates an incident. Existing methods generally rely on pruning-based search, which is time-consuming given high-dimensional data, thus not practical to incident detection in large-scale cloud systems. In this paper, we propose MID (Multi-dimensional Incident Detection), a novel framework for identifying incidents from large-amount, multi-dimensional issue reports effectively and efficiently. Key to the MID design is encoding the problem into a combinatorial optimization problem. Then a specific-tailored meta-heuristic search method is designed, which can rapidly identify attribute combinations that indicate incidents. We evaluate MID with extensive experiments using both synthetic data and real-world data collected from a large-scale production cloud system. The experimental results show that MID significantly outperforms the current state-of-the-art methods in terms of effectiveness and efficiency. Additionally, MID has been successfully applied to Microsoft's cloud systems and helped greatly reduce manual maintenance effort.",
    "published_date": "2020-11-07",
    "citation_count": 27,
    "url": "https://dl.acm.org/doi/10.1145/3368089.3409741",
    "summary": "MID, a novel framework, efficiently identifies incidents in high-dimensional cloud system issue reports by formulating the problem as a combinatorial optimization and employing a tailored meta-heuristic search, significantly outperforming existing methods in both effectiveness and efficiency. This approach has proven successful in reducing manual maintenance efforts in Microsoft's cloud systems."
  }
]