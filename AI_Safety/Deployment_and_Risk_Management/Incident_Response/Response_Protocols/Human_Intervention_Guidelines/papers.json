[
  {
    "url": "http://arxiv.org/abs/2401.04729",
    "title": "Human Delegation Behavior in Human-AI Collaboration: The Effect of Contextual Information",
    "published_date": "2024-01-09",
    "abstract": "The integration of artificial intelligence (AI) into human decision-making processes at the workplace presents both opportunities and challenges. One promising approach to leverage existing complementary capabilities is allowing humans to delegate individual instances of decision tasks to AI. However, enabling humans to delegate instances effectively requires them to assess several factors. One key factor is the analysis of both their own capabilities and those of the AI in the context of the given task. In this work, we conduct a behavioral study to explore the effects of providing contextual information to support this delegation decision. Specifically, we investigate how contextual information about the AI and the task domain influence humans' delegation decisions to an AI and their impact on the human-AI team performance. Our findings reveal that access to contextual information significantly improves human-AI team performance in delegation settings. Finally, we show that the delegation behavior changes with the different types of contextual information. Overall, this research advances the understanding of computer-supported, collaborative work and provides actionable insights for designing more effective collaborative systems.",
    "citation_count": 2,
    "summary": "This study investigates how providing contextual information about AI capabilities and task domains impacts human delegation decisions in human-AI collaborations. Results show that contextual information significantly improves team performance by influencing delegation behavior."
  },
  {
    "url": "https://arxiv.org/abs/2409.11741",
    "title": "HARP: Human-Assisted Regrouping with Permutation Invariant Critic for Multi-Agent Reinforcement Learning",
    "published_date": "2024-09-18",
    "abstract": "Human-in-the-loop reinforcement learning integrates human expertise to accelerate agent learning and provide critical guidance and feedback in complex fields. However, many existing approaches focus on single-agent tasks and require continuous human involvement during the training process, significantly increasing the human workload and limiting scalability. In this paper, we propose HARP (Human-Assisted Regrouping with Permutation Invariant Critic), a multi-agent reinforcement learning framework designed for group-oriented tasks. HARP integrates automatic agent regrouping with strategic human assistance during deployment, enabling and allowing non-experts to offer effective guidance with minimal intervention. During training, agents dynamically adjust their groupings to optimize collaborative task completion. When deployed, they actively seek human assistance and utilize the Permutation Invariant Group Critic to evaluate and refine human-proposed groupings, allowing non-expert users to contribute valuable suggestions. In multiple collaboration scenarios, our approach is able to leverage limited guidance from non-experts and enhance performance. The project can be found at https://github.com/huawen-hu/HARP.",
    "summary": "HARP is a multi-agent reinforcement learning framework that uses human assistance strategically during deployment, allowing non-experts to improve agent performance with minimal intervention by suggesting regroupings. The system incorporates automatic regrouping during training and a permutation-invariant critic to evaluate human-provided suggestions."
  },
  {
    "url": "https://arxiv.org/abs/2409.07985",
    "title": "Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols",
    "published_date": "2024-09-12",
    "abstract": "To evaluate the safety and usefulness of deployment protocols for untrusted AIs, AI Control uses a red-teaming exercise played between a protocol designer and an adversary. This paper introduces AI-Control Games, a formal decision-making model of the red-teaming exercise as a multi-objective, partially observable, stochastic game. We also introduce methods for finding optimal protocols in AI-Control Games, by reducing them to a set of zero-sum partially observable stochastic games. We apply our formalism to model, evaluate and synthesise protocols for deploying untrusted language models as programming assistants, focusing on Trusted Monitoring protocols, which use weaker language models and limited human assistance. Finally, we demonstrate the utility of our formalism by showcasing improvements over empirical studies in existing settings, evaluating protocols in new settings, and analysing how modelling assumptions affect the safety and usefulness of protocols.",
    "citation_count": 2,
    "summary": "This paper presents AI-Control Games, a formal game-theoretic model for evaluating the safety of AI deployment protocols by framing the red-teaming process as a multi-objective stochastic game. The authors demonstrate its utility by synthesizing and evaluating protocols for untrusted language models, showcasing improvements over existing empirical methods."
  },
  {
    "url": "https://arxiv.org/abs/2406.06051",
    "title": "On the Utility of Accounting for Human Beliefs about AI Intention in Human-AI Collaboration",
    "published_date": "2024-06-10",
    "abstract": "To enable effective human-AI collaboration, merely optimizing AI performance without considering human factors is insufficient. Recent research has shown that designing AI agents that take human behavior into account leads to improved performance in human-AI collaboration. However, a limitation of most existing approaches is their assumption that human behavior remains static, regardless of the AI agent's actions. In reality, humans may adjust their actions based on their beliefs about the AI's intentions, specifically, the subtasks they perceive the AI to be attempting to complete based on its behavior. In this paper, we address this limitation by enabling a collaborative AI agent to consider its human partner's beliefs about its intentions, i.e., what the human partner thinks the AI agent is trying to accomplish, and to design its action plan accordingly to facilitate more effective human-AI collaboration. Specifically, we developed a model of human beliefs that captures how humans interpret and reason about their AI partner's intentions. Using this belief model, we created an AI agent that incorporates both human behavior and human beliefs when devising its strategy for interacting with humans. Through extensive real-world human-subject experiments, we demonstrate that our belief model more accurately captures human perceptions of AI intentions. Furthermore, we show that our AI agent, designed to account for human beliefs over its intentions, significantly enhances performance in human-AI collaboration.",
    "summary": "This paper argues that effective human-AI collaboration requires AI agents to model human beliefs about the AI's intentions, not just human behavior. Experiments demonstrate that an AI agent incorporating this belief model significantly improves collaborative performance."
  },
  {
    "url": "https://www.lesswrong.com/posts/F24kibEdEvRSo7PFi/human-ai-complementarity-a-goal-for-amplified-oversight",
    "author": "rishubjain",
    "title": "Human-AI Complementarity: A Goal for Amplified Oversight",
    "published_date": "2024-12-24",
    "summary": "To ensure AI safety, \"amplified oversight\" uses AI to enhance human evaluation of increasingly complex AI systems. This involves two key approaches: rater assistance (AI tools aiding human evaluators) and hybridization (combining human and AI judgments)."
  },
  {
    "url": "https://arxiv.org/abs/2302.02944",
    "title": "Learning Complementary Policies for Human-AI Teams",
    "published_date": "2023-02-06",
    "abstract": "Human-AI complementarity is important when neither the algorithm nor the human yields dominant performance across all instances in a given context. Recent work that explored human-AI collaboration has considered decisions that correspond to classification tasks. However, in many important contexts where humans can benefit from AI complementarity, humans undertake course of action. In this paper, we propose a framework for a novel human-AI collaboration for selecting advantageous course of action, which we refer to as Learning Complementary Policy for Human-AI teams (\\textsc{lcp-hai}). Our solution aims to exploit the human-AI complementarity to maximize decision rewards by learning both an algorithmic policy that aims to complement humans by a routing model that defers decisions to either a human or the AI to leverage the resulting complementarity. We then extend our approach to leverage opportunities and mitigate risks that arise in important contexts in practice: 1) when a team is composed of multiple humans with differential and potentially complementary abilities, 2) when the observational data includes consistent deterministic actions, and 3) when the covariate distribution of future decisions differ from that in the historical data. We demonstrate the effectiveness of our proposed methods using data on real human responses and semi-synthetic, and find that our methods offer reliable and advantageous performance across setting, and that it is superior to when either the algorithm or the AI make decisions on their own. We also find that the extensions we propose effectively improve the robustness of the human-AI collaboration performance in the presence of different challenging settings.",
    "citation_count": 5,
    "summary": "The paper introduces Learning Complementary Policies for Human-AI teams (LCP-HAI), a framework for human-AI collaboration in selecting advantageous courses of action, maximizing rewards by learning a complementary AI policy and a routing model to optimally allocate decisions between human and AI. This framework is extended to handle multiple humans, deterministic actions, and covariate shift, demonstrating superior performance compared to solely human or AI decision-making."
  },
  {
    "url": "https://arxiv.org/pdf/2305.18165.pdf",
    "title": "An Emergency Disposal Decision-making Method with Human-Machine Collaboration",
    "published_date": "2023-05-29",
    "abstract": "Rapid developments in artificial intelligence technology have led to unmanned systems replacing human beings in many fields requiring high-precision predictions and decisions. In modern operational environments, all job plans are affected by emergency events such as equipment failures and resource shortages, making a quick resolution critical. The use of unmanned systems to assist decision-making can improve resolution efficiency, but their decision-making is not interpretable and may make the wrong decisions. Current unmanned systems require human supervision and control. Based on this, we propose a collaborative human--machine method for resolving unplanned events using two phases: task filtering and task scheduling. In the task filtering phase, we propose a human--machine collaborative decision-making algorithm for dynamic tasks. The GACRNN model is used to predict the state of the job nodes, locate the key nodes, and generate a machine-predicted resolution task list. A human decision-maker supervises the list in real time and modifies and confirms the machine-predicted list through the human--machine interface. In the task scheduling phase, we propose a scheduling algorithm that integrates human experience constraints. The steps to resolve an event are inserted into the normal job sequence to schedule the resolution. We propose several human--machine collaboration methods in each phase to generate steps to resolve an unplanned event while minimizing the impact on the original job plan.",
    "summary": "This paper presents a two-phase human-machine collaborative method for emergency disposal decision-making, using a GACRNN model for initial task prediction and a human-in-the-loop system for refinement and scheduling to minimize disruption to existing plans. The method integrates both machine prediction and human expertise to achieve efficient and reliable emergency response."
  },
  {
    "url": "https://arxiv.org/pdf/2303.09224.pdf",
    "title": "Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction",
    "published_date": "2023-03-16",
    "abstract": "Recent work has proposed artificial intelligence (AI) models that can learn to decide whether to make a prediction for an instance of a task or to delegate it to a human by considering both parties' capabilities. In simulations with synthetically generated or context-independent human predictions, delegation can help improve the performance of human-AI teams—compared to humans or the AI model completing the task alone. However, so far, it remains unclear how humans perform and how they perceive the task when they are aware that an AI model delegated task instances to them. In an experimental study with 196 participants, we show that task performance and task satisfaction improve through AI delegation, regardless of whether humans are aware of the delegation. Additionally, we identify humans' increased levels of self-efficacy as the underlying mechanism for these improvements in performance and satisfaction. Our findings provide initial evidence that allowing AI models to take over more management responsibilities can be an effective form of human-AI collaboration in workplaces.",
    "citation_count": 26,
    "summary": "An experimental study demonstrated that AI delegation of tasks to humans improves both task performance and satisfaction, regardless of human awareness, primarily by increasing human self-efficacy. This suggests AI's increased management role can be a beneficial approach to human-AI collaboration."
  }
]