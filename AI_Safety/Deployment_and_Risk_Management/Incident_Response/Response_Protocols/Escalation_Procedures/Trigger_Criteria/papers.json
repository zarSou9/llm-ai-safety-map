[
  {
    "url": "https://arxiv.org/abs/2501.08933",
    "title": "Separation Assurance in Urban Air Mobility Systems using Shared Scheduling Protocols",
    "published_date": "2025-01-15",
    "abstract": "Ensuring safe separation between aircraft is a critical challenge in air traffic management, particularly in urban air mobility (UAM) environments where high traffic density and low altitudes require precise control. In these environments, conflicts often arise at the intersections of flight corridors, posing significant risks. We propose a tactical separation approach leveraging shared scheduling protocols, originally designed for Ethernet networks and operating systems, to coordinate access to these intersections. Using a decentralized Markov decision process framework, the proposed approach enables aircraft to autonomously adjust their speed and timing as they navigate these critical areas, maintaining safe separation without a central controller. We evaluate the effectiveness of this approach in simulated UAM scenarios, demonstrating its ability to reduce separation violations to zero while acknowledging trade-offs in flight times as traffic density increases. Additionally, we explore the impact of non-compliant aircraft, showing that while shared scheduling protocols can no longer guarantee safe separation, they still provide significant improvements over systems without scheduling protocols."
  },
  {
    "url": "https://arxiv.org/abs/2401.03408",
    "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
    "published_date": "2024-01-07",
    "abstract": "Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns. We observe that models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons. Qualitatively, we also collect the models' reported reasoning for chosen actions and observe worrying justifications based on deterrence and first-strike tactics. Given the high stakes of military and foreign-policy contexts, we recommend further examination and cautious consideration before deploying autonomous language model agents for strategic military or diplomatic decision-making.",
    "citation_count": 27
  },
  {
    "url": "https://arxiv.org/abs/2406.14713",
    "title": "Risk thresholds for frontier AI",
    "published_date": "2024-06-20",
    "abstract": "Frontier artificial intelligence (AI) systems could pose increasing risks to public safety and security. But what level of risk is acceptable? One increasingly popular approach is to define capability thresholds, which describe AI capabilities beyond which an AI system is deemed to pose too much risk. A more direct approach is to define risk thresholds that simply state how much risk would be too much. For instance, they might state that the likelihood of cybercriminals using an AI system to cause X amount of economic damage must not increase by more than Y percentage points. The main upside of risk thresholds is that they are more principled than capability thresholds, but the main downside is that they are more difficult to evaluate reliably. For this reason, we currently recommend that companies (1) define risk thresholds to provide a principled foundation for their decision-making, (2) use these risk thresholds to help set capability thresholds, and then (3) primarily rely on capability thresholds to make their decisions. Regulators should also explore the area because, ultimately, they are the most legitimate actors to define risk thresholds. If AI risk estimates become more reliable, risk thresholds should arguably play an increasingly direct role in decision-making.",
    "citation_count": 6
  },
  {
    "url": "https://arxiv.org/abs/2302.04734v2",
    "title": "Pricing cyber-insurance for systems via maturity models",
    "published_date": "2023-02-09",
    "abstract": "Pricing insurance for risks associated with information technology systems presents a complex modelling challenge, combining the disciplines of operations management, security, and economics. This work proposes a socioeconomic modelling framework for cyber-insurance decisions compromised of entity relationship diagrams, security maturity models, and economic models, addressing a long-standing research challenge of capturing organizational structure in the design and pricing of cyber-insurance policies. Insurance pricing is usually informed by the long experience insurance companies have of the magnitude and frequency of losses that arise in organizations based on their size, industry sector, and location. Consequently, their calculations of premia will start from a baseline determined by these considerations. A unique challenge of cyber-insurance is that data history is limited and not necessarily informative of future loss risk meaning that established actuarial methodology for other lines of insurance may not be the optimal pricing strategy. The modelling framework proposed in this paper provides a vehicle for agreement between practitioners in the cyber-insurance ecosystem on cyber-security risks and allows for the users to choose their desired level of abstraction in the description of a system.",
    "citation_count": 1
  },
  {
    "url": "https://arxiv.org/pdf/2108.12702v1.pdf",
    "title": "Performance-Barrier-Based Event-Triggered Control With Applications to Network Systems",
    "published_date": "2021-08-28",
    "abstract": "This article proposes a novel framework for resource-aware control design termed performance-barrier-based triggering. Given a feedback policy, along with a Lyapunov function certificate that guarantees its correctness, we examine the problem of designing its digital implementation through event-triggered control while ensuring a prescribed performance on the certificate's convergence rate is met and triggers occur as sparingly as possible. Our methodology takes into account the performance residual, i.e., how well the system is doing in regards to the prescribed performance. Inspired by the notion of control barrier function, the trigger design allows the certificate to deviate from monotonically decreasing, with leeway specified as an increasing function of the performance residual, resulting in greater flexibility in prescribing update times. We study different types of performance specifications, with particular attention to quantifying the benefits of the proposed approach in the exponential case. We build on this to design intrinsically Zeno-free distributed triggers for network systems. A comparison of event-triggered approaches in a vehicle platooning problem shows how the proposed design meets the prescribed performance with a significantly lower number of controller updates.",
    "citation_count": 11
  },
  {
    "url": "https://arxiv.org/abs/2111.00289",
    "title": "Intrusion Prevention Through Optimal Stopping",
    "published_date": "2021-10-30",
    "abstract": "We study automated intrusion prevention using reinforcement learning. Following a novel approach, we formulate the problem of intrusion prevention as an (optimal) multiple stopping problem. This formulation gives us insight into the structure of optimal policies, which we show to have threshold properties. For most practical cases, it is not feasible to obtain an optimal defender policy using dynamic programming. We therefore develop a reinforcement learning approach to approximate an optimal threshold policy. We introduce T- SPSA, an efficient reinforcement learning algorithm that learns threshold policies through stochastic approximation. We show that T- SPSA outperforms state-of-the-art algorithms for our use case. Our overall method for learning and validating policies includes two systems: a simulation system where defender policies are incrementally learned and an emulation system where statistics are produced that drive simulation runs and where learned policies are evaluated. We show that this approach can produce effective defender policies for a practical IT infrastructure.",
    "citation_count": 20
  }
]