[
  {
    "url": "https://arxiv.org/abs/2411.11761",
    "title": "Mapping out the Space of Human Feedback for Reinforcement Learning: A Conceptual Framework",
    "published_date": "2024-11-18",
    "abstract": "Reinforcement Learning from Human feedback (RLHF) has become a powerful tool to fine-tune or train agentic machine learning models. Similar to how humans interact in social contexts, we can use many types of feedback to communicate our preferences, intentions, and knowledge to an RL agent. However, applications of human feedback in RL are often limited in scope and disregard human factors. In this work, we bridge the gap between machine learning and human-computer interaction efforts by developing a shared understanding of human feedback in interactive learning scenarios. We first introduce a taxonomy of feedback types for reward-based learning from human feedback based on nine key dimensions. Our taxonomy allows for unifying human-centered, interface-centered, and model-centered aspects. In addition, we identify seven quality metrics of human feedback influencing both the human ability to express feedback and the agent's ability to learn from the feedback. Based on the feedback taxonomy and quality criteria, we derive requirements and design choices for systems learning from human feedback. We relate these requirements and design choices to existing work in interactive machine learning. In the process, we identify gaps in existing work and future research opportunities. We call for interdisciplinary collaboration to harness the full potential of reinforcement learning with data-driven co-adaptive modeling and varied interaction mechanics.",
    "summary": "This paper presents a framework for understanding and improving reinforcement learning from human feedback (RLHF), proposing a taxonomy of feedback types based on nine key dimensions and seven quality metrics to guide system design and highlight areas for future interdisciplinary research. The framework unifies human-centered, interface-centered, and model-centered aspects of RLHF."
  },
  {
    "url": "http://arxiv.org/abs/2401.11198",
    "title": "A Deep Learning Approach for Selective Relevance Feedback",
    "published_date": "2024-01-20",
    "abstract": "Pseudo-relevance feedback (PRF) can enhance average retrieval effectiveness over a sufficiently large number of queries. However, PRF often introduces a drift into the original information need, thus hurting the retrieval effectiveness of several queries. While a selective application of PRF can potentially alleviate this issue, previous approaches have largely relied on unsupervised or feature-based learning to determine whether a query should be expanded. In contrast, we revisit the problem of selective PRF from a deep learning perspective, presenting a model that is entirely data-driven and trained in an end-to-end manner. The proposed model leverages a transformer-based bi-encoder architecture. Additionally, to further improve retrieval effectiveness with this selective PRF approach, we make use of the model's confidence estimates to combine the information from the original and expanded queries. In our experiments, we apply this selective feedback on a number of different combinations of ranking and feedback models, and show that our proposed approach consistently improves retrieval effectiveness for both sparse and dense ranking models, with the feedback models being either sparse, dense or generative.",
    "summary": "This paper proposes a deep learning-based approach for selective pseudo-relevance feedback (PRF), using a transformer-based bi-encoder to determine which queries benefit from PRF expansion and combining original and expanded query information based on model confidence. Experiments demonstrate consistent improvements in retrieval effectiveness across various ranking and feedback models."
  },
  {
    "url": "https://arxiv.org/abs/2303.16433v1",
    "title": "A Bandit Learning Method for Continuous Games Under Feedback Delays with Residual Pseudo-Gradient Estimate",
    "published_date": "2023-03-29",
    "abstract": "Learning in multi-player games can model a large variety of practical scenarios, where each player seeks to optimize its own local objective function, which at the same time relies on the actions taken by others. Motivated by the frequent absence of first-order information such as partial gradients in solving local optimization problems and the prevalence of asynchronicity and feedback delays in multi-agent systems, we introduce a bandit learning algorithm, which integrates mirror descent, residual pseudo-gradient estimates, and the priority-based feedback utilization strategy, to contend with these challenges. We establish that for pseudo-monotone plus games, the actual sequences of play generated by the proposed algorithm converge a.s. to critical points. Compared with the existing method, the proposed algorithm yields more consistent estimates with less variation and allows for more aggressive choices of parameters. Finally, we illustrate the validity of the proposed algorithm through a thermal load management problem of building complexes.",
    "summary": "This paper proposes a novel bandit learning algorithm for continuous games with feedback delays, using mirror descent and residual pseudo-gradient estimates to handle the lack of first-order information and asynchronous updates. The algorithm is proven to converge to critical points for pseudo-monotone plus games and demonstrates improved performance and parameter flexibility compared to existing methods."
  },
  {
    "url": "https://arxiv.org/pdf/2205.05888v1.pdf",
    "title": "How Does Feedback Signal Quality Impact Effectiveness of Pseudo Relevance Feedback for Passage Retrieval",
    "published_date": "2022-05-12",
    "abstract": "Pseudo-Relevance Feedback (PRF) assumes that the top results retrieved by a first-stage ranker are relevant to the original query and uses them to improve the query representation for a second round of retrieval. This assumption however is often not correct: some or even all of the feedback documents may be irrelevant. Indeed, the effectiveness of PRF methods may well depend on the quality of the feedback signal and thus on the effectiveness of the first-stage ranker. This aspect however has received little attention before. In this paper we control the quality of the feedback signal and measure its impact on a range of PRF methods, including traditional bag-of-words methods (Rocchio), and dense vector-based methods (learnt and not learnt). Our results show the important role the quality of the feedback signal plays on the effectiveness of PRF methods. Importantly, and surprisingly, our analysis reveals that not all PRF methods are the same when dealing with feedback signals of varying quality. These findings are critical to gain a better understanding of the PRF methods and of which and when they should be used, depending on the feedback signal quality, and set the basis for future research in this area.",
    "citation_count": 7,
    "summary": "This paper investigates how the quality of initial retrieval results (feedback signal) affects the performance of various pseudo-relevance feedback (PRF) methods in passage retrieval, revealing significant performance variations across PRF techniques based on feedback quality. The findings highlight the crucial, previously under-studied, role of initial ranker effectiveness in determining PRF success."
  },
  {
    "url": "https://www.lesswrong.com/posts/d9MkMeLWvoDEsqpQP/a-compilation-of-misuses-of-statistics",
    "author": "Younes Kamel",
    "title": "A compilation of misuses of statistics",
    "published_date": "2022-02-14",
    "summary": "The article highlights common statistical errors, primarily the false assumption of Gaussian distributions in areas like finance and the misinterpretation of p-values, base rates, and statistical power. These errors lead to flawed conclusions in scientific studies and data-driven applications."
  },
  {
    "url": "https://www.lesswrong.com/posts/cCcCJnvMEHqrgiCnx/practical-use-of-the-beta-distribution-for-data-analysis",
    "author": "Maxwell Peterson",
    "title": "Practical use of the Beta distribution for data analysis",
    "published_date": "2022-04-03",
    "summary": "For calculating probabilities from binary count data, the Beta distribution is more accurate than the commonly used Gaussian (Normal) distribution, especially with small datasets or probabilities near 0 or 1, where the Gaussian approximation breaks down and produces nonsensical results. The easily implemented Beta distribution provides a superior alternative."
  },
  {
    "title": "ColorfulFeedback: Enhancing Interest Prediction Performance through Multi-dimensional Labeled Feedback from Users",
    "abstract": "Recommendation systems help to predict user demand and improve the quality of services offered. While the performance of a recommendation system depends on the quality and quantity of feedback from users, the two major approaches to feedback sacrifice quality for quantity or vice versa; implicit feedback is more abundant but less reliable, while explicit feedback is more credible but harder to collect. Although a hybrid approach has the potential to combine the strengths of both kinds of feedback, the existing approaches using explicit feedback are not suitable for such a combination. In this study, we design a novel feedback suitable for the hybrid approach and use it improve the performance of a recommendation system. The system enables us to collect more varied and less biased feedback from users. It improves performance without requiring major changes to the inference model. It also provides a unique and rich source of information of the model itself. We demonstrate an application of Colorful Feedback showing how it can improve an existing recommendation model.",
    "published_date": "2021-03-08",
    "citation_count": 2,
    "url": "https://dl.acm.org/doi/10.1145/3437963.3441700",
    "summary": "ColorfulFeedback improves recommendation system performance by introducing a novel multi-dimensional explicit feedback mechanism that combines the strengths of implicit and explicit feedback without requiring significant model changes, yielding richer, less biased user data. This enhanced feedback leads to improved interest prediction accuracy."
  },
  {
    "url": "https://arxiv.org/pdf/2111.08596v1.pdf",
    "title": "Reinforcement Learning with Feedback from Multiple Humans with Diverse Skills",
    "published_date": "2021-11-16",
    "abstract": "A promising approach to improve the robustness and exploration in Reinforcement Learning is collecting human feedback and that way incorporating prior knowledge of the target environment. It is, however, often too expensive to obtain enough feedback of good quality. To mitigate the issue, we aim to rely on a group of multiple experts (and non-experts) with different skill levels to generate enough feedback. Such feedback can therefore be inconsistent and infrequent. In this paper, we build upon prior work -- Advise, a Bayesian approach attempting to maximise the information gained from human feedback -- extending the algorithm to accept feedback from this larger group of humans, the trainers, while also estimating each trainer's reliability. We show how aggregating feedback from multiple trainers improves the total feedback's accuracy and make the collection process easier in two ways. Firstly, this approach addresses the case of some of the trainers being adversarial. Secondly, having access to the information about each trainer reliability provides a second layer of robustness and offers valuable information for people managing the whole system to improve the overall trust in the system. It offers an actionable tool for improving the feedback collection process or modifying the reward function design if needed. We empirically show that our approach can accurately learn the reliability of each trainer correctly and use it to maximise the information gained from the multiple trainers' feedback, even if some of the sources are adversarial.",
    "citation_count": 5,
    "summary": "This paper extends the Advise algorithm to incorporate diverse and potentially unreliable human feedback in reinforcement learning, improving data efficiency by aggregating feedback from multiple sources and estimating each source's reliability to mitigate inconsistencies and adversarial inputs. The approach enhances robustness and provides insights for optimizing the feedback collection process."
  }
]