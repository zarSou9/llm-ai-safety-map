[
  {
    "url": "https://arxiv.org/pdf/2012.01557.pdf",
    "title": "Value Alignment Verification",
    "published_date": "2020-12-02",
    "abstract": "As humans interact with autonomous agents to perform increasingly complicated, potentially risky tasks, it is important that humans can verify these agents' trustworthiness and efficiently evaluate their performance and correctness. In this paper we formalize the problem of value alignment verification: how to efficiently test whether the goals and behavior of another agent are aligned with a human's values? We explore several different value alignment verification settings and provide foundational theory regarding value alignment verification. We study alignment verification problems with an idealized human that has an explicit reward function as well as value alignment verification problems where the human has implicit values. Our theoretical and empirical results in both a discrete grid navigation domain and a continuous autonomous driving domain demonstrate that it is possible to synthesize highly efficient and accurate value alignment verification tests for certifying the alignment of autonomous agents.",
    "citation_count": 28,
    "summary": "This paper formalizes the problem of value alignment verification for autonomous agents, exploring methods to efficiently test if an agent's goals and behavior align with human values, both explicitly and implicitly defined, and demonstrating the feasibility of creating accurate verification tests."
  },
  {
    "url": "https://arxiv.org/abs/2409.13919",
    "title": "Measuring Error Alignment for Decision-Making Systems",
    "published_date": "2024-09-20",
    "abstract": "Given that AI systems are set to play a pivotal role in future decision-making processes, their trustworthiness and reliability are of critical concern. Due to their scale and complexity, modern AI systems resist direct interpretation, and alternative ways are needed to establish trust in those systems, and determine how well they align with human values. We argue that good measures of the information processing similarities between AI and humans, may be able to achieve these same ends. While Representational alignment (RA) approaches measure similarity between the internal states of two systems, the associated data can be expensive and difficult to collect for human systems. In contrast, Behavioural alignment (BA) comparisons are cheaper and easier, but questions remain as to their sensitivity and reliability. We propose two new behavioural alignment metrics misclassification agreement which measures the similarity between the errors of two systems on the same instances, and class-level error similarity which measures the similarity between the error distributions of two systems. We show that our metrics correlate well with RA metrics, and provide complementary information to another BA metric, within a range of domains, and set the scene for a new approach to value alignment.",
    "summary": "This paper introduces two novel behavioral alignment metrics, misclassification agreement and class-level error similarity, to assess the alignment of AI systems with human decision-making by comparing their error patterns, offering a more accessible and practical alternative to resource-intensive representational alignment methods. These metrics demonstrate strong correlation with representational alignment and provide valuable complementary information for evaluating AI trustworthiness."
  },
  {
    "url": "https://arxiv.org/abs/2409.09174",
    "title": "Incorporation of Verifier Functionality in the Software for Operations and Network Attack Results Review and the Autonomous Penetration Testing System",
    "published_date": "2024-09-13",
    "abstract": "The software for operations and network attack results review (SONARR) and the autonomous penetration testing system (APTS) use facts and common properties in digital twin networks to represent real-world entities. However, in some cases fact values will change regularly, making it difficult for objects in SONARR and APTS to consistently and accurately represent their real-world counterparts. This paper proposes and evaluates the addition of verifiers, which check real-world conditions and update network facts, to SONARR. This inclusion allows SONARR to retrieve fact values from its executing environment and update its network, providing a consistent method of ensuring that the operations and, therefore, the results align with the real-world systems being assessed. Verifiers allow arbitrary scripts and dynamic arguments to be added to normal SONARR operations. This provides a layer of flexibility and consistency that results in more reliable output from the software.",
    "summary": "This paper proposes adding \"verifier\" functionality to the Software for Operations and Network Attack Results Review (SONARR) system to dynamically update its network representation based on real-world conditions, ensuring consistent alignment between simulated and actual systems during penetration testing. This enhances the reliability of SONARR's output by incorporating real-time data verification."
  },
  {
    "url": "https://arxiv.org/abs/2406.04231",
    "title": "Quantifying Misalignment Between Agents: Towards a Sociotechnical Understanding of Alignment",
    "published_date": "2024-06-06",
    "abstract": "Existing work on the alignment problem has focused mainly on (1) qualitative descriptions of the alignment problem; (2) attempting to align AI actions with human interests by focusing on value specification and learning; and/or (3) focusing on a single agent or on humanity as a monolith. Recent sociotechnical approaches highlight the need to understand complex misalignment among multiple human and AI agents. We address this gap by adapting a computational social science model of human contention to the alignment problem. Our model quantifies misalignment in large, diverse agent groups with potentially conflicting goals across various problem areas. Misalignment scores in our framework depend on the observed agent population, the domain in question, and conflict between agents' weighted preferences. Through simulations, we demonstrate how our model captures intuitive aspects of misalignment across different scenarios. We then apply our model to two case studies, including an autonomous vehicle setting, showcasing its practical utility. Our approach offers enhanced explanatory power for complex sociotechnical environments and could inform the design of more aligned AI systems in real-world applications.",
    "summary": "This paper introduces a computational model to quantify misalignment among multiple human and AI agents with potentially conflicting goals, addressing a gap in existing alignment research that primarily focuses on single agents or monolithic human interests. The model uses weighted preferences to calculate misalignment scores across various scenarios, demonstrated through simulations and case studies."
  },
  {
    "url": "https://arxiv.org/abs/2411.18798",
    "title": "Formal Verification of Digital Twins with TLA and Information Leakage Control",
    "published_date": "2024-11-27",
    "abstract": "Verifying the correctness of a digital twin provides a formal guarantee that the digital twin operates as intended. Digital twin verification is challenging due to the presence of uncertainties in the virtual representation, the physical environment, and the bidirectional flow of information between physical and virtual. A further challenge is that a digital twin of a complex system is composed of distributed components. This paper presents a methodology to specify and verify digital twin behavior, translating uncertain processes into a formally verifiable finite state machine. We use the Temporal Logic of Actions (TLA) to create a specification, an implementation abstraction that defines the properties required for correct system behavior. Our approach includes a novel weakening of formal security properties, allowing controlled information leakage while preserving theoretical guarantees. We demonstrate this approach on a digital twin of an unmanned aerial vehicle, verifying synchronization of physical-to-virtual and virtual-to-digital data flows to detect unintended misalignments.",
    "summary": "This paper presents a methodology for formally verifying digital twin behavior using TLA, addressing challenges posed by uncertainties and distributed components by modeling uncertain processes as finite state machines and incorporating a novel approach to controlled information leakage. The approach is demonstrated on an unmanned aerial vehicle digital twin, verifying data synchronization between physical and virtual components."
  },
  {
    "url": "https://arxiv.org/abs/2409.11052",
    "title": "A logical alarm for misaligned binary classifiers",
    "published_date": "2024-09-17",
    "abstract": "If two agents disagree in their decisions, we may suspect they are not both correct. This intuition is formalized for evaluating agents that have carried out a binary classification task. Their agreements and disagreements on a joint test allow us to establish the only group evaluations logically consistent with their responses. This is done by establishing a set of axioms (algebraic relations) that must be universally obeyed by all evaluations of binary responders. A complete set of such axioms are possible for each ensemble of size N. The axioms for $N = 1, 2$ are used to construct a fully logical alarm - one that can prove that at least one ensemble member is malfunctioning using only unlabeled data. The similarities of this approach to formal software verification and its utility for recent agendas of safe guaranteed AI are discussed.",
    "summary": "This paper presents a logical framework for detecting misaligned binary classifiers by analyzing their agreement and disagreement on a common dataset, establishing axioms that define logically consistent evaluations and enabling the construction of an alarm that detects malfunctioning classifiers using only unlabeled data. This approach draws parallels to formal software verification and contributes to safe AI development."
  },
  {
    "url": "https://arxiv.org/abs/2402.07791",
    "title": "Discovering Decision Manifolds to Assure Trusted Autonomous Systems",
    "published_date": "2024-02-12",
    "abstract": "Developing and fielding complex systems requires proof that they are reliably correct with respect to their design and operating requirements. Especially for autonomous systems which exhibit unanticipated emergent behavior, fully enumerating the range of possible correct and incorrect behaviors is intractable. Therefore, we propose an optimization-based search technique for generating high-quality, high-variance, and non-trivial data which captures the range of correct and incorrect responses a system could exhibit. This manifold between desired and undesired behavior provides a more detailed understanding of system reliability than traditional testing or Monte Carlo simulations. After discovering data points along the manifold, we apply machine learning techniques to quantify the decision manifold's underlying mathematical function. Such models serve as correctness properties which can be utilized to enable both verification during development and testing, as well as continuous assurance during operation, even amidst system adaptations and dynamic operating environments. This method can be applied in combination with a simulator in order to provide evidence of dependability to system designers and users, with the ultimate aim of establishing trust in the deployment of complex systems. In this proof-of-concept, we apply our method to a software-in-the-loop evaluation of an autonomous vehicle.",
    "summary": "This paper presents an optimization-based method for generating diverse test data revealing the boundary between correct and incorrect behaviors in autonomous systems, creating a \"decision manifold\" that's then modeled using machine learning for verification and continuous assurance. This approach improves upon traditional testing by providing a richer understanding of system reliability."
  },
  {
    "url": "https://www.alignmentforum.org/posts/wtTz6hyP6hnX5NiuA/aspiration-based-designs-2-formal-framework-basic-algorithm",
    "author": "Jobst Heitzig, Simon Dima, Simon Fischer",
    "title": "[Aspiration-based designs] 2. Formal framework, basic algorithm",
    "published_date": "2024-04-28",
    "summary": "This article introduces an aspiration-based algorithm for AI agents to achieve specific expected values of a given evaluation metric, rather than maximizing it. The algorithm, proven to guarantee goal fulfillment under certain conditions, propagates aspirations over time within a Markov Decision Process framework."
  }
]