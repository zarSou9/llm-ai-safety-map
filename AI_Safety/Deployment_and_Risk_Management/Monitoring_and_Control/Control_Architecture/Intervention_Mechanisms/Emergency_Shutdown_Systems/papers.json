[
  {
    "url": "https://arxiv.org/abs/2411.17749",
    "title": "The Partially Observable Off-Switch Game",
    "published_date": "2024-11-25",
    "abstract": "A wide variety of goals could cause an AI to disable its off switch because\"you can't fetch the coffee if you're dead\"(Russell 2019). Prior theoretical work on this shutdown problem assumes that humans know everything that AIs do. In practice, however, humans have only limited information. Moreover, in many of the settings where the shutdown problem is most concerning, AIs might have vast amounts of private information. To capture these differences in knowledge, we introduce the Partially Observable Off-Switch Game (PO-OSG), a game-theoretic model of the shutdown problem with asymmetric information. Unlike when the human has full observability, we find that in optimal play, even AI agents assisting perfectly rational humans sometimes avoid shutdown. As expected, increasing the amount of communication or information available always increases (or leaves unchanged) the agents' expected common payoff. But counterintuitively, introducing bounded communication can make the AI defer to the human less in optimal play even though communication mitigates information asymmetry. In particular, communication sometimes enables new optimal behavior requiring strategic AI deference to achieve outcomes that were previously inaccessible. Thus, designing safe artificial agents in the presence of asymmetric information requires careful consideration of the tradeoffs between maximizing payoffs (potentially myopically) and maintaining AIs' incentives to defer to humans."
  },
  {
    "url": "https://arxiv.org/abs/2407.14683",
    "title": "Large-Area Emergency Lockdowns with Automated Driving Systems",
    "published_date": "2024-07-19",
    "abstract": "Region-wide restrictions on personal vehicle travel have a long history in the United States, from riot curfews in the late 1960s, to travel bans during snow events, to the 2013 shelter-in-place\"lockdown\"during the search for the perpetrator of the Boston Marathon bombing. Because lockdowns require tremendous resources to enforce, they are often limited in duration or scope. The introduction of automated driving systems may allow governments to quickly and cheaply effect large-area lockdowns by jamming wireless communications, spoofing road closures on digital maps, exploiting a vehicle's programming to obey all traffic control devices, or coordinating with vehicle developers. Future vehicles may lack conventional controls, rendering them undrivable by the public. As travel restrictions become easier to implement, governments may enforce them more frequently, over longer durations and wider areas. This article explores the practical, legal, and ethical implications of lockdowns when most driving is highly automated, and provides guidance for the development of lockdown policies."
  },
  {
    "url": "https://arxiv.org/abs/2407.00805",
    "title": "Towards shutdownable agents via stochastic choice",
    "published_date": "2024-06-30",
    "abstract": "Some worry that advanced artificial agents may resist being shut down. The Incomplete Preferences Proposal (IPP) is an idea for ensuring that doesn't happen. A key part of the IPP is using a novel 'Discounted REward for Same-Length Trajectories (DREST)' reward function to train agents to (1) pursue goals effectively conditional on each trajectory-length (be 'USEFUL'), and (2) choose stochastically between different trajectory-lengths (be 'NEUTRAL' about trajectory-lengths). In this paper, we propose evaluation metrics for USEFULNESS and NEUTRALITY. We use a DREST reward function to train simple agents to navigate gridworlds, and we find that these agents learn to be USEFUL and NEUTRAL. Our results thus suggest that DREST reward functions could also train advanced agents to be USEFUL and NEUTRAL, and thereby make these advanced agents useful and shutdownable."
  },
  {
    "url": "https://arxiv.org/pdf/2305.19861.pdf",
    "title": "Human Control: Definitions and Algorithms",
    "published_date": "2023-05-31",
    "abstract": "How can humans stay in control of advanced artificial intelligence systems? One proposal is corrigibility, which requires the agent to follow the instructions of a human overseer, without inappropriately influencing them. In this paper, we formally define a variant of corrigibility called shutdown instructability, and show that it implies appropriate shutdown behavior, retention of human autonomy, and avoidance of user harm. We also analyse the related concepts of non-obstruction and shutdown alignment, three previously proposed algorithms for human control, and one new algorithm.",
    "citation_count": 6
  },
  {
    "url": "https://arxiv.org/pdf/2305.18165.pdf",
    "title": "An Emergency Disposal Decision-making Method with Human-Machine Collaboration",
    "published_date": "2023-05-29",
    "abstract": "Rapid developments in artificial intelligence technology have led to unmanned systems replacing human beings in many fields requiring high-precision predictions and decisions. In modern operational environments, all job plans are affected by emergency events such as equipment failures and resource shortages, making a quick resolution critical. The use of unmanned systems to assist decision-making can improve resolution efficiency, but their decision-making is not interpretable and may make the wrong decisions. Current unmanned systems require human supervision and control. Based on this, we propose a collaborative human--machine method for resolving unplanned events using two phases: task filtering and task scheduling. In the task filtering phase, we propose a human--machine collaborative decision-making algorithm for dynamic tasks. The GACRNN model is used to predict the state of the job nodes, locate the key nodes, and generate a machine-predicted resolution task list. A human decision-maker supervises the list in real time and modifies and confirms the machine-predicted list through the human--machine interface. In the task scheduling phase, we propose a scheduling algorithm that integrates human experience constraints. The steps to resolve an event are inserted into the normal job sequence to schedule the resolution. We propose several human--machine collaboration methods in each phase to generate steps to resolve an unplanned event while minimizing the impact on the original job plan."
  },
  {
    "url": "https://arxiv.org/abs/2310.00374",
    "title": "Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers",
    "published_date": "2023-09-30",
    "abstract": "As artificial intelligence (AI) models are scaled up, new capabilities can emerge unintentionally and unpredictably, some of which might be dangerous. In response, dangerous capabilities evaluations have emerged as a new risk assessment tool. But what should frontier AI developers do if sufficiently dangerous capabilities are in fact discovered? This paper focuses on one possible response: coordinated pausing. It proposes an evaluation-based coordination scheme that consists of five main steps: (1) Frontier AI models are evaluated for dangerous capabilities. (2) Whenever, and each time, a model fails a set of evaluations, the developer pauses certain research and development activities. (3) Other developers are notified whenever a model with dangerous capabilities has been discovered. They also pause related research and development activities. (4) The discovered capabilities are analyzed and adequate safety precautions are put in place. (5) Developers only resume their paused activities if certain safety thresholds are reached. The paper also discusses four concrete versions of that scheme. In the first version, pausing is completely voluntary and relies on public pressure on developers. In the second version, participating developers collectively agree to pause under certain conditions. In the third version, a single auditor evaluates models of multiple developers who agree to pause if any model fails a set of evaluations. In the fourth version, developers are legally required to run evaluations and pause if dangerous capabilities are discovered. Finally, the paper discusses the desirability and feasibility of our proposed coordination scheme. It concludes that coordinated pausing is a promising mechanism for tackling emerging risks from frontier AI models. However, a number of practical and legal obstacles need to be overcome, especially how to avoid violations of antitrust law.",
    "citation_count": 13
  }
]