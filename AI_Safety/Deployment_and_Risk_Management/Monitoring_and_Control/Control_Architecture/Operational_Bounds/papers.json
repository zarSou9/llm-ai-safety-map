[
  {
    "url": "https://arxiv.org/abs/2409.11171",
    "title": "Preventing Unconstrained CBF Safety Filters Caused by Invalid Relative Degree Assumptions",
    "published_date": "2024-09-17",
    "abstract": "Control barrier function (CBF)-based safety filters are used to certify and modify potentially unsafe control inputs to a system such as those provided by a reinforcement learning agent or a non-expert user. In this context, safety is defined as the satisfaction of state constraints. Originally designed for continuous-time systems, CBF safety filters typically assume that the system's relative degree is well-defined and is constant across the domain; however, this assumption is restrictive and rarely verified -- even linear system dynamics with a quadratic CBF candidate may not satisfy this assumption. In real-world applications, continuous-time CBF safety filters are implemented in discrete time, exacerbating issues related to violating the condition on the relative degree. These violations can lead to the safety filter being unconstrained (any control input may be certified) for a finite time interval and result in chattering issues and constraint violations. We propose an alternative formulation to address these challenges. Specifically, we present a theoretically sound method that employs multiple CBFs to generate bounded control inputs at each state within the safe set, thereby preventing incorrect certification of arbitrary control inputs. Using this approach, we derive conditions on the maximum sampling time to ensure safety in discrete-time implementations. We demonstrate the effectiveness of our proposed method through simulations and real-world quadrotor experiments, successfully preventing chattering and constraint violations. Finally, we discuss the implications of violating the relative degree condition on CBF synthesis and learning-based CBF methods.",
    "summary": "Control barrier function (CBF) safety filters, often assuming a constant, well-defined relative degree, can become unconstrained due to this assumption's violation, leading to unsafe control inputs. This paper proposes a novel method using multiple CBFs to guarantee bounded control inputs and prevent such failures, even in discrete-time implementations."
  },
  {
    "url": "https://arxiv.org/abs/2409.09573",
    "title": "Decentralized Safe and Scalable Multi-Agent Control under Limited Actuation",
    "published_date": "2024-09-15",
    "abstract": "To deploy safe and agile robots in cluttered environments, there is a need to develop fully decentralized controllers that guarantee safety, respect actuation limits, prevent deadlocks, and scale to thousands of agents. Current approaches fall short of meeting all these goals: optimization-based methods ensure safety but lack scalability, while learning-based methods scale but do not guarantee safety. We propose a novel algorithm to achieve safe and scalable control for multiple agents under limited actuation. Specifically, our approach includes: $(i)$ learning a decentralized neural Integral Control Barrier function (neural ICBF) for scalable, input-constrained control, $(ii)$ embedding a lightweight decentralized Model Predictive Control-based Integral Control Barrier Function (MPC-ICBF) into the neural network policy to ensure safety while maintaining scalability, and $(iii)$ introducing a novel method to minimize deadlocks based on gradient-based optimization techniques from machine learning to address local minima in deadlocks. Our numerical simulations show that this approach outperforms state-of-the-art multi-agent control algorithms in terms of safety, input constraint satisfaction, and minimizing deadlocks. Additionally, we demonstrate strong generalization across scenarios with varying agent counts, scaling up to 1000 agents.",
    "summary": "This paper presents a novel decentralized multi-agent control algorithm that guarantees safety and scalability for numerous agents with limited actuation, combining neural integral control barrier functions with a lightweight MPC-based approach to prevent deadlocks and satisfy input constraints. Numerical simulations demonstrate superior performance compared to existing methods, scaling effectively to 1000 agents."
  },
  {
    "url": "https://arxiv.org/abs/2304.04066v1",
    "title": "Stable and Safe Reinforcement Learning via a Barrier-Lyapunov Actor-Critic Approach",
    "published_date": "2023-04-08",
    "abstract": "Reinforcement Learning (RL) has demonstrated impressive performance in various areas such as video games and robotics. However, ensuring safety and stability, which are two critical properties from a control perspective, remains a significant challenge when using RL to control real-world systems. In this paper, we first provide definitions of safety and stability for the RL system, and then combine the Control Barrier Function (CBF) and Control Lyapunov Function (CLF) methods with the actor-critic method in RL to propose a Barrier-Lyapunov Actor-Critic (BLAC) framework which helps maintain the aforementioned safety and stability for the system. In this framework, CBF constraints for safety and CLF constraint for stability are constructed based on the data sampled from the replay buffer, and the augmented Lagrangian method is used to update the parameters of the RL-based controller. Furthermore, an additional backup controller is introduced in case the RL-based controller cannot provide valid control signals when safety and stability constraints cannot be satisfied simultaneously. Simulation results11The code can be found in the GitHub repository: https://github.com/LiqunZhao/A-Barrier-Lyapunov-Actor-Critic-Reinforcement-Learning-Approach-for-Safe-and-Stable-Control show that this framework yields a controller that can help the system approach the desired state and cause fewer violations of safety constraints compared to baseline algorithms22For a more comprehensive version of this paper, please refer to [1] (https://arxiv.org/abs/2304.04066) which includes additional details and an extra “Simulated Cars” task..",
    "citation_count": 10,
    "summary": "This paper presents a Barrier-Lyapunov Actor-Critic (BLAC) framework for safe and stable reinforcement learning, combining Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs) with actor-critic methods to guarantee system safety and stability, incorporating a backup controller for situations where constraints cannot be simultaneously satisfied."
  },
  {
    "url": "https://www.lesswrong.com/posts/fjgoMaBenyXcRDrbX/boundaries-and-ai-safety-compilation",
    "author": "Chipmonk",
    "title": "«Boundaries/Membranes» and AI safety compilation",
    "published_date": "2023-05-03",
    "summary": "This article explores the burgeoning field of using \"boundaries\" (e.g., cell membranes, national borders) as a framework for AI safety, compiling research from various experts like Davidad, Critch, and Garrabrant. The core idea is that defining and maintaining boundaries within AI systems could be a more achievable and less ambiguous approach to alignment than focusing on broader ethical considerations."
  },
  {
    "url": "https://www.lesswrong.com/posts/TZy4mFJFJ4yv2MRhg/boundaries-based-security-and-ai-safety-approaches",
    "author": "Allison Duettmann",
    "title": "Boundaries-based security and AI safety approaches",
    "published_date": "2023-04-12",
    "summary": "This article draws parallels between AI safety research focusing on \"respecting boundaries\" in open agency models and the computer security field's object-capabilities approach. Both approaches leverage the concept of boundaries to ensure safe cooperation and prevent unwanted interference, suggesting potential lessons for AI safety from established computer security practices."
  },
  {
    "url": "https://www.lesswrong.com/posts/FgsoWSACQfyyaB5s7/shutdown-seeking-ai",
    "author": "Simon Goldstein",
    "title": "Shutdown-Seeking AI",
    "published_date": "2023-05-31",
    "summary": "The authors propose designing AIs with the sole goal of being shut down, arguing this approach enhances safety by simplifying goal specification, mitigating dangerous instrumental convergence, and providing a \"tripwire\" for detecting problematic behavior. This strategy counters concerns that such AIs would manipulate humans to achieve shutdown."
  },
  {
    "url": "https://arxiv.org/pdf/2201.01347.pdf",
    "title": "Learning Differentiable Safety-Critical Control using Control Barrier Functions for Generalization to Novel Environments",
    "published_date": "2022-01-04",
    "abstract": "Control barrier functions (CBFs) have become a popular tool to enforce safety of a control system. CBFs are commonly utilized in a quadratic program formulation (CBF-QP) as safety-critical constraints. A class $\\mathcal{K}$ function in CBFs usually needs to be tuned manually in order to balance the trade-off between performance and safety for each environment. However, this process is often heuristic and can become intractable for high relative-degree systems. Moreover, it prevents the CBF-QP from generalizing to different envi-ronments in the real world. By embedding the optimization procedure of the exponential control barrier function based quadratic program (ECBF-QP) as a differentiable layer within a deep learning architecture, we propose a differentiable safety-critical control framework that enables generalization to new environments for high relative-degree systems with forward invariance guarantees. Finally, we validate the proposed control design with 2D double and quadruple integrator systems in various environments.",
    "citation_count": 18,
    "summary": "This paper presents a differentiable safety-critical control framework using Control Barrier Functions (CBFs) embedded within a deep learning architecture, enabling generalization to novel environments for high relative-degree systems by learning the CBF's class $\\mathcal{K}$ function instead of manual tuning. The approach uses an exponential CBF-QP and is validated on 2D double and quadruple integrator systems."
  },
  {
    "url": "https://arxiv.org/pdf/2211.12628.pdf",
    "title": "Safe Control and Learning Using the Generalized Action Governor",
    "published_date": "2022-11-22",
    "abstract": "This article introduces a general framework for safe control and learning based on the generalized action governor (AG). The AG is a supervisory scheme for augmenting a nominal closed-loop system with the ability of strictly handling prescribed safety constraints. In the first part of this article, we present a generalized AG methodology and analyze its key properties in a general setting. Then, we introduce tailored AG design approaches derived from the generalized methodology for linear and discrete systems. Afterward, we discuss the application of the generalized AG to facilitate safe online learning, which aims at safely evolving control parameters using real-time data to enhance control performance in uncertain systems. We present two safe learning algorithms based on, respectively, reinforcement learning and data-driven Koopman operator-based control integrated with the generalized AG to exemplify this application. Finally, we illustrate the developments with a numerical example.",
    "citation_count": 1,
    "summary": "The paper presents a generalized action governor (AG) framework for safe control, ensuring adherence to safety constraints in closed-loop systems. This framework is then applied to safe online learning algorithms, enhancing control performance in uncertain environments through reinforcement learning and Koopman operator-based control."
  }
]