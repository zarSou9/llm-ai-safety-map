[
  {
    "url": "https://arxiv.org/abs/1907.10662v2",
    "title": "Art: Abstraction Refinement-Guided Training for Provably Correct Neural Networks",
    "published_date": "2019-07-17",
    "abstract": "Artificial Neural Networks (ANNs) have demonstrated remarkable utility in various challenging machine learning applications. While formally verified properties of their behaviors are highly desired, they have proven notoriously difficult to derive and enforce. Existing approaches typically formulate this problem as a post facto analysis process. In this paper, we present a novel learning framework that ensures such formal guarantees are enforced by construction. Our technique enables training provably correct networks with respect to a broad class of safety properties, a capability that goes well-beyond existing approaches, without compromising much accuracy. Our key insight is that we can integrate an optimization-based abstraction refinement loop into the learning process and operate over dynamically constructed partitions of the input space that considers accuracy and safety objectives synergistically. The refinement procedure iteratively splits the input space from which training data is drawn, guided by the efficacy with which such partitions enable safety verification. We have implemented our approach in a tool (ART) and applied it to enforce general safety properties on unmanned aviator collision avoidance system ACAS Xu dataset and the Collision Detection dataset. Importantly, we empirically demonstrate that realizing safety does not come at the price of much accuracy. Our methodology demonstrates that an abstraction refinement methodology provides a meaningful pathway for building both accurate and correct machine learning networks.",
    "citation_count": 26,
    "summary": "ART is a novel training framework for neural networks that integrates abstraction refinement to guarantee safety properties by construction, achieving provably correct networks without significant accuracy loss. This is accomplished by iteratively refining input space partitions based on safety verification efficacy, synergistically balancing safety and accuracy objectives."
  },
  {
    "url": "https://arxiv.org/abs/2409.09687",
    "title": "Training Safe Neural Networks with Global SDP Bounds",
    "published_date": "2024-09-15",
    "abstract": "This paper presents a novel approach to training neural networks with formal safety guarantees using semidefinite programming (SDP) for verification. Our method focuses on verifying safety over large, high-dimensional input regions, addressing limitations of existing techniques that focus on adversarial robustness bounds. We introduce an ADMM-based training scheme for an accurate neural network classifier on the Adversarial Spheres dataset, achieving provably perfect recall with input dimensions up to $d=40$. This work advances the development of reliable neural network verification methods for high-dimensional systems, with potential applications in safe RL policies.",
    "summary": "This paper introduces a novel method for training safe neural networks using semidefinite programming (SDP) to verify safety over large input regions, achieving provably perfect recall on high-dimensional data (up to 40 dimensions) using an ADMM-based training scheme. This approach addresses limitations of existing adversarial robustness verification methods and has implications for safe reinforcement learning."
  },
  {
    "url": "https://arxiv.org/abs/2412.13229",
    "title": "Training Verification-Friendly Neural Networks via Neuron Behavior Consistency",
    "published_date": "2024-12-17",
    "abstract": "Formal verification provides critical security assurances for neural networks, yet its practical application suffers from the long verification time. This work introduces a novel method for training verification-friendly neural networks, which are robust, easy to verify, and relatively accurate. Our method integrates neuron behavior consistency into the training process, making neuron activation states remain consistent across different inputs within a local neighborhood. This reduces the number of unstable neurons and tightens the bounds of neurons thereby enhancing the network's verifiability. We evaluated our method using the MNIST, Fashion-MNIST, and CIFAR-10 datasets with various network architectures. The experimental results demonstrate that networks trained using our method are verification-friendly across different radii and architectures, whereas other tools fail to maintain verifiability as the radius increases. Additionally, we show that our method can be combined with existing approaches to further improve the verifiability of networks.",
    "summary": "This paper presents a novel training method that improves the verifiability of neural networks by enforcing neuron behavior consistency, leading to more stable neuron activations and tighter verification bounds. Experiments demonstrate improved verification performance across various datasets and architectures compared to existing methods, especially as input variations increase."
  },
  {
    "url": "http://arxiv.org/abs/2401.14961",
    "title": "Set-Based Training for Neural Network Verification",
    "published_date": "2024-01-26",
    "abstract": "Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can significantly affect the outputs of a neural network. Therefore, to ensure safety of safety-critical environments, the robustness of a neural network must be formally verified against input perturbations, e.g., from noisy sensors. To improve the robustness of neural networks and thus simplify the formal verification, we present a novel set-based training procedure in which we compute the set of possible outputs given the set of possible inputs and compute for the first time a gradient set, i.e., each possible output has a different gradient. Therefore, we can directly reduce the size of the output enclosure by choosing gradients toward its center. Small output enclosures increase the robustness of a neural network and, at the same time, simplify its formal verification. The latter benefit is due to the fact that a larger size of propagated sets increases the conservatism of most verification methods. Our extensive evaluation demonstrates that set-based training produces robust neural networks with competitive performance, which can be verified using fast (polynomial-time) verification algorithms due to the reduced output set.",
    "citation_count": 1,
    "summary": "This paper introduces a novel set-based training method for neural networks that directly minimizes the range of possible outputs given a set of possible inputs, thereby improving robustness to adversarial attacks and simplifying formal verification. This is achieved by calculating a gradient set and using it to reduce the output enclosure, leading to faster verification times and improved network resilience."
  },
  {
    "url": "https://www.alignmentforum.org/posts/SyeQjjBoEC48MvnQC/formal-verification-heuristic-explanations-and-surprise",
    "author": "Jacob Hilton",
    "title": "Formal verification, heuristic explanations and surprise accounting",
    "published_date": "2024-06-25",
    "summary": "The article discusses the challenges of formally verifying neural networks, arguing that proving guarantees for complex models is unrealistic due to the need to account for all possible interactions. Instead, the authors propose \"heuristic explanations,\" a less rigorous but more practical approach to understanding and quantifying a model's behavior."
  },
  {
    "url": "https://www.lesswrong.com/posts/uSSPuttae5GHfsNQL/ai-compute-governance-verifying-ai-chip-location",
    "author": "Farhan",
    "title": "AI Compute governance: Verifying AI chip location",
    "published_date": "2024-10-12",
    "summary": "This article proposes a delay-based on-chip compute governance mechanism using the speed of light to verify the location of AI chips, aiming to regulate AI development by controlling compute power. However, the mechanism's reliance on network latency introduces potential frequent false positives, a problem the author addresses with a proposed solution."
  },
  {
    "url": "https://www.alignmentforum.org/posts/LkECxpbjvSifPfjnb/towards-guaranteed-safe-ai-a-framework-for-ensuring-robust-1",
    "author": "Joar Skalse",
    "title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems",
    "published_date": "2024-05-17",
    "summary": "There is no article provided to summarize."
  },
  {
    "url": "https://www.alignmentforum.org/posts/xj5nzResmDZDqLuLo/estimating-tail-risk-in-neural-networks",
    "author": "Jacob Hilton; Mark Xu",
    "title": "Estimating Tail Risk in Neural Networks",
    "published_date": "2024-09-13",
    "summary": "Current methods for ensuring AI safety by minimizing catastrophic events rely on identifying harmful inputs, which is insufficient given the vast input space. The article proposes researching alternative methods for estimating the probability of catastrophic tail events that don't depend on finding specific harmful inputs."
  }
]