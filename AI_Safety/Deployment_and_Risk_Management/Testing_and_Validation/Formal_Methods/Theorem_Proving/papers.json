[
  {
    "url": "https://arxiv.org/pdf/2112.10664v1.pdf",
    "title": "Proving Theorems using Incremental Learning and Hindsight Experience Replay",
    "published_date": "2021-12-20",
    "abstract": "Traditional automated theorem provers for first-order logic depend on speed-optimized search and many handcrafted heuristics that are designed to work best over a wide range of domains. Machine learning approaches in literature either depend on these traditional provers to bootstrap themselves or fall short on reaching comparable performance. In this paper, we propose a general incremental learning algorithm for training domain specific provers for first-order logic without equality, based only on a basic given-clause algorithm, but using a learned clause-scoring function. Clauses are represented as graphs and presented to transformer networks with spectral features. To address the sparsity and the initial lack of training data as well as the lack of a natural curriculum, we adapt hindsight experience replay to theorem proving, so as to be able to learn even when no proof can be found. We show that provers trained this way can match and sometimes surpass state-of-the-art traditional provers on the TPTP dataset in terms of both quantity and quality of the proofs.",
    "citation_count": 12,
    "summary": "This paper introduces an incremental learning algorithm for automated theorem proving in first-order logic, using a learned clause-scoring function and hindsight experience replay to overcome data sparsity and achieve performance comparable to or exceeding state-of-the-art traditional provers on the TPTP dataset. The approach avoids reliance on handcrafted heuristics and complex search strategies."
  },
  {
    "url": "https://arxiv.org/abs/2410.16429",
    "title": "Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4",
    "published_date": "2024-10-21",
    "abstract": "Machine-assisted theorem proving refers to the process of conducting structured reasoning to automatically generate proofs for mathematical theorems. Recently, there has been a surge of interest in using machine learning models in conjunction with proof assistants to perform this task. In this paper, we introduce Pantograph, a tool that provides a versatile interface to the Lean 4 proof assistant and enables efficient proof search via powerful search algorithms such as Monte Carlo Tree Search. In addition, Pantograph enables high-level reasoning by enabling a more robust handling of Lean 4's inference steps. We provide an overview of Pantograph's architecture and features. We also report on an illustrative use case: using machine learning models and proof sketches to prove Lean 4 theorems. Pantograph's innovative features pave the way for more advanced machine learning models to perform complex proof searches and high-level reasoning, equipping future researchers to design more versatile and powerful theorem provers.",
    "summary": "Pantograph is a Lean 4 tool facilitating machine-to-machine interaction for advanced theorem proving, employing powerful search algorithms like Monte Carlo Tree Search and enabling robust handling of inference steps for high-level reasoning. It leverages machine learning models and proof sketches to automate theorem proving within the Lean 4 environment."
  },
  {
    "url": "https://arxiv.org/abs/2403.03401",
    "title": "BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving",
    "published_date": "2024-03-06",
    "abstract": "Artificial Intelligence for Theorem Proving (AITP) has given\nrise to a plethora of benchmarks and methodologies, particularly in Interactive Theorem Proving (ITP). Research in the\narea is fragmented, with a diverse set of approaches being\nspread across several ITP systems. This presents a significant challenge to the comparison of methods, which are often\ncomplex and difficult to replicate.\nAddressing this, we present BAIT, a framework for the fair\nand streamlined comparison of learning approaches in ITP.\nWe demonstrate BAIT's capabilities with an in-depth comparison, across several ITP benchmarks, of state-of-the-art\narchitectures applied to the problem of formula embedding.\nWe find that Structure Aware Transformers perform particularly well, improving on techniques associated with the original problem sets. BAIT also allows us to assess the end-to-end proving performance of systems built on interactive\nenvironments. This unified perspective reveals a novel end-to-end system that improves on prior work. We also provide\na qualitative analysis, illustrating that improved performance\nis associated with more semantically-aware embeddings. By\nstreamlining the implementation and comparison of Machine\nLearning algorithms in the ITP context, we anticipate BAIT\nwill be a springboard for future research.",
    "citation_count": 2,
    "summary": "BAIT is a framework for benchmarking machine learning approaches in interactive theorem proving, enabling fair comparison of embedding architectures and revealing that Structure Aware Transformers yield superior performance in several benchmarks, leading to an improved end-to-end proving system."
  },
  {
    "url": "https://arxiv.org/abs/2412.16075",
    "title": "Formal Mathematical Reasoning: A New Frontier in AI",
    "published_date": "2024-12-20",
    "abstract": "AI for Mathematics (AI4Math) is not only intriguing intellectually but also crucial for AI-driven discovery in science, engineering, and beyond. Extensive efforts on AI4Math have mirrored techniques in NLP, in particular, training large language models on carefully curated math datasets in text form. As a complementary yet less explored avenue, formal mathematical reasoning is grounded in formal systems such as proof assistants, which can verify the correctness of reasoning and provide automatic feedback. In this position paper, we advocate for formal mathematical reasoning and argue that it is indispensable for advancing AI4Math to the next level. In recent years, we have seen steady progress in using AI to perform formal reasoning, including core tasks such as theorem proving and autoformalization, as well as emerging applications such as verifiable generation of code and hardware designs. However, significant challenges remain to be solved for AI to truly master mathematics and achieve broader impact. We summarize existing progress, discuss open challenges, and envision critical milestones to measure future success. At this inflection point for formal mathematical reasoning, we call on the research community to come together to drive transformative advancements in this field.",
    "summary": "This paper argues that formal mathematical reasoning, using systems like proof assistants, is crucial for advancing AI in mathematics beyond current text-based approaches, highlighting recent progress while also emphasizing significant remaining challenges. The authors advocate for increased research focus on this less-explored yet vital area to unlock AI's full potential in mathematical discovery and application."
  },
  {
    "title": "Towards AI-Assisted Synthesis of Verified Dafny Methods",
    "abstract": "Large language models show great promise in many domains, including programming. A promise is easy to make but hard to keep, and language models often fail to keep their promises, generating erroneous code. A promising avenue to keep models honest is to incorporate formal verification: generating programs' specifications as well as code so that the code can be proved correct with respect to the specifications. Unfortunately, existing large language models show a severe lack of proficiency in verified programming. \n \n \n \n \n \n \n \nIn this paper, we demonstrate how to improve two pretrained models' proficiency in the Dafny verification-aware language. Using 178 problems from the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to synthesize Dafny methods. We use three different types of prompts: a direct Contextless prompt; a Signature prompt that includes a method signature and test cases, and a Chain of Thought (CoT) prompt that decomposes the problem into steps and includes retrieval augmentation generated example problems and solutions. Our results show that GPT-4 performs better than PaLM-2 on these tasks and that both models perform best with the retrieval augmentation generated CoT prompt. GPT-4 was able to generate verified, human-evaluated, Dafny methods for 58% of the problems, however, GPT-4 managed only 19% of the problems with the Contextless prompt, and even fewer (10%) for the Signature prompt. We are thus able to contribute 153 verified Dafny solutions to MBPP problems, 50 that we wrote manually, and 103 synthesized by GPT-4. \n \n \n \n \n \n \n \nOur results demonstrate that the benefits of formal program verification are now within reach of code generating large language models. Likewise, program verification systems can benefit from large language models, whether to synthesize code wholesale, to generate specifications, or to act as a \"programmer's verification apprentice\", to construct annotations such as loop invariants which are hard for programmers to write or verification tools to find. Finally, we expect that the approach we have pioneered here — generating candidate solutions that are subsequently formally checked for correctness — should transfer to other domains (e.g., legal arguments, transport signaling, structural engineering) where solutions must be correct, where that correctness must be demonstrated, explained and understood by designers and end-users.",
    "published_date": "2024-02-01",
    "citation_count": 19,
    "url": "https://dl.acm.org/doi/10.1145/3643763",
    "summary": "This paper investigates using large language models (LLMs) like GPT-4 and PaLM-2 to generate formally verified Dafny code, finding that prompting techniques significantly impact success rates. Retrieval-augmented Chain of Thought prompting improved GPT-4's ability to generate verified solutions to 58% of problems from the MBPP dataset, highlighting the potential of LLMs for AI-assisted program verification."
  },
  {
    "url": "https://arxiv.org/abs/2409.05977",
    "title": "Mathematical Formalized Problem Solving and Theorem Proving in Different Fields in Lean 4",
    "published_date": "2024-09-09",
    "abstract": "Formalizing mathematical proofs using computerized verification languages like Lean 4 has the potential to significantly impact the field of mathematics, it offers prominent capabilities for advancing mathematical reasoning. However, existing efforts are largely limited to creating formalized versions of proofs from extensive online mathematical corpora, struggling to keep pace with the rapidly evolving nature of mathematics. To bridge the gap between traditional and computerized proof techniques, this paper explores the use of Large Language Models (LLMs) to generate formal proof steps and complete formalized proofs. By converting natural language (NL) mathematical proofs into formalized versions, this work introduces the basic structure and tactics of the Lean 4 language. The goal is to determine how AI can be leveraged to assist the mathematical formalization process and improve its performance. Several examples are provided that demonstrate solving problems using both traditional and Lean 4-based approaches. Ultimately, this paper presents an explanation of the foundations of Lean 4 and comparative analyses of the mathematical formalization process using traditional and AI-augmented techniques. The findings indicate that AI- powered tools have significant potential to accelerate and enhance the formalization of mathematical proofs, paving the way for more efficient and reliable theorem-proving for AI for Math in the future.",
    "summary": "This paper investigates using Large Language Models (LLMs) to automate the formalization of mathematical proofs in Lean 4, aiming to bridge the gap between traditional and computer-assisted proof techniques and accelerate the process of mathematical theorem proving. The authors demonstrate the potential of AI to enhance the efficiency and reliability of formal proof generation."
  },
  {
    "url": "https://www.alignmentforum.org/posts/LkECxpbjvSifPfjnb/towards-guaranteed-safe-ai-a-framework-for-ensuring-robust-1",
    "author": "Joar Skalse",
    "title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems",
    "published_date": "2024-05-17",
    "summary": "There is no article provided to summarize."
  },
  {
    "url": "https://www.alignmentforum.org/posts/B2bg677TaS4cmDPzL/limitations-on-formal-verification-for-ai-safety",
    "author": "Andrew Dickson",
    "title": "Limitations on Formal Verification for AI Safety",
    "published_date": "2024-08-19",
    "summary": "The article argues that claims of using formal verification to guarantee AI safety are overly optimistic. The inherent complexity of the real world, encompassing physics, biology, and unpredictable human factors, makes achieving the strong guarantees needed to prevent catastrophic AI risks through formal verification practically impossible in the near term."
  }
]