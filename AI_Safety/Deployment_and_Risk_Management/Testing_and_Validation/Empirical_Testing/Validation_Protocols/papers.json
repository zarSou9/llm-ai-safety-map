[
  {
    "url": "https://arxiv.org/pdf/2309.05381.pdf",
    "title": "Hazards in Deep Learning Testing: Prevalence, Impact and Recommendations",
    "published_date": "2023-09-11",
    "abstract": "Much research on Machine Learning testing relies on empirical studies that evaluate and show their potential. However, in this context empirical results are sensitive to a number of parameters that can adversely impact the results of the experiments and potentially lead to wrong conclusions (Type I errors, i.e., incorrectly rejecting the Null Hypothesis). To this end, we survey the related literature and identify 10 commonly adopted empirical evaluation hazards that may significantly impact experimental results. We then perform a sensitivity analysis on 30 influential studies that were published in top-tier SE venues, against our hazard set and demonstrate their criticality. Our findings indicate that all 10 hazards we identify have the potential to invalidate experimental findings, such as those made by the related literature, and should be handled properly. Going a step further, we propose a point set of 10 good empirical practices that has the potential to mitigate the impact of the hazards. We believe our work forms the first step towards raising awareness of the common pitfalls and good practices within the software engineering community and hopefully contribute towards setting particular expectations for empirical research in the field of deep learning testing.",
    "summary": "This paper identifies ten common hazards in empirical deep learning testing research that can lead to inaccurate conclusions, demonstrating their prevalence through a sensitivity analysis of 30 influential studies and proposing ten good practices to mitigate these risks. The work aims to improve the rigor and reliability of empirical research in deep learning testing."
  },
  {
    "url": "https://arxiv.org/abs/2212.11368",
    "title": "When and Why Test Generators for Deep Learning Produce Invalid Inputs: an Empirical Study",
    "published_date": "2022-12-21",
    "abstract": "Testing Deep Learning (DL) based systems inherently requires large and representative test sets to evaluate whether DL systems generalise beyond their training datasets. Diverse Test Input Generators (TIGs) have been proposed to produce artificial inputs that expose issues of the DL systems by triggering misbehaviours. Unfortunately, such generated inputs may be invalid, i.e., not recognisable as part of the input domain, thus providing an unreliable quality assessment. Automated validators can ease the burden of manually checking the validity of inputs for human testers, although input validity is a concept difficult to formalise and, thus, automate. In this paper, we investigate to what extent TIGs can generate valid inputs, according to both automated and human validators. We conduct a large empirical study, involving 2 different automated validators, 220 human assessors, 5 different TIGs and 3 classification tasks. Our results show that 84% artificially generated inputs are valid, according to automated validators, but their expected label is not always preserved. Automated validators reach a good consensus with humans (78% accuracy), but still have limitations when dealing with feature-rich datasets.",
    "citation_count": 23,
    "summary": "This study empirically investigates the validity of test inputs generated by five different Deep Learning test input generators (TIGs) across three classification tasks, finding that while automated validators deem 84% of generated inputs valid, human assessment reveals limitations, particularly with complex datasets. A significant discrepancy exists between automated and human validation of input label preservation."
  },
  {
    "url": "https://arxiv.org/pdf/2103.01281.pdf",
    "title": "Validation of cluster analysis results on validation data: A systematic framework",
    "published_date": "2021-03-01",
    "abstract": "Cluster analysis refers to a wide range of data analytic techniques for class discovery and is popular in many application fields. To assess the quality of a clustering result, different cluster validation procedures have been proposed in the literature. While there is extensive work on classical validation techniques, such as internal and external validation, less attention has been given to validating and replicating a clustering result using a validation dataset. Such a dataset may be part of the original dataset, which is separated before analysis begins, or it could be an independently collected dataset. We present a systematic, structured review of the existing literature about this topic. For this purpose, we outline a formal framework that covers most existing approaches for validating clustering results on validation data. In particular, we review classical validation techniques such as internal and external validation, stability analysis, and visual validation, and show how they can be interpreted in terms of our framework. We define and formalize different types of validation of clustering results on a validation dataset, and give examples of how clustering studies from the applied literature that used a validation dataset can be seen as instances of our framework.",
    "citation_count": 55,
    "summary": "This paper presents a systematic framework for validating cluster analysis results using a separate validation dataset, reviewing existing techniques like internal, external validation, and stability analysis, and showing how they fit within this framework. The framework formalizes different validation types and illustrates their application using examples from the literature."
  },
  {
    "url": "https://arxiv.org/pdf/2109.12854.pdf",
    "title": "Quality Control Methodology for Simulation Models of Computer Network Protocols",
    "published_date": "2021-09-27",
    "abstract": "This paper summarizes know-how about modeling and simulation of computer networking protocols we contributed to the OMNeT++ community. We propose a methodology aiming to set a reliable ground truth for the quality of simulation models of networking protocols. We demonstrate the application of this methodology on our EIGRP source code pull-requested to the INET framework.",
    "citation_count": 1,
    "summary": "This paper presents a quality control methodology for computer network protocol simulation models, using the OMNeT++ framework and demonstrated through an EIGRP implementation. The methodology aims to establish a reliable benchmark for model accuracy."
  },
  {
    "title": "Towards rigorous validation of energy optimisation experiments",
    "abstract": "The optimisation of software energy consumption is of growing importance across all scales of modern computing, i.e., from embedded systems to data-centres. Practitioners in the field of Search-Based Software Engineering and Genetic Improvement of Software acknowledge that optimising software energy consumption is difficult due to noisy and expensive fitness evaluations. However, it is apparent from results to date that more progress needs to be made in rigorously validating optimisation results. This problem is pressing because modern computing platforms have highly complex and variable behaviour with respect to energy consumption. To compare solutions fairly we propose in this paper a new validation approach called R3-validation which exercises software variants in a rotated-round-robin order. Using a case study, we present an in-depth analysis of the impacts of changing system states on software energy usage, and we show how R3-validation mitigates these. We compare it with current validation approaches across multiple devices and operating systems, and we show that it aligns best with actual platform behaviour.",
    "published_date": "2020-04-09",
    "citation_count": 13,
    "url": "https://dl.acm.org/doi/10.1145/3377930.3390245",
    "summary": "This paper introduces R3-validation, a novel approach for rigorously validating software energy optimization results by mitigating the impact of fluctuating system states through a rotated-round-robin execution order. The authors demonstrate its superior performance compared to existing methods across various devices and operating systems."
  },
  {
    "url": "https://arxiv.org/abs/2010.03525",
    "title": "ACM SIGSOFT Empirical Standards",
    "published_date": "2020-10-07",
    "abstract": "Empirical Standards are brief public document that communicate expectations for a specific kind of study (e.g. a questionnaire survey). The ACM SIGSOFT Paper and Peer Review Quality Initiative generated empirical standards for common research methods in software engineering. These living documents, which should be continuously revised to reflect evolving consensus around research best practices, can be used to make peer review more transparent, structured, harmonious and fair.",
    "citation_count": 41,
    "summary": "ACM SIGSOFT developed empirical standards—short, publicly available documents outlining expectations for various software engineering research methods—to improve the transparency, structure, and fairness of peer review. These living documents are intended to be continuously updated to reflect best practices."
  },
  {
    "url": "https://www.lesswrong.com/posts/QJfiKwicwTXYMzJ7q/",
    "author": "jefftk",
    "title": "Bleach",
    "published_date": "2020-03-04",
    "summary": "Household chlorine bleach, while effective for disinfection and sanitization when properly diluted, requires careful consideration due to varying concentrations and degradation over time. Safe handling practices, including protective gear and avoidance of mixing with other chemicals, are crucial due to its corrosive and toxic properties."
  },
  {
    "url": "https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2",
    "author": "Kaj_Sotala",
    "title": "Dual Process Theory (System 1 & System 2) - LessWrong",
    "published_date": "2019-11-12",
    "summary": "Dual Process Theory describes two types of cognitive processes: Type 1 (fast, intuitive) and Type 2 (slow, deliberative), though the \"System 1/System 2\" terminology is considered misleading due to the heterogeneity of processes within each type. Neither type is inherently more biased; Type 2's perceived unbiasedness stems from its capacity to correct Type 1 errors using learned rules."
  }
]