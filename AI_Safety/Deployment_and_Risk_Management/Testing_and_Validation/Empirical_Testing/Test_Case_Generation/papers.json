[
  {
    "url": "https://arxiv.org/abs/2410.00752",
    "title": "TestGenEval: A Real World Unit Test Generation and Test Completion Benchmark",
    "published_date": "2024-10-01",
    "abstract": "Code generation models can help improve many common software tasks ranging from code completion to defect prediction. Most of the existing benchmarks for code generation LLMs focus on code authoring or code completion. Surprisingly, there has been far less effort dedicated to benchmarking software testing, despite the strong correlation between well-tested software and effective bug detection. To address this gap, we create and release TestGenEval, a large-scale benchmark to measure test generation performance. Based on SWEBench, TestGenEval comprises 68,647 tests from 1,210 code and test file pairs across 11 well-maintained Python repositories. It covers initial tests authoring, test suite completion, and code coverage improvements. Test authoring simulates the process of a developer writing a test suite from scratch, while test completion mimics the scenario where a developer aims to improve the coverage of an existing test suite. We evaluate several popular models, with sizes ranging from 7B to 405B parameters. Our detailed analysis highlights TestGenEval's contribution to a comprehensive evaluation of test generation performance. In particular, models struggle to generate high-coverage test suites, with the best model, GPT-4o, achieving an average coverage of only 35.2%. This is primarily due to models struggling to reason about execution, and their frequent assertion errors when addressing complex code paths.",
    "citation_count": 1,
    "summary": "TestGenEval is a new benchmark for evaluating code generation models' ability to create and complete unit tests, revealing that even large language models struggle to generate high-coverage tests due to difficulties reasoning about code execution. The benchmark comprises 68,647 tests from 11 Python repositories and evaluates models on test authoring and completion tasks."
  },
  {
    "url": "https://arxiv.org/abs/2408.11324",
    "title": "HITS: High-coverage LLM-based Unit Test Generation via Method Slicing",
    "published_date": "2024-08-21",
    "abstract": "Large language models (LLMs) have behaved well in generating unit tests for Java projects. However, the performance for covering the complex focal methods within the projects is poor. Complex methods comprise many conditions and loops, requiring the test cases to be various enough to cover all lines and branches. However, existing test generation methods with LLMs provide the whole method-to-test to the LLM without assistance on input analysis. The LLM has difficulty inferring the test inputs to cover all conditions, resulting in missing lines and branches. To tackle the problem, we propose decomposing the focal methods into slices and asking the LLM to generate test cases slice by slice. Our method simplifies the analysis scope, making it easier for the LLM to cover more lines and branches in each slice. We build a dataset comprising complex focal methods collected from the projects used by existing state-of-the-art approaches. Our experiment results show that our method significantly outperforms current test case generation methods with LLMs and the typical SBST method Evosuite regarding both line and branch coverage scores.CCS CONCEPTS• Software and its engineering → Software testing and debugging; • Computing methodologies → Natural language processing.",
    "citation_count": 5,
    "summary": "HITS improves LLM-based unit test generation for complex Java methods by employing method slicing to decompose methods into smaller, more manageable slices, enabling LLMs to generate more comprehensive test cases with significantly higher line and branch coverage. This approach outperforms existing LLM-based and traditional methods like Evosuite."
  },
  {
    "url": "https://www.lesswrong.com/tag/knuths-up-arrow-notation",
    "title": "Knuth's Up-Arrow Notation - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Knuth's up-arrow notation provides a concise way to represent extremely large numbers, exemplified by 3^^^3, a power tower of threes 7625597484987 high, demonstrating that a short description can encompass incomprehensibly vast quantities."
  },
  {
    "url": "https://arxiv.org/pdf/2301.08134.pdf",
    "title": "A Benchmark Generator for Combinatorial Testing",
    "published_date": "2022-12-29",
    "abstract": "Combinatorial Testing (CT) tools are essential to test properly a wide range of systems (train systems, Graphical User Interfaces (GUIs), autonomous driving systems, etc). While there is an active research community working on developing CT tools, paradoxically little attention has been paid to making available enough resources to test the CT tools themselves. In particular, the set of available benchmarks to asses their correctness, effectiveness and efficiency is rather limited. In this paper, we introduce a new generator of CT benchmarks that essentially borrows the structure contained in the plethora of available Combinatorial Problems from other research communities in order to create meaningful benchmarks. We additionally perform an extensive evaluation of CT tools with these new benchmarks. Thanks to this study we provide some insights on under which circumstances a particular CT tool should be used.",
    "citation_count": 1,
    "summary": "This paper introduces a novel benchmark generator for combinatorial testing tools, leveraging existing combinatorial problems to create a more comprehensive evaluation suite and provide insights into tool performance and optimal usage scenarios. The generator addresses the scarcity of resources for testing combinatorial testing tools themselves."
  },
  {
    "title": "Enhancing Search-based Testing with Testability Transformations for Existing APIs",
    "abstract": "\n Search-based software testing (SBST) has been shown to be an effective technique to generate test cases automatically. Its effectiveness strongly depends on the guidance of the fitness function. Unfortunately, a common issue in SBST is the so-called\n flag problem\n , where the fitness landscape presents a plateau that provides no guidance to the search. In this article, we provide a series of novel\n testability transformations\n aimed at providing guidance in the context of commonly used API calls (e.g., strings that need to be converted into valid date/time objects). We also provide specific transformations aimed at helping the testing of REST Web Services. We implemented our novel techniques as an extension to\n EvoMaster\n , an SBST tool that generates system-level test cases. Experiments on nine open-source REST web services, as well as an industrial web service, show that our novel techniques improve performance significantly.\n",
    "published_date": "2022-01-31",
    "citation_count": 27,
    "url": "https://dl.acm.org/doi/10.1145/3477271",
    "summary": "This paper introduces novel testability transformations to enhance search-based software testing (SBST), specifically addressing the \"flag problem\" by improving fitness function guidance for API calls, particularly in REST web services. Experiments demonstrate significant performance improvements using these transformations within the EvoMaster SBST tool."
  },
  {
    "title": "Automated Software Test Data Generation With Generative Adversarial Networks",
    "abstract": "With the rapid increase of software scale and complexity, the cost of traditional software testing methods will increase faster than the scale of software. In order to improve test efficiency, it is particularly important to automatically generate high-quality test cases. This paper introduces a framework for automatic test data generation based on the generative adversarial network (GAN). GAN is employed to train a generative model over execution path information to learn the behavior of the software. Then we can use the trained generative model to produce new test data, and select the test data that can improve the branch coverage according to our proposed selection strategy. Compared to prior work, our proposed method is able to handle programs under test with large-scale branches without analyzing branch expressions. In the experiment, we exhibit the performance of our method by using two modules in GNU Scientific Library. In particular, we consider the application of our method in two testing scenarios; unit testing and integration testing, and conduct a series of experiments to compare the performance of three types of GAN models. Results indicate that the WGAN-GP shows the best performance in our framework. Compared with the random testing method, the WGAN-GP based framework improves the test coverage of five functions out of the seven in the unit testing.",
    "published_date": "2022-01-01",
    "citation_count": 16,
    "url": "https://ieeexplore.ieee.org/ielx7/6287639/9668973/09718329.pdf",
    "summary": "This paper proposes a framework for automated software test data generation using Generative Adversarial Networks (GANs), leveraging execution path information to generate test data that improves branch coverage without requiring analysis of branch expressions. Experiments using GNU Scientific Library modules demonstrate the effectiveness of the WGAN-GP model within this framework, particularly in enhancing test coverage compared to random testing."
  },
  {
    "url": "https://arxiv.org/pdf/2203.09596v2.pdf",
    "title": "Prioritized Variable-length Test Cases Generation for Finite State Machines",
    "published_date": "2022-03-17",
    "abstract": "Model-based Testing (MBT) is an effective approach for testing when parts of a system-under-test have the characteristics of a finite state machine (FSM). Despite various strategies in the literature on this topic, little work exists to handle special testing situations. More specifically, when concurrently: (1) the test paths can start and end only in defined states of the FSM, (2) a prioritization mechanism that requires only defined states and transitions of the FSM to be visited by test cases is required, and (3) the test paths must be in a given length range, not necessarily of explicit uniform length. This paper presents a test generation strategy that satisfies all these requirements. A concurrent combination of these requirements is highly practical for real industrial testing. Six variants of possible algorithms to implement this strategy are described. Using a mixture of 180 problem instances from real automotive and defense projects and artificially generated FSMs, all variants are compared with a baseline strategy based on an established N-switch coverage concept modification. Various properties of the generated test paths and their potential to activate fictional defects defined in FSMs are evaluated. The presented strategy outperforms the baseline in most problem configurations. Out of the six analyzed variants, three give the best results even though a universal best performer is hard to identify. Depending on the application of the FSM, the strategy and evaluation presented in this paper are applicable both in testing functional and non-functional software requirements.",
    "citation_count": 1,
    "summary": "This paper introduces a novel test case generation strategy for finite state machines that handles prioritized, variable-length test paths starting and ending in specified states, outperforming a baseline approach across various real-world and artificial problem instances. Six algorithm variants are presented and compared, demonstrating effectiveness in both functional and non-functional software testing."
  },
  {
    "url": "https://arxiv.org/pdf/2204.05561v1.pdf",
    "title": "Toward Granular Automatic Unit Test Case Generation",
    "published_date": "2022-04-12",
    "abstract": "Unit testing verifies the presence of faults in individual software components. Previous research has been targeting the automatic generation of unit tests through the adoption of random or search-based algorithms. Despite their effectiveness, these approaches do not implement any strategy that allows them to create unit tests in a structured manner: indeed, they aim at creating tests by optimizing metrics like code coverage without ensuring that the resulting tests follow good design principles. In order to structure the automatic test case generation process, we propose a two-step systematic approach to the generation of unit tests: we first force search-based algorithms to create tests that cover individual methods of the production code, hence implementing the so-called intra-method tests; then, we relax the constraints to enable the creation of intra-class tests that target the interactions among production code methods.",
    "summary": "This paper proposes a two-step approach to automatic unit test generation, first generating tests focusing on individual methods (intra-method) and then expanding to cover interactions between methods within a class (intra-class), aiming for a more structured and principled approach than purely random or search-based methods."
  }
]