[
  {
    "url": "https://www.alignmentforum.org/posts/XSqntCNMafhcy9irf/third-party-testing-as-a-key-ingredient-of-ai-policy",
    "author": "Zac Hatfield-Dodds",
    "title": "Third-party testing as a key ingredient of AI policy",
    "published_date": "2024-03-25",
    "summary": "The article advocates for a third-party testing regime for large-scale, generative AI systems to mitigate societal harm from misuse or accidental consequences. This regime, involving industry, government, and academia, aims to build trust, avoid overly burdensome regulations for smaller companies, and facilitate international cooperation."
  },
  {
    "title": "Testing deep neural networks (keynote)",
    "abstract": "The reliability of software that has a Deep Neural Network (DNN) as a component is urgently important today given the increasing number of critical applications being deployed with DNNs. The need for reliability raises a need for rigorous testing of the safety and trustworthiness of these systems. In the last few years, there have been a number of research efforts focused on testing DNNs. However, the test generation techniques proposed so far lack a check to determine whether the test inputs they are generating are valid, and thus invalid inputs are produced. To illustrate this situation, we explored three recent DNN testing techniques. Using deep generative model based input validation, we show that all the three techniques generate significant number of invalid test inputs. We further analyzed the test coverage achieved by the test inputs generated by the DNN testing techniques and showed how invalid test inputs can falsely inflate test coverage metrics. To overcome the inclusion of invalid inputs in testing, we propose a technique to incorporate the valid input space of the DNN model under test in the test generation process. Our technique uses a deep generative model-based algorithm to generate only valid inputs. Results of our empirical studies show that our technique is effective in eliminating invalid tests and boosting the number of valid test inputs generated.",
    "published_date": "2020-11-15",
    "url": "https://dl.acm.org/doi/10.1145/3426430.3434071",
    "summary": "This paper highlights the problem of invalid test inputs in existing deep neural network (DNN) testing techniques, demonstrating that these techniques often generate many invalid inputs that inflate coverage metrics. The authors propose a new technique using deep generative models to generate only valid test inputs, improving the reliability and accuracy of DNN testing."
  },
  {
    "title": "Safety and Robustness for Deep Learning with Provable Guarantees",
    "abstract": "Computing systems are becoming ever more complex, with decisions increasingly often based on deep learning components. A wide variety of applications are being developed, many of them safety-critical, such as self-driving cars and medical diagnosis. Since deep learning is unstable with respect to adversarial perturbations, there is a need for rigorous software development methodologies that encompass machine learning components. This lecture will describe progress with developing automated verification and testing techniques for deep neural networks to ensure safety and robustness of their decisions with respect to bounded input perturbations. The techniques exploit Lipschitz continuity of the networks and aim to approximate, for a given set of inputs, the reachable set of network outputs in terms of lower and upper bounds, in anytime manner, with provable guarantees. We develop novel algorithms based on feature-guided search, games, global optimisation and Bayesian methods, and evaluate them on state-of-the-art networks. The lecture will conclude with an overview of the challenges in this field.",
    "published_date": "2020-09-01",
    "citation_count": 4,
    "url": "https://dl.acm.org/doi/10.1145/3324884.3418901",
    "summary": "This lecture presents progress in automated verification and testing of deep neural networks to ensure safety and robustness against input perturbations, focusing on techniques that provide provable guarantees on output bounds using methods like Lipschitz continuity and feature-guided search. The work aims to improve the reliability of deep learning in safety-critical applications."
  },
  {
    "url": "https://www.lesswrong.com/posts/324pQjqoHEHeF2vPs/ai-clarity-an-initial-research-agenda",
    "author": "Justin Bullock, Corin Katzke, Zershaaneh Qureshi, David_Kristoffersson",
    "title": "AI Clarity: An Initial Research Agenda",
    "published_date": "2024-05-03",
    "summary": "The AI Clarity research program uses scenario planning to explore potential pathways to existential risks from transformative AI (TAI), focusing on short timelines (within a decade). The program will then evaluate strategies for AI safety and governance across these scenarios to identify those best mitigating existential risk."
  },
  {
    "url": "https://www.lesswrong.com/posts/2eaLH7zp6pxdQwYSH",
    "author": "Austin Witte",
    "title": "A Brief Overview of AI Safety/Alignment Orgs, Fields, Researchers, and Resources for ML Researchers",
    "published_date": "2023-02-02",
    "summary": "Two overview documents, a short and a long version, have been created to help machine learning researchers quickly assess the AI safety field and identify relevant research areas based on their existing skills. These resources list organizations, researchers, key papers, and keywords to facilitate efficient exploration of the field."
  },
  {
    "url": "https://www.alignmentforum.org/tag/ai",
    "author": "Evan Hubinger",
    "title": "AI - AI Alignment Forum",
    "published_date": "2023-02-06",
    "summary": "Artificial intelligence alignment focuses on ensuring powerful AI systems act in accordance with human values, addressing the risk of unintended consequences and existential threats. This involves diverse approaches, from narrowly defining AI goals to achieving a broadly beneficial future for humanity."
  },
  {
    "url": "https://www.alignmentforum.org/tag/organization-updates",
    "author": "Jesse Hoogland, Daniel Murfet, Stan van Wingerden, Alexander Gietelink Oldenziel",
    "title": "Organization Updates - AI Alignment Forum",
    "published_date": "2023-05-30",
    "summary": "Organization updates pertain to news and changes within a specific group or organization. The term describes information relevant to that entity's members or stakeholders."
  },
  {
    "url": "https://arxiv.org/abs/2205.08589v1",
    "title": "Hierarchical Distribution-aware Testing of Deep Learning",
    "published_date": "2022-05-17",
    "abstract": "With its growing use in safety/security-critical applications, Deep Learning (DL) has raised increasing concerns regarding its dependability. In particular, DL has a notorious problem of lacking robustness. Input added with adversarial perturbations, i.e., Adversarial Examples (AEs), are easily mispredicted by the DL model. Despite recent efforts made in detecting AEs via state-of-the-art attack and testing methods, they are normally input distribution–agnostic and/or disregard the perceptual quality of adversarial perturbations. Consequently, the detected AEs are irrelevant inputs in the application context or noticeably unrealistic to humans. This may lead to a limited effect on improving the DL model's dependability, as the testing budget is likely to be wasted on detecting AEs that are encountered very rarely in its real-life operations. In this article, we propose a new robustness testing approach for detecting AEs that considers both the feature-level distribution and the pixel-level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. First, we select test seeds based on the density of feature-level distribution and the vulnerability of adversarial robustness. The vulnerability of test seeds is indicated by the auxiliary information, which are highly correlated with local robustness. Given a test seed, we then develop a novel genetic algorithm–based local test case generation method, in which two fitness functions work alternatively to control the perceptual quality of detected AEs. Finally, extensive experiments confirm that our holistic approach considering hierarchical distributions is superior to the state-of-the-arts that either disregard any input distribution or only consider a single (non-hierarchical) distribution, in terms of not only detecting imperceptible AEs but also improving the overall robustness of the DL model under testing.",
    "citation_count": 7,
    "summary": "This paper introduces a novel hierarchical distribution-aware testing approach for deep learning models that improves robustness testing by considering both feature-level and pixel-level input distributions, generating perceptually realistic adversarial examples, and ultimately enhancing model dependability. The method uses a genetic algorithm and auxiliary information to guide the generation of these examples, outperforming existing distribution-agnostic methods."
  },
  {
    "url": "https://arxiv.org/pdf/2201.05647v1.pdf",
    "title": "Tools and Practices for Responsible AI Engineering",
    "published_date": "2022-01-14",
    "abstract": "Responsible Artificial Intelligence (AI) - the practice of developing, evaluating, and maintaining accurate AI systems that also exhibit essential properties such as robustness and explainability - represents a multifaceted challenge that often stretches standard machine learning tooling, frameworks, and testing methods beyond their limits. In this paper, we present two new software libraries - hydra-zen and the rAI-toolbox - that address critical needs for responsible AI engineering. hydra-zen dramatically simplifies the process of making complex AI applications configurable, and their behaviors reproducible. The rAI-toolbox is designed to enable methods for evaluating and enhancing the robustness of AI-models in a way that is scalable and that composes naturally with other popular ML frameworks. We describe the design principles and methodologies that make these tools effective, including the use of property-based testing to bolster the reliability of the tools themselves. Finally, we demonstrate the composability and flexibility of the tools by showing how various use cases from adversarial robustness and explainable AI can be concisely implemented with familiar APIs.",
    "citation_count": 14,
    "summary": "This paper introduces hydra-zen and the rAI-toolbox, two software libraries designed to improve responsible AI engineering by simplifying configuration, ensuring reproducibility, and enabling scalable robustness evaluation of AI models. The libraries utilize property-based testing and compose well with existing machine learning frameworks."
  },
  {
    "url": "https://arxiv.org/abs/2212.11368",
    "title": "When and Why Test Generators for Deep Learning Produce Invalid Inputs: an Empirical Study",
    "published_date": "2022-12-21",
    "abstract": "Testing Deep Learning (DL) based systems inherently requires large and representative test sets to evaluate whether DL systems generalise beyond their training datasets. Diverse Test Input Generators (TIGs) have been proposed to produce artificial inputs that expose issues of the DL systems by triggering misbehaviours. Unfortunately, such generated inputs may be invalid, i.e., not recognisable as part of the input domain, thus providing an unreliable quality assessment. Automated validators can ease the burden of manually checking the validity of inputs for human testers, although input validity is a concept difficult to formalise and, thus, automate. In this paper, we investigate to what extent TIGs can generate valid inputs, according to both automated and human validators. We conduct a large empirical study, involving 2 different automated validators, 220 human assessors, 5 different TIGs and 3 classification tasks. Our results show that 84% artificially generated inputs are valid, according to automated validators, but their expected label is not always preserved. Automated validators reach a good consensus with humans (78% accuracy), but still have limitations when dealing with feature-rich datasets.",
    "citation_count": 23,
    "summary": "This study empirically investigates the validity of test inputs generated by five different Deep Learning test input generators across three classification tasks, finding that while automated validators deem 84% of generated inputs valid, human assessment reveals limitations, particularly with complex datasets, highlighting challenges in automated validation of artificial test data."
  },
  {
    "title": "Evaluating Surprise Adequacy for Deep Learning System Testing",
    "abstract": "The rapid adoption of Deep Learning (DL) systems in safety critical domains such as medical imaging and autonomous driving urgently calls for ways to test their correctness and robustness. Borrowing from the concept of test adequacy in traditional software testing, existing work on testing of DL systems initially investigated DL systems from structural point of view, leading to a number of coverage metrics. Our lack of understanding of the internal mechanism of Deep Neural Networks (DNNs), however, means that coverage metrics defined on the Boolean dichotomy of coverage are hard to intuitively interpret and understand. We propose the degree of out-of-distribution-ness of a given input as its adequacy for testing: the more surprising a given input is to the DNN under test, the more likely the system will show unexpected behavior for the input. We develop the concept of surprise into a test adequacy criterion, called Surprise Adequacy (SA). Intuitively, SA measures the difference in the behavior of the DNN for the given input and its behavior for the training data. We posit that a good test input should be sufficiently, but not overtly, surprising compared to the training dataset. This article evaluates SA using a range of DL systems from simple image classifiers to autonomous driving car platforms, as well as both small and large data benchmarks ranging from MNIST to ImageNet. The results show that the SA value of an input can be a reliable predictor of the correctness of the mode behavior. We also show that SA can be used to detect adversarial examples, and also be efficiently computed against large training dataset such as ImageNet using sampling.",
    "published_date": "2022-07-06",
    "citation_count": 14,
    "url": "https://dl.acm.org/doi/10.1145/3546947",
    "summary": "This paper introduces Surprise Adequacy (SA), a novel test adequacy criterion for deep learning systems, measuring the degree to which a test input deviates from the training data distribution to predict unexpected system behavior. Experiments across diverse DL systems and datasets demonstrate SA's effectiveness in identifying correctness issues and adversarial examples."
  },
  {
    "url": "https://arxiv.org/pdf/2101.10430.pdf",
    "title": "Test and Evaluation Framework for Multi-Agent Systems of Autonomous Intelligent Agents",
    "published_date": "2021-01-25",
    "abstract": "Test and evaluation is a necessary process for ensuring that engineered systems perform as intended under a variety of conditions, both expected and unexpected. In this work, we consider the unique challenges of developing a unifying test and evaluation framework for complex ensembles of cyber-physical systems with embedded artificial intelligence. We propose a framework that incorporates test and evaluation throughout not only the development life cycle, but continues into operation as the system learns and adapts in a noisy, changing, and contended environment. The framework accounts for the challenges of testing the integration of diverse systems at various hierarchical scales of composition while respecting that testing time and resources are limited. A generic use case is provided for illustrative purposes. Research directions emerging as a result of exploring the use case via the framework are suggested.",
    "citation_count": 6,
    "summary": "This paper proposes a unifying test and evaluation framework for complex multi-agent systems, addressing the challenges of testing integrated cyber-physical systems with AI throughout their lifecycle, from development to ongoing operation in dynamic environments. The framework considers resource limitations and integrates testing at multiple hierarchical scales."
  },
  {
    "url": "https://arxiv.org/pdf/2102.13602v1.pdf",
    "title": "Distribution-Aware Testing of Neural Networks Using Generative Models",
    "published_date": "2021-02-26",
    "abstract": "The reliability of software that has a Deep Neural Network (DNN) as a component is urgently important today given the increasing number of critical applications being deployed with DNNs. The need for reliability raises a need for rigorous testing of the safety and trustworthiness of these systems. In the last few years, there have been a number of research efforts focused on testing DNNs. However the test generation techniques proposed so far lack a check to determine whether the test inputs they are generating are valid, and thus invalid inputs are produced. To illustrate this situation, we explored three recent DNN testing techniques. Using deep generative model based input validation, we show that all the three techniques generate significant number of invalid test inputs. We further analyzed the test coverage achieved by the test inputs generated by the DNN testing techniques and showed how invalid test inputs can falsely inflate test coverage metrics. To overcome the inclusion of invalid inputs in testing, we propose a technique to incorporate the valid input space of the DNN model under test in the test generation process. Our technique uses a deep generative model-based algorithm to generate only valid inputs. Results of our empirical studies show that our technique is effective in eliminating invalid tests and boosting the number of valid test inputs generated.",
    "citation_count": 48,
    "summary": "This paper highlights the problem of invalid test inputs generated by existing Deep Neural Network (DNN) testing techniques, demonstrating that these invalid inputs inflate coverage metrics. The authors propose a solution using deep generative models to ensure test inputs are valid, improving the reliability of DNN testing."
  },
  {
    "url": "https://arxiv.org/pdf/2104.04999v1.pdf",
    "title": "ALT-MAS: A Data-Efficient Framework for Active Testing of Machine Learning Algorithms",
    "published_date": "2021-04-11",
    "abstract": "Machine learning models are being used extensively in many important areas, but there is no guarantee a model will always perform well or as its developers intended. Understanding the correctness of a model is crucial to prevent potential failures that may have significant detrimental impact in critical application areas. In this paper, we propose a novel framework to efficiently test a machine learning model using only a small amount of labeled test data. The idea is to estimate the metrics of interest for a model-under-test using Bayesian neural network (BNN). We develop a novel data augmentation method helping to train the BNN to achieve high accuracy. We also devise a theoretic information based sampling strategy to sample data points so as to achieve accurate estimations for the metrics of interest. Finally, we conduct an extensive set of experiments to test various machine learning models for different types of metrics. Our experiments show that the metrics estimations by our method are significantly better than existing baselines.",
    "citation_count": 3,
    "summary": "ALT-MAS is a data-efficient framework for testing machine learning models by using a Bayesian neural network to estimate performance metrics, employing a novel data augmentation technique and an information-based sampling strategy to achieve high accuracy with limited labeled data. Experiments demonstrate superior performance compared to existing baselines."
  },
  {
    "url": "https://arxiv.org/abs/2108.01734",
    "title": "Tutorials on Testing Neural Networks",
    "published_date": "2021-08-03",
    "abstract": "Deep learning achieves remarkable performance on pattern recognition, but can be vulnerable to defects of some important properties such as robustness and security. This tutorial is based on a stream of research conducted since the summer of 2018 at a few UK universities, including the University of Liverpool, University of Oxford, Queen's University Belfast, University of Lancaster, University of Loughborough, and University of Exeter. The research aims to adapt software engineering methods, in particular software testing methods, to work with machine learning models. Software testing techniques have been successful in identifying software bugs, and helping software developers in validating the software they design and implement. It is for this reason that a few software testing techniques -- such as the MC/DC coverage metric -- have been mandated in industrial standards for safety critical systems, including the ISO26262 for automotive systems and the RTCA DO-178B/C for avionics systems. However, these techniques cannot be directly applied to machine learning models, because the latter are drastically different from traditional software, and their design follows a completely different development life-cycle. As the outcome of this thread of research, the team has developed a series of methods that adapt the software testing techniques to work with a few classes of machine learning models. The latter notably include convolutional neural networks, recurrent neural networks, and random forest. The tools developed from this research are now collected, and publicly released, in a GitHub repository: \\url{https://github.com/TrustAI/DeepConcolic}, with the BSD 3-Clause licence. This tutorial is to go through the major functionalities of the tools with a few running examples, to exhibit how the developed techniques work, what the results are, and how to interpret them.",
    "citation_count": 7,
    "summary": "This tutorial adapts software testing methods, particularly MC/DC coverage, to evaluate the robustness and security of neural networks, offering tools and examples via a publicly available GitHub repository. The methods are demonstrated on convolutional, recurrent neural networks, and random forests."
  }
]