[
  {
    "url": "https://arxiv.org/abs/2409.11171",
    "title": "Preventing Unconstrained CBF Safety Filters Caused by Invalid Relative Degree Assumptions",
    "published_date": "2024-09-17",
    "abstract": "Control barrier function (CBF)-based safety filters are used to certify and modify potentially unsafe control inputs to a system such as those provided by a reinforcement learning agent or a non-expert user. In this context, safety is defined as the satisfaction of state constraints. Originally designed for continuous-time systems, CBF safety filters typically assume that the system's relative degree is well-defined and is constant across the domain; however, this assumption is restrictive and rarely verified -- even linear system dynamics with a quadratic CBF candidate may not satisfy this assumption. In real-world applications, continuous-time CBF safety filters are implemented in discrete time, exacerbating issues related to violating the condition on the relative degree. These violations can lead to the safety filter being unconstrained (any control input may be certified) for a finite time interval and result in chattering issues and constraint violations. We propose an alternative formulation to address these challenges. Specifically, we present a theoretically sound method that employs multiple CBFs to generate bounded control inputs at each state within the safe set, thereby preventing incorrect certification of arbitrary control inputs. Using this approach, we derive conditions on the maximum sampling time to ensure safety in discrete-time implementations. We demonstrate the effectiveness of our proposed method through simulations and real-world quadrotor experiments, successfully preventing chattering and constraint violations. Finally, we discuss the implications of violating the relative degree condition on CBF synthesis and learning-based CBF methods.",
    "summary": "Control barrier function (CBF) safety filters, commonly used to ensure system safety by modifying potentially unsafe control inputs, often rely on restrictive assumptions about system relative degree that are frequently violated, leading to unconstrained filters and safety issues. This paper proposes a novel method using multiple CBFs to overcome these limitations, guaranteeing bounded control inputs and preventing constraint violations, as validated through simulations and real-world experiments."
  },
  {
    "url": "https://www.lesswrong.com/tag/knuths-up-arrow-notation",
    "title": "Knuth's Up-Arrow Notation - LessWrong",
    "published_date": "2024-02-01",
    "summary": "Knuth's up-arrow notation provides a concise way to represent extremely large numbers, as demonstrated by 3^^^3, a number too vast to write out in base 10 despite its simple description and computability by a Turing machine."
  },
  {
    "url": "https://arxiv.org/pdf/2302.01450.pdf",
    "title": "Performance Bounds for Policy-Based Average Reward Reinforcement Learning Algorithms",
    "published_date": "2023-02-02",
    "abstract": "Many policy-based reinforcement learning (RL) algorithms can be viewed as instantiations of approximate policy iteration (PI), i.e., where policy improvement and policy evaluation are both performed approximately. In applications where the average reward objective is the meaningful performance metric, discounted reward formulations are often used with the discount factor being close to $1,$ which is equivalent to making the expected horizon very large. However, the corresponding theoretical bounds for error performance scale with the square of the horizon. Thus, even after dividing the total reward by the length of the horizon, the corresponding performance bounds for average reward problems go to infinity. Therefore, an open problem has been to obtain meaningful performance bounds for approximate PI and RL algorithms for the average-reward setting. In this paper, we solve this open problem by obtaining the first finite-time error bounds for average-reward MDPs, and show that the asymptotic error goes to zero in the limit as policy evaluation and policy improvement errors go to zero.",
    "citation_count": 3,
    "summary": "This paper provides the first finite-time error bounds for approximate policy iteration in average-reward Markov Decision Processes, resolving a long-standing open problem by showing that asymptotic error vanishes as policy evaluation and improvement errors decrease."
  },
  {
    "url": "https://arxiv.org/abs/2202.00129v4",
    "title": "Fundamental limits for sensor-based robot control",
    "published_date": "2022-01-31",
    "abstract": "Our goal is to develop theory and algorithms for establishing fundamental limits on performance imposed by a robot's sensors for a given task. In order to achieve this, we define a quantity that captures the amount of task-relevant information provided by a sensor. Using a novel version of the generalized Fano's inequality from information theory, we demonstrate that this quantity provides an upper bound on the highest achievable expected reward for one-step decision-making tasks. We then extend this bound to multi-step problems via a dynamic programming approach. We present algorithms for numerically computing the resulting bounds, and demonstrate our approach on three examples: (i) the lava problem from the literature on partially observable Markov decision processes, (ii) an example with continuous state and observation spaces corresponding to a robot catching a freely-falling object, and (iii) obstacle avoidance using a depth sensor with non-Gaussian noise. We demonstrate the ability of our approach to establish strong limits on achievable performance for these problems by comparing our upper bounds with achievable lower bounds (computed by synthesizing or learning concrete control policies).",
    "citation_count": 1,
    "summary": "This paper establishes fundamental limits on robot control performance based on sensor information, deriving upper bounds on achievable reward using a novel information-theoretic approach and demonstrating these bounds on various examples with both discrete and continuous state spaces. The authors compare these upper bounds to achievable lower bounds obtained from synthesized or learned control policies."
  },
  {
    "url": "https://arxiv.org/abs/2210.10298",
    "title": "Evaluation Metrics for Object Detection for Autonomous Systems",
    "published_date": "2022-10-19",
    "abstract": "This paper studies the evaluation of learning-based object detection models in conjunction with model-checking of formal specifications defined on an abstract model of an autonomous system and its environment. In particular, we define two metrics -- \\emph{proposition-labeled} and \\emph{class-labeled} confusion matrices -- for evaluating object detection, and we incorporate these metrics to compute the satisfaction probability of system-level safety requirements. While confusion matrices have been effective for comparative evaluation of classification and object detection models, our framework fills two key gaps. First, we relate the performance of object detection to formal requirements defined over downstream high-level planning tasks. In particular, we provide empirical results that show that the choice of a good object detection algorithm, with respect to formal requirements on the overall system, significantly depends on the downstream planning and control design. Secondly, unlike the traditional confusion matrix, our metrics account for variations in performance with respect to the distance between the ego and the object being detected. We demonstrate this framework on a car-pedestrian example by computing the satisfaction probabilities for safety requirements formalized in Linear Temporal Logic (LTL).",
    "citation_count": 3,
    "summary": "This paper introduces novel proposition- and class-labeled confusion matrices for evaluating object detection models within autonomous systems, linking object detection performance to formal safety requirements and accounting for distance-dependent detection variations. The framework demonstrates that optimal object detection algorithms depend on downstream planning tasks, impacting overall system safety."
  },
  {
    "url": "https://arxiv.org/abs/2201.08278",
    "title": "Lifelong Learning Metrics",
    "published_date": "2022-01-20",
    "abstract": "The DARPA Lifelong Learning Machines (L2M) program seeks to yield advances in artificial intelligence (AI) systems so that they are capable of learning (and improving) continuously, leveraging data on one task to improve performance on another, and doing so in a computationally sustainable way. Performers on this program developed systems capable of performing a diverse range of functions, including autonomous driving, real-time strategy, and drone simulation. These systems featured a diverse range of characteristics (e.g., task structure, lifetime duration), and an immediate challenge faced by the program's testing and evaluation team was measuring system performance across these different settings. This document, developed in close collaboration with DARPA and the program performers, outlines a formalism for constructing and characterizing the performance of agents performing lifelong learning scenarios.",
    "citation_count": 16,
    "summary": "The DARPA L2M program developed diverse lifelong learning AI systems, necessitating a novel metric framework to evaluate performance across varying tasks, structures, and durations. This document formalizes such a framework for characterizing lifelong learning agent performance."
  },
  {
    "url": "https://arxiv.org/pdf/2210.12061.pdf",
    "title": "Validation of Composite Systems by Discrepancy Propagation",
    "published_date": "2022-10-21",
    "abstract": "Assessing the validity of a real-world system with respect to given quality criteria is a common yet costly task in industrial applications due to the vast number of required real-world tests. Validating such systems by means of simulation offers a promising and less expensive alternative, but requires an assessment of the simulation accuracy and therefore end-to-end measurements. Additionally, covariate shifts between simulations and actual usage can cause difficulties for estimating the reliability of such systems. In this work, we present a validation method that propagates bounds on distributional discrepancy measures through a composite system, thereby allowing us to derive an upper bound on the failure probability of the real system from potentially inaccurate simulations. Each propagation step entails an optimization problem, where -- for measures such as maximum mean discrepancy (MMD) -- we develop tight convex relaxations based on semidefinite programs. We demonstrate that our propagation method yields valid and useful bounds for composite systems exhibiting a variety of realistic effects. In particular, we show that the proposed method can successfully account for data shifts within the experimental design as well as model inaccuracies within the simulation.",
    "citation_count": 2,
    "summary": "This paper introduces a method for validating composite systems using simulations by propagating bounds on distributional discrepancies, providing an upper bound on real-system failure probability even with simulation inaccuracies and covariate shifts. The method utilizes convex relaxations of discrepancy measures, such as MMD, solved via semidefinite programming."
  },
  {
    "url": "https://arxiv.org/abs/2208.14660",
    "title": "Unifying Evaluation of Machine Learning Safety Monitors",
    "published_date": "2022-08-31",
    "abstract": "With the increasing use of Machine Learning (ML) in critical autonomous systems, runtime monitors have been developed to detect prediction errors and keep the system in a safe state during operations. Monitors have been proposed for different applications involving diverse perception tasks and ML models, and specific evaluation procedures and metrics are used for different contexts. This paper introduces three unified safety-oriented metrics, representing the safety benefits of the monitor (Safety Gain), the remaining safety gaps after using it (Residual Hazard), and its negative impact on the system's performance (Availability Cost). To compute these metrics, one requires to define two return functions, representing how a given ML prediction will impact expected future rewards and hazards. Three use-cases (classification, drone landing, and autonomous driving) are used to demonstrate how metrics from the literature can be expressed in terms of the proposed metrics. Experimental results on these examples show how different evaluation choices impact the perceived performance of a monitor. As our formalism requires us to formulate explicit safety assumptions, it allows us to ensure that the evaluation conducted matches the high-level system requirements.",
    "citation_count": 9,
    "summary": "This paper proposes three unified metrics – Safety Gain, Residual Hazard, and Availability Cost – to evaluate machine learning safety monitors across diverse applications, enabling standardized comparison and clearer understanding of a monitor's impact on system safety and performance. The framework uses return functions to quantify future rewards and hazards, facilitating consistent evaluation aligned with system requirements."
  }
]