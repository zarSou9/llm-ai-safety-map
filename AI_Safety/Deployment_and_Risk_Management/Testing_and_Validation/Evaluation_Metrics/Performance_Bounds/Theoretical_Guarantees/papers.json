[
  {
    "url": "https://arxiv.org/abs/2405.08405",
    "title": "A constraint-based approach to function interpolation, with application to performance estimation for weakly convex optimisation",
    "published_date": "2024-05-14",
    "abstract": "We propose a novel approach to obtain interpolation constraints for a wide range of function classes, i.e. necessary and sufficient constraints that a set of points, functions values and (sub)gradients must satisfy to ensure the existence of a global function of the class considered, consistent with this set. The derivation of such constraints is crucial for instance in the performance analysis of optimization methods, since obtaining a priori tight performance guarantees requires using a tight description of function classes of interest. Our method allows setting aside all analytic properties of the function class to work only at an algebraic level, and to easily obtain counterexamples when a condition characterizing a function class cannot serve as an interpolation constraint. As an illustration, we provide interpolation constraints for a class of non convex non smooth functions: weakly convex functions with bounded subgradients, and rely on these new interpolation constraints to outperform state of the art bounds on the performance of the subgradient method on this class.",
    "citation_count": 1
  },
  {
    "url": "https://arxiv.org/abs/2404.13831",
    "title": "Data-Driven Performance Guarantees for Classical and Learned Optimizers",
    "published_date": "2024-04-22",
    "abstract": "We introduce a data-driven approach to analyze the performance of continuous optimization algorithms using generalization guarantees from statistical learning theory. We study classical and learned optimizers to solve families of parametric optimization problems. We build generalization guarantees for classical optimizers, using a sample convergence bound, and for learned optimizers, using the Probably Approximately Correct (PAC)-Bayes framework. To train learned optimizers, we use a gradient-based algorithm to directly minimize the PAC-Bayes upper bound. Numerical experiments in signal processing, control, and meta-learning showcase the ability of our framework to provide strong generalization guarantees for both classical and learned optimizers given a fixed budget of iterations. For classical optimizers, our bounds are much tighter than those that worst-case guarantees provide. For learned optimizers, our bounds outperform the empirical outcomes observed in their non-learned counterparts.",
    "citation_count": 3
  },
  {
    "url": "https://arxiv.org/abs/2410.14811",
    "title": "Performance Estimation for Smooth and Strongly Convex Sets",
    "published_date": "2024-10-18",
    "abstract": "We extend recent computer-assisted design and analysis techniques for first-order optimization over structured functions--known as performance estimation--to apply to structured sets. We prove\"interpolation theorems\"for smooth and strongly convex sets with Slater points and bounded diameter, showing a wide range of extremal questions amount to structured mathematical programs. Prior function interpolation theorems are recovered as a limit of our set interpolation theory. Our theory provides finite-dimensional formulations of performance estimation problems for algorithms utilizing separating hyperplane oracles, linear optimization oracles, and/or projection oracles of smooth/strongly convex sets. As direct applications of this computer-assisted machinery, we identify the minimax optimal separating hyperplane method and several areas for improvement in the theory of Frank-Wolfe, Alternating Projections, and non-Lipschitz Smooth Optimization. While particular applications and methods are not our primary focus, several simple theorems and numerically supported conjectures are provided."
  },
  {
    "url": "https://arxiv.org/abs/2410.20375",
    "title": "On performance bounds for topology optimization",
    "published_date": "2024-10-27",
    "abstract": "Topology optimization has matured to become a powerful engineering design tool that is capable of designing extraordinary structures and materials taking into account various physical phenomena. Despite the method's great advancements in recent years, several unanswered questions remain. This paper takes a step towards answering one of the larger questions, namely: How far from the global optimum is a given topology optimized design? Typically this is a hard question to answer, as almost all interesting topology optimization problems are non-convex. Unfortunately, this non-convexity implies that local minima may plague the design space, resulting in optimizers ending up in suboptimal designs. In this work, we investigate performance bounds for topology optimization via a computational framework that utilizes Lagrange duality theory. This approach provides a viable measure of how \\say{close} a given design is to the global optimum for a subset of optimization formulations. The method's capabilities are exemplified via several numerical examples, including the design of mode converters and resonating plates."
  },
  {
    "title": "How much data is sufficient to learn high-performing algorithms? generalization guarantees for data-driven algorithm design",
    "abstract": "Algorithms often have tunable parameters that impact performance metrics such as runtime and solution quality. For many algorithms used in practice, no parameter settings admit meaningful worst-case bounds, so the parameters are made available for the user to tune. Alternatively, parameters may be tuned implicitly within the proof of a worst-case guarantee. Worst-case instances, however, may be rare or nonexistent in practice. A growing body of research has demonstrated that data-driven algorithm design can lead to significant improvements in performance. This approach uses a training set of problem instances sampled from an unknown, application-specific distribution and returns a parameter setting with strong average performance on the training set. We provide a broadly applicable theory for deriving generalization guarantees that bound the difference between the algorithm's average performance over the training set and its expected performance on the unknown distribution. Our results apply no matter how the parameters are tuned, be it via an automated or manual approach. The challenge is that for many types of algorithms, performance is a volatile function of the parameters: slightly perturbing the parameters can cause a large change in behavior. Prior research (e.g., Gupta and Roughgarden, SICOMP'17; Balcan et al., COLT'17, ICML'18, EC'18) has proved generalization bounds by employing case-by-case analyses of greedy algorithms, clustering algorithms, integer programming algorithms, and selling mechanisms. We uncover a unifying structure which we use to prove extremely general guarantees, yet we recover the bounds from prior research. Our guarantees, which are tight up to logarithmic factors in the worst case, apply whenever an algorithm's performance is a piecewise-constant, -linear, or—more generally—piecewise-structured function of its parameters. Our theory also implies novel bounds for voting mechanisms and dynamic programming algorithms from computational biology.",
    "published_date": "2021-06-15",
    "citation_count": 32,
    "url": "https://dl.acm.org/doi/10.1145/3406325.3451036"
  },
  {
    "url": "https://arxiv.org/abs/2011.08002v1",
    "title": "Heuristic methods and performance bounds for photonic design.",
    "published_date": "2020-11-12",
    "abstract": "In the photonic design problem, a scientist or engineer chooses the physical parameters of a device to best match some desired device behavior. Many instances of the photonic design problem can be naturally stated as a mathematical optimization problem that is computationally difficult to solve globally. Because of this, several heuristic methods have been developed to approximately solve such problems. These methods often produce very good designs, and, in many practical applications, easily outperform 'traditional' designs that rely on human intuition. Yet, because these heuristic methods do not guarantee that the approximate solution found is globally optimal, the question remains of just how much better a designer might hope to do. This question is addressed by performance bounds or impossibility results, which determine a performance level that no design can achieve. We focus on algorithmic performance bounds, which involve substantial computation to determine. We illustrate a variety of both heuristic methods and performance bounds on two examples. In these examples (and many others not reported here) the performance bounds show that the heuristic designs are nearly optimal, and can be considered globally optimal in practice. This review serves to clearly set up the photonic design problem and unify existing approaches for calculating performance bounds, while also providing some natural generalizations and properties.",
    "citation_count": 24
  }
]