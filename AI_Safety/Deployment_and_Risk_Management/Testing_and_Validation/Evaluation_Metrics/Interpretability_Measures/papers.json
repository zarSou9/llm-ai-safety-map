[
  {
    "url": "https://arxiv.org/pdf/2205.12254.pdf",
    "title": "Interpretation Quality Score for Measuring the Quality of interpretability methods",
    "published_date": "2022-05-24",
    "abstract": "Machine learning (ML) models have been applied to a wide range of natural language processing (NLP) tasks in recent years. In addition to making accurate decisions, the necessity of understanding how models make their decisions has become apparent in many applications. To that end, many interpretability methods that help explain the decision processes of ML models have been developed. Yet, there currently exists no widely-accepted metric to evaluate the quality of explanations generated by these methods. As a result, there currently is no standard way of measuring to what degree an interpretability method achieves an intended objective. Moreover, there is no accepted standard of performance by which we can compare and rank the current existing interpretability methods. In this paper, we propose a novel metric for quantifying the quality of explanations generated by interpretability methods. We compute the metric on three NLP tasks using six interpretability methods and present our results.",
    "citation_count": 4,
    "summary": "This paper introduces a new metric, the Interpretation Quality Score, to evaluate the quality of explanations produced by machine learning interpretability methods, addressing the current lack of standardized evaluation in this field. The metric is demonstrated on three NLP tasks using six different interpretability methods."
  },
  {
    "url": "https://arxiv.org/pdf/1811.11839v2.pdf",
    "title": "A Survey of Evaluation Methods and Measures for Interpretable Machine Learning",
    "published_date": "2018-11-28",
    "abstract": "The need for interpretable and accountable intelligent system gets sensible as artificial intelligence plays more role in human life. Explainable artificial intelligence systems can be a solution by self-explaining the reasoning behind the decisions and predictions of the intelligent system. Researchers from different disciplines work together to define, design and evaluate interpretable intelligent systems for the user. Our work supports the different evaluation goals in interpretable machine learning research by a thorough review of evaluation methodologies used in machine-explanation research across the fields of human-computer interaction, visual analytics, and machine learning. We present a 2D categorization of interpretable machine learning evaluation methods and show a mapping between user groups and evaluation measures. Further, we address the essential factors and steps for a right evaluation plan by proposing a nested model for design and evaluation of explainable artificial intelligence systems.",
    "citation_count": 102,
    "summary": "This survey paper reviews evaluation methods for interpretable machine learning, categorizing existing approaches and proposing a nested model for designing and evaluating explainable AI systems to address the growing need for accountable AI. It maps evaluation measures to different user groups across various research fields."
  },
  {
    "url": "https://arxiv.org/pdf/2306.10175.pdf",
    "title": "SSE: A Metric for Evaluating Search System Explainability",
    "published_date": "2023-06-16",
    "abstract": "Explainable Information Retrieval (XIR) is a growing research area focused on enhancing transparency and trustworthiness of the complex decision-making processes taking place in modern information retrieval systems. While there has been progress in developing XIR systems, empirical evaluation tools to assess the degree of explainability attained by such systems are lacking. To close this gap and gain insights into the true merit of XIR systems, we extend existing insights from a factor analysis of search explainability to introduce SSE (Search System Explainability), an evaluation metric for XIR search systems. Through a crowdsourced user study, we demonstrate SSE's ability to distinguish between explainable and non-explainable systems, showing that systems with higher scores indeed indicate greater interpretability. Additionally, we observe comparable perceived temporal demand and performance levels between non-native and native English speakers. We hope that aside from these concrete contributions to XIR, this line of work will serve as a blueprint for similar explainability evaluation efforts in other domains of machine learning and natural language processing.",
    "summary": "The paper introduces SSE, a new metric for evaluating the explainability of search systems, validated through a crowdsourced user study demonstrating its ability to differentiate between explainable and unexplainable systems based on perceived interpretability. This metric addresses the lack of robust evaluation tools in Explainable Information Retrieval (XIR)."
  },
  {
    "url": "https://www.alignmentforum.org/tag/interpretability-ml-and-ai",
    "author": "Lee Sharkey, Dan Braun, Beren Millidge",
    "title": "Interpretability (ML & AI) - AI Alignment Forum",
    "published_date": "2023-04-30",
    "summary": "Machine learning model interpretability focuses on understanding how these models arrive at their outputs, a crucial aspect often lacking in current opaque systems. This includes mechanistic approaches that examine internal processes and attribution methods that link outputs to specific input features."
  },
  {
    "url": "https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/f8nd9F7dL9SxueLFA",
    "author": "scasper",
    "title": "EIS IV: A Spotlight on Feature Attribution/Saliency",
    "published_date": "2023-02-15",
    "summary": "This article critiques the field of feature attribution/saliency methods in explainable AI, arguing that many methods lack rigorous evaluation and often fail basic sanity checks. Despite a large body of research, these methods frequently prove ineffective at identifying truly influential features or improving model predictability."
  },
  {
    "url": "https://www.lesswrong.com/posts/sTe78dNJDGywu9Dz6/solving-the-mechanistic-interpretability-challenges-eis-vii",
    "author": "StefanHex, Marius Hobbhahn",
    "title": "Solving the Mechanistic Interpretability challenges: EIS VII Challenge 1",
    "published_date": "2023-05-09",
    "summary": "Researchers solved a machine learning interpretability challenge by reverse-engineering a convolutional neural network (CNN) trained on MNIST digits. They determined the CNN classified images based on their similarity to a \"1\" template and its inverse, using a simple dot-product similarity measure."
  },
  {
    "url": "https://www.lesswrong.com/posts/ExRN5Bu3696cf9Ccm/the-engineer-s-interpretability-sequence-eis-i-intro",
    "author": "scasper",
    "title": "The Engineer's Interpretability Sequence (EIS) I: Intro",
    "published_date": "2023-02-09",
    "summary": "While interest in AI interpretability tools has surged, driven by concerns about AI safety, a significant gap exists between research and practical engineering applications; this article series aims to bridge that gap by focusing on an engineer's perspective on interpretability."
  },
  {
    "url": "https://arxiv.org/abs/2203.02928v2",
    "title": "Fidelity of Interpretability Methods and Perturbation Artifacts in Neural Networks",
    "published_date": "2022-03-06",
    "abstract": "Despite excellent performance of deep neural networks (DNNs) in image classification, detection, and prediction, characterizing how DNNs make a given decision remains an open problem, resulting in a number of interpretability methods. Post-hoc interpretability methods primarily aim to quantify the importance of input features with respect to the class probabilities. However, due to the lack of ground truth and the existence of interpretability methods with diverse operating characteristics, evaluating these methods is a crucial challenge. A popular approach to evaluate interpretability methods is to perturb input features deemed important for a given prediction and observe the decrease in accuracy. However, perturbation itself may introduce artifacts. We propose a method for estimating the impact of such artifacts on the fidelity estimation by utilizing model accuracy curves from perturbing input features according to the Most Import First (MIF) and Least Import First (LIF) orders. Using the ResNet-50 trained on the ImageNet, we demonstrate the proposed fidelity estimation of four popular post-hoc interpretability methods.",
    "citation_count": 4,
    "summary": "The paper assesses the fidelity of post-hoc interpretability methods for deep neural networks by analyzing perturbation artifacts, proposing a novel method using Most Import First and Least Import First perturbation orderings to estimate their impact, and demonstrating this on ResNet-50 trained on ImageNet. This approach addresses the challenge of evaluating interpretability methods in the absence of ground truth."
  }
]