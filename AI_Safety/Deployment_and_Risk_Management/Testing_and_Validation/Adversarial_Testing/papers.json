[
  {
    "url": "https://arxiv.org/pdf/2002.12078v1.pdf",
    "title": "Training Adversarial Agents to Exploit Weaknesses in Deep Control Policies",
    "published_date": "2020-02-27",
    "abstract": "Deep learning has become an increasingly common technique for various control problems, such as robotic arm manipulation, robot navigation, and autonomous vehicles. However, the downside of using deep neural networks to learn control policies is their opaque nature and the difficulties of validating their safety. As the networks used to obtain state-of-the-art results become increasingly deep and complex, the rules they have learned and how they operate become more challenging to understand. This presents an issue, since in safety-critical applications the safety of the control policy must be ensured to a high confidence level. In this paper, we propose an automated black box testing framework based on adversarial reinforcement learning. The technique uses an adversarial agent, whose goal is to degrade the performance of the target model under test. We test the approach on an autonomous vehicle problem, by training an adversarial reinforcement learning agent, which aims to cause a deep neural network-driven autonomous vehicle to collide. Two neural networks trained for autonomous driving are compared, and the results from the testing are used to compare the robustness of their learned control policies. We show that the proposed framework is able to find weaknesses in both control policies that were not evident during online testing and therefore, demonstrate a significant benefit over manual testing methods.",
    "citation_count": 42,
    "summary": "This paper introduces an adversarial reinforcement learning framework for automated black-box testing of deep control policies, using an adversarial agent to identify vulnerabilities and compare the robustness of different autonomous driving systems. The results demonstrate the framework's effectiveness in uncovering weaknesses not detected through standard testing methods."
  },
  {
    "url": "https://arxiv.org/auth/show-endorsers/1910.13645",
    "title": "Automatic Testing With Reusable Adversarial Agents",
    "published_date": "2019-10-30",
    "abstract": "Autonomous systems such as self-driving cars and general-purpose robots are safety-critical systems that operate in highly uncertain and dynamic environments. We propose an interactive multi-agent framework where the system-underdesign is modeled as an ego agent and its environment is modeled by a number of adversarial (ado) agents. For example, a selfdriving car is an ego agent whose behavior is influenced by ado agents such as pedestrians, bicyclists, traffic lights, road geometry etc. Given a logical specification of the correct behavior of the ego agent, and a set of constraints that encode reasonable adversarial behavior, our framework reduces the adversarial testing problem to the problem of synthesizing controllers for (constrained) ado agents that cause the ego agent to violate its specifications. Specifically, we explore the use of tabular and deep reinforcement learning approaches for synthesizing adversarial agents. We show that ado agents trained in this fashion are better than traditional falsification or testing techniques because they can generalize to ego agents and environments that differ from the original ego agent. We demonstrate the efficacy of our technique on two realworld case studies from the domain of self-driving cars.",
    "citation_count": 4,
    "summary": "This paper introduces a multi-agent framework for automatically testing autonomous systems by training adversarial agents via reinforcement learning to violate system specifications. These reusable adversarial agents generalize better than traditional testing methods, enabling more robust evaluation across diverse scenarios."
  },
  {
    "url": "https://arxiv.org/abs/2411.19567",
    "title": "AdvFuzz: Finding More Violations Caused by the EGO Vehicle in Simulation Testing by Adversarial NPC Vehicles",
    "published_date": "2024-11-29",
    "abstract": "Recently, there has been a significant escalation in both academic and industrial commitment towards the development of autonomous driving systems (ADSs). A number of simulation testing approaches have been proposed to generate diverse driving scenarios for ADS testing. However, scenarios generated by these previous approaches are static and lack interactions between the EGO vehicle and the NPC vehicles, resulting in a large amount of time on average to find violation scenarios. Besides, a large number of the violations they found are caused by aggressive behaviors of NPC vehicles, revealing none bugs of ADS. In this work, we propose the concept of adversarial NPC vehicles and introduce AdvFuzz, a novel simulation testing approach, to generate adversarial scenarios on main lanes (e.g., urban roads and highways). AdvFuzz allows NPC vehicles to dynamically interact with the EGO vehicle and regulates the behaviors of NPC vehicles, finding more violation scenarios caused by the EGO vehicle more quickly. We compare AdvFuzz with a random approach and three state-of-the-art scenario-based testing approaches. Our experiments demonstrate that AdvFuzz can generate 198.34% more violation scenarios compared to the other four approaches in 12 hours and increase the proportion of violations caused by the EGO vehicle to 87.04%, which is more than 7 times that of other approaches. Additionally, AdvFuzz is at least 92.21% faster in finding one violation caused by the EGO vehicle than that of the other approaches.",
    "summary": "AdvFuzz is a novel simulation testing approach for autonomous driving systems that uses adversarial non-player character (NPC) vehicles to efficiently generate scenarios revealing EGO vehicle bugs, significantly outperforming existing methods in both speed and the proportion of EGO-caused violations. It achieves this by dynamically interacting with the EGO vehicle to quickly find more relevant violation scenarios."
  },
  {
    "url": "https://www.alignmentforum.org/posts/m6poxWegJkp8LPpjw/can-generalized-adversarial-testing-enable-more-rigorous-llm",
    "author": "Stephen Casper",
    "title": "Can Generalized Adversarial Testing Enable More Rigorous LLM Safety Evals?",
    "published_date": "2024-07-30",
    "summary": "The article argues that current LLM safety evaluations are insufficient because they primarily focus on \"input-space\" attacks, neglecting the potential for attackers to modify the model itself (e.g., via fine-tuning). It proposes \"generalized\" adversarial testing, manipulating model weights or activations, as a more robust approach to uncover latent harmful capabilities and better assess real-world risks."
  },
  {
    "url": "https://www.alignmentforum.org/posts/2iALhBnMpcjbWJJ3w/beyond-the-board-exploring-ai-robustness-through-go",
    "author": "AdamGleave",
    "title": "Beyond the Board: Exploring AI Robustness Through Go",
    "published_date": "2024-06-19",
    "summary": "Researchers demonstrated that even superhuman Go AIs are vulnerable to surprisingly simple, cyclic patterns, highlighting AI robustness challenges. While adversarial training offered some defense, new, more computationally intensive attacks were subsequently discovered, emphasizing the ongoing need for improved AI safety measures."
  },
  {
    "url": "https://arxiv.org/abs/2205.08589v1",
    "title": "Hierarchical Distribution-aware Testing of Deep Learning",
    "published_date": "2022-05-17",
    "abstract": "With its growing use in safety/security-critical applications, Deep Learning (DL) has raised increasing concerns regarding its dependability. In particular, DL has a notorious problem of lacking robustness. Input added with adversarial perturbations, i.e., Adversarial Examples (AEs), are easily mispredicted by the DL model. Despite recent efforts made in detecting AEs via state-of-the-art attack and testing methods, they are normally input distribution–agnostic and/or disregard the perceptual quality of adversarial perturbations. Consequently, the detected AEs are irrelevant inputs in the application context or noticeably unrealistic to humans. This may lead to a limited effect on improving the DL model's dependability, as the testing budget is likely to be wasted on detecting AEs that are encountered very rarely in its real-life operations. In this article, we propose a new robustness testing approach for detecting AEs that considers both the feature-level distribution and the pixel-level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. First, we select test seeds based on the density of feature-level distribution and the vulnerability of adversarial robustness. The vulnerability of test seeds is indicated by the auxiliary information, which are highly correlated with local robustness. Given a test seed, we then develop a novel genetic algorithm–based local test case generation method, in which two fitness functions work alternatively to control the perceptual quality of detected AEs. Finally, extensive experiments confirm that our holistic approach considering hierarchical distributions is superior to the state-of-the-arts that either disregard any input distribution or only consider a single (non-hierarchical) distribution, in terms of not only detecting imperceptible AEs but also improving the overall robustness of the DL model under testing.",
    "citation_count": 7,
    "summary": "This paper introduces a novel hierarchical distribution-aware testing approach for deep learning models that improves robustness testing by considering both feature-level and pixel-level input distributions to generate perceptually realistic adversarial examples, thereby more effectively identifying vulnerabilities and enhancing model dependability. This approach surpasses existing methods which ignore or only partially consider input distribution."
  },
  {
    "url": "https://arxiv.org/pdf/2106.00576v1.pdf",
    "title": "Exposing previously undetectable faults in deep neural networks",
    "published_date": "2021-06-01",
    "abstract": "Existing methods for testing DNNs solve the oracle problem by constraining the raw features (e.g. image pixel values) to be within a small distance of a dataset example for which the desired DNN output is known. But this limits the kinds of faults these approaches are able to detect. In this paper, we introduce a novel DNN testing method that is able to find faults in DNNs that other methods cannot. The crux is that, by leveraging generative machine learning, we can generate fresh test inputs that vary in their high-level features (for images, these include object shape, location, texture, and colour). We demonstrate that our approach is capable of detecting deliberately injected faults as well as new faults in state-of-the-art DNNs, and that in both cases, existing methods are unable to find these faults.",
    "citation_count": 24,
    "summary": "This paper presents a novel deep neural network (DNN) testing method using generative machine learning to create diverse test inputs varying in high-level features, enabling the detection of faults undetectable by existing methods constrained to small perturbations of known examples. The approach reveals both injected and previously unknown faults in state-of-the-art DNNs."
  },
  {
    "url": "https://arxiv.org/abs/2007.15147v2",
    "title": "A General Framework For Detecting Anomalous Inputs to DNN Classifiers",
    "published_date": "2020-07-29",
    "abstract": "Detecting anomalous inputs, such as adversarial and out-of-distribution (OOD) inputs, is critical for classifiers (including deep neural networks or DNNs) deployed in real-world applications. While prior works have proposed various methods to detect such anomalous samples using information from the internal layer representations of a DNN, there is a lack of consensus on a principled approach for the different components of such a detection method. As a result, often heuristic and one-off methods are applied for different aspects of this problem. We propose an unsupervised anomaly detection framework based on the internal DNN layer representations in the form of a meta-algorithm with configurable components. We proceed to propose specific instantiations for each component of the meta-algorithm based on ideas grounded in statistical testing and anomaly detection. We evaluate the proposed methods on well-known image classification datasets with strong adversarial attacks and OOD inputs, including an adaptive attack that uses the internal layer representations of the DNN (often not considered in prior work). Comparisons with five recently-proposed competing detection methods demonstrates the effectiveness of our method in detecting adversarial and OOD inputs.",
    "citation_count": 29,
    "summary": "This paper introduces a general, configurable framework for detecting anomalous inputs (adversarial and out-of-distribution) in deep neural networks, using a meta-algorithm with statistically-grounded components that outperforms five existing methods in evaluation."
  },
  {
    "title": "Search-based adversarial testing and improvement of constrained credit scoring systems",
    "abstract": "Credit scoring systems are critical FinTech applications that concern the analysis of the creditworthiness of a person or organization. While decisions were previously based on human expertise, they are now increasingly relying on data analysis and machine learning. In this paper, we assess the ability of state-of-the-art adversarial machine learning to craft attacks on a real-world credit scoring system. Interestingly, we find that, while these techniques can generate large numbers of adversarial data, these are practically useless as they all violate domain-specific constraints. In other words, the generated examples are all false positives as they cannot occur in practice. To circumvent this limitation, we propose CoEvA2, a search-based method that generates valid adversarial examples (satisfying the domain constraints). CoEvA2 utilizes multi-objective search in order to simultaneously handle constraints, perform the attack and maximize the overdraft amount requested. We evaluate CoEvA2 on a major bank's real-world system by checking its ability to craft valid attacks. CoEvA2 generates thousands of valid adversarial examples, revealing a high risk for the banking system. Fortunately, by improving the system through adversarial training (based on the produced examples), we increase its robustness and make our attack fail.",
    "published_date": "2020-11-07",
    "citation_count": 24,
    "url": "https://dl.acm.org/doi/10.1145/3368089.3409739",
    "summary": "This paper investigates adversarial attacks on a real-world credit scoring system, finding that existing methods generate impractical attacks violating domain constraints. The authors propose CoEvA2, a novel search-based method generating valid adversarial examples, revealing system vulnerabilities that are subsequently mitigated through adversarial training."
  },
  {
    "url": "https://www.alignmentforum.org/posts/cPCvfrqjgy5Cu2FCs/analysing-adversarial-attacks-with-linear-probing-3",
    "author": "Yoann Poupart; Imene Kerboua; Clement Neo; Jason Hoelscher-Obermaier",
    "title": "Analysing Adversarial Attacks with Linear Probing",
    "published_date": "2024-06-17",
    "summary": "Researchers used linear probes to analyze how adversarial attacks affect different layers of a CLIP-based image classifier. They found that adversarial attacks primarily modify later layers, suggesting a potential detection method based on the discrepancy between early and late layer activations."
  }
]