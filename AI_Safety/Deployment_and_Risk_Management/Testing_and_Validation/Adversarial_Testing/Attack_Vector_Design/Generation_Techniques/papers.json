[
  {
    "url": "https://arxiv.org/abs/2406.02064",
    "title": "Advancing Generalized Transfer Attack with Initialization Derived Bilevel Optimization and Dynamic Sequence Truncation",
    "published_date": "2024-06-04",
    "abstract": "Transfer attacks generate significant interest for real-world black-box applications by crafting transferable adversarial examples through surrogate models. Whereas, existing works essentially directly optimize the single-level objective w.r.t. the surrogate model, which always leads to poor interpretability of attack mechanism and limited generalization performance over unknown victim models. In this work, we propose the BilEvel Transfer AttacK (BETAK) framework by establishing an initialization derived bilevel optimization paradigm, which explicitly reformulates the nested constraint relationship between the Upper-Level (UL) pseudo-victim attacker and the Lower-Level (LL) surrogate attacker. Algorithmically, we introduce the Hyper Gradient Response (HGR) estimation as an effective feedback for the transferability over pseudo-victim attackers, and propose the Dynamic Sequence Truncation (DST) technique to dynamically adjust the back-propagation path for HGR and reduce computational overhead simultaneously. Meanwhile, we conduct detailed algorithmic analysis and provide convergence guarantee to support non-convexity of the LL surrogate attacker. Extensive evaluations demonstrate substantial improvement of BETAK (e.g., 53.41% increase of attack success rates against IncRes-v2_ens victim) against different victims and defense methods in targeted and untargeted attack scenarios."
  },
  {
    "url": "https://arxiv.org/pdf/2303.03535.pdf",
    "title": "Exploration of For-Purpose Decentralized Algorithmic Cyber Attacks in EV Charging Control",
    "published_date": "2023-03-06",
    "abstract": "Distributed and decentralized multi-agent optimization (DMAO) algorithms enable the control of large-scale grid-edge resources, such as electric vehicles (EVs), to provide power grid services. Despite its great scalability, DMAO is fundamentally prone to cyber attacks as it is highly dependent on frequent peer-to-peer communications. Existing cyber-security research in this regard mainly focuses on broad-spectrum attacks aiming at jeopardizing the entire control system while losing the possibility of achieving specific attacking purposes. This paper, for the first time, explores novel for-purpose algorithmic attacks that are launched by participating agents and interface with DMAO to achieve self-interest attack purposes. A decentralized EV charging control problem is formulated as an illustrative use case. Theoretical for-purpose attack vectors with and without the stealthy feature are devised. Simulations on EV charging control show the practicability of the proposed algorithmic for-purpose attacks and the impacts of such attacks on distribution networks.",
    "citation_count": 1
  },
  {
    "url": "https://arxiv.org/pdf/2112.01156v2.pdf",
    "title": "A Unified Framework for Adversarial Attack and Defense in Constrained Feature Space",
    "published_date": "2021-12-02",
    "abstract": "The generation of feasible adversarial examples is necessary for properly assessing models that work in constrained feature space. However, it remains a challenging task to enforce constraints into attacks that were designed for computer vision. We propose a unified framework to generate feasible adversarial examples that satisfy given domain constraints. Our framework can handle both linear and non-linear constraints. We instantiate our framework into two algorithms: a gradient-based attack that introduces constraints in the loss function to maximize, and a multi-objective search algorithm that aims for misclassification, perturbation minimization, and constraint satisfaction. We show that our approach is effective in four different domains, with a success rate of up to 100%, where state-of-the-art attacks fail to generate a single feasible example. In addition to adversarial retraining, we propose to introduce engineered non-convex constraints to improve model adversarial robustness. We demonstrate that this new defense is as effective as adversarial retraining. Our framework forms the starting point for research on constrained adversarial attacks and provides relevant baselines and datasets that future research can exploit.",
    "citation_count": 19
  },
  {
    "url": "https://arxiv.org/pdf/2105.07553v1.pdf",
    "title": "Prototype-supervised Adversarial Network for Targeted Attack of Deep Hashing",
    "published_date": "2021-05-17",
    "abstract": "Due to its powerful capability of representation learning and high-efficiency computation, deep hashing has made significant progress in large-scale image retrieval. However, deep hashing networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is the first generation-based method to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a generator and a discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing.",
    "citation_count": 39
  },
  {
    "url": "https://arxiv.org/pdf/2109.05919v1.pdf",
    "title": "Evolving Architectures with Gradient Misalignment toward Low Adversarial Transferability",
    "published_date": "2021-09-13",
    "abstract": "Deep neural network image classifiers are known to be susceptible, not only to adversarial examples created for them, but also to those created for others. This phenomenon poses a potential security risk in various black-box systems that rely on image classifiers. One of the observations on networks that have transferability of adversarial examples between them is the similarity of their architectures. Networks with high architectural similarity tend to share high transferability as well. Thus, in this study, we address this problem from a novel perspective by investigating the contribution of network architecture to transferability. Specifically, we propose an architecture searching framework that employs neuroevolution to evolve network architectures and gradient misalignment loss to encourage networks to converge into dissimilar functions after training. Our findings indicate that the proposed framework successfully discovers architectures that reduce transferability from four standard networks, including ResNet and VGG, while maintaining good accuracy on unperturbed images. In addition, the evolved networks trained with gradient misalignment exhibit significantly lower transferability than a standard network trained with gradient misalignment, which indicates that network architecture plays an important role in reducing transferability. We demonstrate that designing or exploring proper network architectures is a promising approach to tackle the transferability issue and train adversarially robust image classifiers.",
    "citation_count": 1
  },
  {
    "url": "https://arxiv.org/pdf/2007.01299v1.pdf",
    "title": "Generating Adversarial Examples withControllable Non-transferability",
    "published_date": "2020-07-02",
    "abstract": "Adversarial attacks against Deep Neural Networks have been widely studied. One significant feature that makes such attacks particularly powerful is transferability, where the adversarial examples generated from one model can be effective against other similar models as well. A large number of works have been done to increase the transferability. However, how to decrease the transferability and craft malicious samples only for specific target models are not explored yet. \nIn this paper, we design novel attack methodologies to generate adversarial examples with controllable non-transferability. With these methods, an adversary can efficiently produce precise adversarial examples to attack a set of target models he desires, while keeping benign to other models. The first method is Reversed Loss Function Ensemble, where the adversary can craft qualified examples from the gradients of a reversed loss function. This approach is effective for the white-box and gray-box settings. The second method is Transferability Classification: the adversary trains a transferability-aware classifier from the perturbations of adversarial examples. This classifier further provides the guidance for the generation of non-transferable adversarial examples. This approach can be applied to the black-box scenario. Evaluation results demonstrate the effectiveness and efficiency of our proposed methods. This work opens up a new route for generating adversarial examples with new features and applications.",
    "citation_count": 3
  }
]