[
  {
    "url": "https://arxiv.org/abs/2411.03865",
    "title": "AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making",
    "published_date": "2024-11-06",
    "abstract": "Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety."
  },
  {
    "url": "https://arxiv.org/abs/2211.09001",
    "title": "Multi-Timescale Modeling of Human Behavior",
    "published_date": "2022-11-16",
    "abstract": "In recent years, the role of artificially intelligent (AI) agents has evolved from being basic tools to socially intelligent agents working alongside humans towards common goals. In such scenarios, the ability to predict future behavior by observing past actions of their human teammates is highly desirable in an AI agent. Goal-oriented human behavior is complex, hierarchical, and unfolds across multiple timescales. Despite this observation, relatively little attention has been paid towards using multi-timescale features to model such behavior. In this paper, we propose an LSTM network architecture that processes behavioral information at multiple timescales to predict future behavior. We demonstrate that our approach for modeling behavior in multiple timescales substantially improves prediction of future behavior compared to methods that do not model behavior at multiple timescales. We evaluate our architecture on data collected in an urban search and rescue scenario simulated in a virtual Minecraft-based testbed, and compare its performance to that of a number of valid baselines as well as other methods that do not process inputs at multiple timescales."
  },
  {
    "url": "https://arxiv.org/pdf/2211.02089.pdf",
    "title": "Group Cohesion in Multi-Agent Scenarios as an Emergent Behavior",
    "published_date": "2022-11-03",
    "abstract": "In this paper, we elaborate on the design and discuss the results of a multi-agent simulation that we have developed using the PSI cognitive architecture. We demonstrate that imbuing agents with intrinsic needs for group affiliation, certainty and competence will lead to the emergence of social behavior among agents. This behavior expresses itself in altruism toward in-group agents and adversarial tendencies toward out-group agents. Our simulation also shows how parameterization can have dramatic effects on agent behavior. Introducing an out-group bias, for example, not only made agents behave aggressively toward members of the other group, but it also increased in-group cohesion. Similarly, environmental and situational factors facilitated the emergence of outliers: agents from adversarial groups becoming close friends. Overall, this simulation showcases the power of psychological frameworks, in general, and the PSI paradigm, in particular, to bring about human-like behavioral patterns in an emergent fashion."
  },
  {
    "url": "https://arxiv.org/pdf/2201.02694v1.pdf",
    "title": "To Trust or to Stockpile: Modeling Human-Simulation Interaction in Supply Chain Shortages",
    "published_date": "2022-01-07",
    "abstract": "Understanding decision-making in dynamic and complex settings is a challenge yet essential for preventing, mitigating, and responding to adverse events (e.g., disasters, financial crises). Simulation games have shown promise to advance our understanding of decision-making in such settings. However, an open question remains on how we extract useful information from these games. We contribute an approach to model human-simulation interaction by leveraging existing methods to characterize: (1) system states of dynamic simulation environments (with Principal Component Analysis), (2) behavioral responses from human interaction with simulation (with Hidden Markov Models), and (3) behavioral responses across system states (with Sequence Analysis). We demonstrate this approach with our game simulating drug shortages in a supply chain context. Results from our experimental study with 135 participants show different player types (hoarders, reactors, followers), how behavior changes in different system states, and how sharing information impacts behavior. We discuss how our findings challenge existing literature.",
    "citation_count": 2
  },
  {
    "url": "https://arxiv.org/abs/2101.02185",
    "title": "Adaptive Synthetic Characters for Military Training",
    "published_date": "2021-01-06",
    "abstract": "Behaviors of the synthetic characters in current military simulations are limited since they are generally generated by rule-based and reactive computational models with minimal intelligence. Such computational models cannot adapt to reflect the experience of the characters, resulting in brittle intelligence for even the most effective behavior models devised via costly and labor-intensive processes. Observation-based behavior model adaptation that leverages machine learning and the experience of synthetic entities in combination with appropriate prior knowledge can address the issues in the existing computational behavior models to create a better training experience in military training simulations. In this paper, we introduce a framework that aims to create autonomous synthetic characters that can perform coherent sequences of believable behavior while being aware of human trainees and their needs within a training simulation. This framework brings together three mutually complementary components. The first component is a Unity-based simulation environment Rapid Integration and Development Environment (RIDE) supporting One World Terrain (OWT) models and capable of running and supporting machine learning experiments. The second is Shiva, a novel multi-agent reinforcement and imitation learning framework that can interface with a variety of simulation environments, and that can additionally utilize a variety of learning algorithms. The final component is the Sigma Cognitive Architecture that will augment the behavior models with symbolic and probabilistic reasoning capabilities. We have successfully created proof-of-concept behavior models leveraging this framework on realistic terrain as an essential step towards bringing machine learning into military simulations: (1) in order to improve the quality and complexity of non-player characters in training simulations; (2) in order to create more realistic and challenging training experiences while reducing the cost and time to develop them; and (3) in order to make simulations less dependent on the availability of human participants.",
    "citation_count": 9
  },
  {
    "title": "Deep Integration of Physical Humanoid Control and Crowd Navigation",
    "abstract": "Many multi-agent navigation approaches make use of simplified representations such as a disk. These simplifications allow for fast simulation of thousands of agents but limit the simulation accuracy and fidelity. In this paper, we propose a fully integrated physical character control and multi-agent navigation method. In place of sample complex online planning methods, we extend the use of recent deep reinforcement learning techniques. This extension improves on multi-agent navigation models and simulated humanoids by combining Multi-Agent and Hierarchical Reinforcement Learning. We train a single short term goal-conditioned low-level policy to provide directed walking behaviour. This task-agnostic controller can be shared by higher-level policies that perform longer-term planning. The proposed approach produces reciprocal collision avoidance, robust navigation, and emergent crowd behaviours. Furthermore, it offers several key affordances not previously possible in multi-agent navigation including tunable character morphology and physically accurate interactions with agents and the environment. Our results show that the proposed method outperforms prior methods across environments and tasks, as well as, performing well in terms of zero-shot generalization over different numbers of agents and computation time.",
    "published_date": "2020-10-16",
    "citation_count": 22,
    "url": "https://dl.acm.org/doi/10.1145/3424636.3426894"
  }
]