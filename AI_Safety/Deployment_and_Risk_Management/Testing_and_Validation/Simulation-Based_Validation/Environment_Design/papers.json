[
  {
    "url": "https://arxiv.org/abs/2410.04313",
    "title": "Vehicle-in-Virtual-Environment Method for ADAS and Connected and Automated Driving Function Development/Demonstration/Evaluation",
    "published_date": "2024-10-05",
    "abstract": "The current approach for new Advanced Driver Assistance System (ADAS) and Connected and Automated Driving (CAD) function development involves a significant amount of public road testing which is inefficient due to the number miles that need to be driven for rare and extreme events to take place, thereby being very costly also, and unsafe as the rest of the road users become involuntary test subjects. A new development, evaluation and demonstration method for safe, efficient, and repeatable development, demonstration and evaluation of ADAS and CAD functions called VehicleInVirtualEnvironment (VVE) was recently introduced as a solution to this problem. The vehicle is operated in a large, empty, and flat area during VVE while its localization and perception sensor data is fed from the virtual environment with other traffic and rare and extreme events being generated as needed. The virtual environment can be easily configured and modified to construct different testing scenarios on demand. This paper focuses on the VVE approach and introduces the coordinate transformations needed to sync pose (location and orientation) in the virtual and physical worlds and handling of localization and perception sensor data using the highly realistic 3D simulation model of a recent autonomous shuttle deployment site in Columbus, Ohio as the virtual world. As a further example that uses multiple actors, the use of VVE for VehicleToVRU communication based Vulnerable Road User (VRU) safety is presented in the paper using VVE experiments and real pedestrian(s) in a safe and repeatable manner. VVE experiments are used to demonstrate the efficacy of the method.",
    "citation_count": 1,
    "summary": "The Vehicle-in-Virtual-Environment (VVE) method offers a safer, more efficient alternative to public road testing for ADAS and CAD function development, using a physical vehicle interacting with a virtual environment to simulate various driving scenarios including rare and extreme events. This approach improves testing speed and repeatability while mitigating safety risks associated with real-world testing."
  },
  {
    "url": "https://arxiv.org/pdf/2007.04954.pdf",
    "title": "ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation",
    "published_date": "2020-07-09",
    "abstract": "We introduce ThreeDWorld (TDW), a platform for interactive multi-modal physical simulation. With TDW, users can simulate high-fidelity sensory data and physical interactions between mobile agents and objects in a wide variety of rich 3D environments. TDW has several unique properties: 1) realtime near photo-realistic image rendering quality; 2) a library of objects and environments with materials for high-quality rendering, and routines enabling user customization of the asset library; 3) generative procedures for efficiently building classes of new environments 4) high-fidelity audio rendering; 5) believable and realistic physical interactions for a wide variety of material types, including cloths, liquid, and deformable objects; 6) a range of \"avatar\" types that serve as embodiments of AI agents, with the option for user avatar customization; and 7) support for human interactions with VR devices. TDW also provides a rich API enabling multiple agents to interact within a simulation and return a range of sensor and physics data representing the state of the world. We present initial experiments enabled by the platform around emerging research directions in computer vision, machine learning, and cognitive science, including multi-modal physical scene understanding, multi-agent interactions, models that \"learn like a child\", and attention studies in humans and neural networks. The simulation platform will be made publicly available.",
    "citation_count": 275,
    "summary": "ThreeDWorld (TDW) is an open-source platform for interactive, multi-modal physical simulation offering high-fidelity rendering, realistic physics, and a rich API for studying computer vision, machine learning, and cognitive science. It supports diverse agents, environments, and sensor modalities, including VR interaction."
  },
  {
    "url": "https://arxiv.org/abs/2410.03756",
    "title": "Real-World Data and Calibrated Simulation Suite for Offline Training of Reinforcement Learning Agents to Optimize Energy and Emission in Buildings for Environmental Sustainability",
    "published_date": "2024-10-02",
    "abstract": "Commercial office buildings contribute 17 percent of Carbon Emissions in the US, according to the US Energy Information Administration (EIA), and improving their efficiency will reduce their environmental burden and operating cost. A major contributor of energy consumption in these buildings are the Heating, Ventilation, and Air Conditioning (HVAC) devices. HVAC devices form a complex and interconnected thermodynamic system with the building and outside weather conditions, and current setpoint control policies are not fully optimized for minimizing energy use and carbon emission. Given a suitable training environment, a Reinforcement Learning (RL) agent is able to improve upon these policies, but training such a model, especially in a way that scales to thousands of buildings, presents many practical challenges. Most existing work on applying RL to this important task either makes use of proprietary data, or focuses on expensive and proprietary simulations that may not be grounded in the real world. We present the Smart Buildings Control Suite, the first open source interactive HVAC control dataset extracted from live sensor measurements of devices in real office buildings. The dataset consists of two components: six years of real-world historical data from three buildings, for offline RL, and a lightweight interactive simulator for each of these buildings, calibrated using the historical data, for online and model-based RL. For ease of use, our RL environments are all compatible with the OpenAI gym environment standard. We also demonstrate a novel method of calibrating the simulator, as well as baseline results on training an RL agent on the simulator, predicting real-world data, and training an RL agent directly from data. We believe this benchmark will accelerate progress and collaboration on building optimization and environmental sustainability research.",
    "summary": "This paper introduces the Smart Buildings Control Suite, an open-source resource providing real-world HVAC data from three office buildings and calibrated simulators, enabling offline and online reinforcement learning for energy and emission optimization in buildings. The suite aims to accelerate research in building control and environmental sustainability by offering a standardized, accessible benchmark."
  },
  {
    "url": "https://arxiv.org/abs/2403.12753",
    "title": "Developing Algorithms for the Internet of Flying Things Through Environments With Varying Degrees of Realism - Extended Version",
    "published_date": "2024-03-19",
    "abstract": "Developing for the internet of flying things is a complicated task. The fragility and cost of the equipment required to deploy in the field motivates the use of simulation software for prototyping and developing robust applications. This work proposes GrADyS-SIM NextGen as a solution that enables development on a single programming language and toolset over multiple environments with varying levels of realism. Finally, we illustrate the usefulness of this approach with a toy problem that makes use of the simulation framework.",
    "citation_count": 1,
    "summary": "GrADyS-SIM NextGen is a simulation framework enabling development of Internet of Flying Things (IoFT) applications using a single programming language and toolset across diverse, realistically modeled environments. Its utility is demonstrated through a sample problem."
  },
  {
    "url": "https://arxiv.org/abs/2301.08025v1",
    "title": "Generalization through Diversity: Improving Unsupervised Environment Design",
    "published_date": "2023-01-19",
    "abstract": "Agent decision making using Reinforcement Learning (RL) heavily relies on either a model or simulator of the environment (e.g., moving in an 8x8 maze with three rooms, playing Chess on an 8x8 board). Due to this dependence, small changes in the environment (e.g., positions of obstacles in the maze, size of the board) can severely affect the effectiveness of the policy learned by the agent. To that end, existing work has proposed training RL agents on an adaptive curriculum of environments (generated automatically) to improve performance on out-of-distribution (OOD) test scenarios. Specifically, existing research has employed the potential for the agent to learn in an environment (captured using Generalized Advantage Estimation, GAE) as the key factor to select the next environment(s) to train the agent. However, such a mechanism can select similar environments (with a high potential to learn) thereby making agent training redundant on all but one of those environments. To that end, we provide a principled approach to adaptively identify diverse environments based on a novel distance measure relevant to environment design. We empirically demonstrate the versatility and effectiveness of our method in comparison to multiple leading approaches for unsupervised environment design on three distinct benchmark problems used in literature.",
    "citation_count": 5,
    "summary": "This paper introduces a novel method for unsupervised environment design in reinforcement learning, focusing on generating diverse training environments to improve an agent's generalization capabilities to out-of-distribution scenarios. The approach uses a new distance metric to select diverse environments, outperforming existing methods that prioritize learning potential, leading to more robust and generalized agent policies."
  },
  {
    "url": "https://arxiv.org/abs/2210.10304",
    "title": "Synthesizing Reactive Test Environments for Autonomous Systems: Testing Reach-Avoid Specifications with Multi-Commodity Flows",
    "published_date": "2022-10-19",
    "abstract": "We study automated test generation for testing discrete decision-making modules in autonomous systems. Linear temporal logic is used to encode the system specification - requirements of the system under test - and the test specification, which is unknown to the system and describes the desired test behavior. The reactive test synthesis problem is to find constraints on system actions such that in a test execution, both the system and test specifications are satisfied. To do this, we use the specifications and their corresponding Büchi automata to construct the specification product automaton. Then, a virtual product graph representing all possible test executions of the system is constructed from the transition system and the specification product automaton. The main result of this paper is framing the test synthesis problem as a multi-commodity network flow optimization. This optimization is used to derive reactive constraints on system actions, which constitute the test environment. The resulting test environment ensures that the system meets the test specification while also satisfying the system specification. We illustrate this framework in simulation using grid world examples and demonstrate it on hardware with the Unitree A1 quadruped, where we test dynamic locomotion behaviors reactively.",
    "citation_count": 2,
    "summary": "This paper presents a novel method for automated test generation of autonomous systems, framing the reactive test synthesis problem as a multi-commodity network flow optimization to generate constraints ensuring both system and test specifications are met during execution. This approach is demonstrated through simulations and hardware experiments using a quadruped robot."
  },
  {
    "url": "https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained",
    "author": "Jay Bailey",
    "title": "Deep Q-Networks Explained",
    "published_date": "2022-09-13",
    "summary": "This article explains Deep Q-Networks (DQN), a deep reinforcement learning algorithm, at multiple levels of detail. It progressively introduces the concept, starting with high-level explanations and then delving into the mathematical and implementation specifics for readers with varying levels of prior knowledge."
  },
  {
    "url": "https://www.lesswrong.com/posts/rCP5iTYLtfcoC8NXd/self-organised-neural-networks-a-simple-natural-and",
    "author": "D𝜋",
    "title": "Self-Organised Neural Networks:\nA simple, natural and efficient way to intelligence",
    "published_date": "2022-01-01",
    "summary": "The article presents a novel spiking neural network achieving state-of-the-art accuracy (98.9%) on the PI-MNIST dataset using a simple, biologically-inspired learning method involving only additions, no backpropagation, and minimal parameters. Its performance and theoretical grounding are highlighted as requiring further investigation and validation."
  }
]